{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YZdxOxa97fZO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import util\n",
    "from math import exp, log\n",
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "r0dm5c_T7fZX"
   },
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "    def get_batch_test(self):\n",
    "    \n",
    "        return self.test_data, self.test_label\n",
    "    \n",
    "    def get_local_noise(self, noise_num=400, batch_size=10000):\n",
    "        \"\"\"\n",
    "        Generate noise_num # of gassaion noise knowing location \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        process_image = np.copy(self.test_data[:batch_size])\n",
    "        W_x = intitial_W_x(process_image)\n",
    "        uncer_mass = np.zeros(W_x[:,0,0,:,0].shape) + 0.01\n",
    "        base_rate = np.zeros(W_x[:,0,0,:,0].shape) + 0.5\n",
    "        image_size = process_image.shape[1]\n",
    "        index_noise = np.random.randint(0, image_size*image_size, noise_num)\n",
    "        \n",
    "        for i in index_noise:\n",
    "            noise_value = np.random.randn(batch_size,1) * 0.4\n",
    "            feature_value = process_image[:,int(i/image_size),int(i%image_size),:]\n",
    "            process_image[:,int(i/image_size),int(i%image_size),:] = feature_value + noise_value\n",
    "            belief_mass = W_x[:,int(i/image_size),int(i%image_size),:,0] * (1-noise_value)\n",
    "            disbelief_mass = 1 - belief_mass - uncer_mass\n",
    "            W_x[:,int(i/image_size),int(i%image_size),:] = np.moveaxis(np.array([belief_mass,disbelief_mass,uncer_mass,base_rate]), 0, -1)\n",
    "            \n",
    "        return tf.clip_by_value(process_image, 0, 1), self.test_label[:batch_size], W_x, index_noise\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_noise(self, noise_mode):\n",
    "        process_image = np.copy(self.test_data)\n",
    "        noise_gs_img = util.random_noise(process_image,mode=noise_mode,var=0.2)\n",
    "        return noise_gs_img, self.test_label\n",
    "    \n",
    "data_loader = MNISTLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kFxBUMwrU1sC"
   },
   "outputs": [],
   "source": [
    "def intitial_opinion_trust(X):\n",
    "    \"\"\"\n",
    "    X - 50,32,32,3\n",
    "    return W_x - 50,32,32,3,4\n",
    "    \n",
    "    \"\"\"\n",
    "    W_dis = np.zeros(X.shape).astype(np.float32)\n",
    "    W_base = W_dis + 0.5\n",
    "    W_x = np.array([X, W_dis, 1 - X, W_base]).astype(np.float32)\n",
    "    x_trust = (W_x[0] + W_x[2] * 0.5)\n",
    "    W_x = np.moveaxis(W_x, 0, -1)\n",
    "    \n",
    "    return W_x, x_trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intitial_W_x_location(X, index):\n",
    "    \"\"\"\n",
    "    X - 5000,28,28,1\n",
    "    return W_x - 400,\n",
    "    \n",
    "    \"\"\"\n",
    "    W_x = intitial_W_x(X)\n",
    "    uncer_mass = np.zeros(W_x[:,0,0,:,0].shape) + 0.01\n",
    "    base_rate = np.zeros(W_x[:,0,0,:,0].shape) + 0.5\n",
    "    image_size = X.shape[1]\n",
    "    for i in index:\n",
    "        belief_mass = W_x[:,int(i/image_size),int(i%image_size),:,0]\n",
    "        disbelief_mass = 1 - belief_mass - uncer_mass\n",
    "        W_x[:,int(i/image_size),int(i%image_size),:] = np.moveaxis(np.array([belief_mass,disbelief_mass,uncer_mass,base_rate]), 0, -1)\n",
    "\n",
    "    return W_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bXJNv0AWxVOC"
   },
   "outputs": [],
   "source": [
    "def padding(opinion, kernel_size):\n",
    "    \"\"\"\n",
    "    opinion - (batch_size 500, image_size 32， image_size 32，channel 3, 4) \n",
    "    W_w - (kernel_size, kernel_size, 3, filter_num, 4)\n",
    "    p - int(( n_W_prev - f + 2 * pad )/ stride) + 1\n",
    "    \n",
    "    \"\"\"\n",
    "    image_size = opinion.shape[1]\n",
    "    p = int((kernel_size - 1)/2)\n",
    "    padding_size = int(image_size + 2 * p)\n",
    "    opinion_pad = np.zeros((int(opinion.shape[0]),padding_size,padding_size,int(opinion.shape[-2]),int(opinion.shape[-1])))\n",
    "    opinion_pad[:,:,:,:] = np.array([0.0, 0.99, 0.01, 0.5])\n",
    "    opinion_pad[:,p:p+image_size,p:p+image_size,:,:] = opinion\n",
    "    return opinion_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yvzahj3w7fZZ"
   },
   "outputs": [],
   "source": [
    "def multi(W_x, W_y): # \n",
    "    W_x = W_x.astype(np.float64) \n",
    "    W_y = W_y.astype(np.float64) \n",
    "#     print(np.isnan(np.min(W_x[0]*W_y[0])))    \n",
    "    W_b = W_x[0]*W_y[0]+((1-W_x[3])*W_y[3]*W_x[0]*W_y[2]+W_x[3]*(1-W_y[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n",
    "#     print(np.isnan(np.min(W_b)))\n",
    "    W_d = W_x[1]+W_y[1]-W_x[1]*W_y[1]\n",
    "    W_u = W_x[2]*W_y[2]+((1-W_y[3])*W_x[0]*W_y[2]+(1-W_x[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n",
    "    W_a = W_x[3]*W_y[3]\n",
    "    return W_b,W_d,W_u,W_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HGsdrX0k7fZZ"
   },
   "outputs": [],
   "source": [
    "def conv_single_step_trust(W_x, W_w, W_b): \n",
    "    \"\"\"\n",
    "    W_x - (50,5,5,3,4)\n",
    "    W_w - (5,5,3,32,4)\n",
    "    W_b - (32,4)\n",
    "    W_wx - (4, 32, 3, 5, 5)\n",
    "    fusion_result - (32,)\n",
    "    \n",
    "    \"\"\"    \n",
    "    W_x = W_x.astype(np.float64)   \n",
    "    W_w = W_w.numpy().astype(np.float64) \n",
    "    W_b = W_b.numpy().astype(np.float64) \n",
    "    \n",
    "    filter_number = W_b.shape[0]\n",
    "    batch_size = W_x.shape[0]\n",
    "    fusion_result = []\n",
    "    W_x_expand = np.tile(np.expand_dims(W_x, axis=(4)), [1,1,1,1,filter_number,1])\n",
    "    W_w_expand = np.tile(np.expand_dims(W_w, axis=(0)), [batch_size,1,1,1,1,1])\n",
    "    W_b_expand = np.tile(np.expand_dims(W_b, axis=(0)), [batch_size,1,1])\n",
    "#     print(W_x_expand,W_w_expand)\n",
    "#     print('W_x_expand',np.isnan(np.min(W_x_expand)))\n",
    "#     print('W_w_expand',np.isnan(np.min(W_w_expand)))\n",
    "    W_wx = multi(np.transpose(W_x_expand),np.transpose(W_w_expand)) # (4, 32, 3, 5, 5, 50)\n",
    "#     print(np.asarray(W_wx)[:])\n",
    "#     print(np.isnan(np.min(W_wx)))\n",
    "    fusion_result = avg_fusion(np.asarray(W_wx), np.transpose(W_b_expand))\n",
    "#     print(np.isnan(np.min(fusion_result[0])))\n",
    "    return np.transpose(fusion_result[0]),np.transpose(fusion_result[1])\n",
    "\n",
    "def avg_fusion(W_wx, W_b):\n",
    "    \"\"\"\n",
    "    W_wx - (4, 32, 3, 5, 5, 50)\n",
    "    W_b -  (4, 32, 50)\n",
    "    fusion_result - (32, 32, 32)\n",
    "    \n",
    "    return opinion - 4,32,50\n",
    "           trust - 32,50\n",
    "    \"\"\"\n",
    "    \n",
    "    W_wx = np.reshape(W_wx, (W_wx.shape[0],W_wx.shape[1], \n",
    "                             W_wx.shape[2]*W_wx.shape[3]*W_wx.shape[4], W_wx.shape[5])).astype(np.float64) # (4, 32, 75, 50)\n",
    "    W_b = W_b.astype(np.float64)\n",
    "    \n",
    "    n_filter = W_b.shape[1]\n",
    "    batch_size = W_wx.shape[-1]\n",
    "    num_para = W_wx.shape[2]\n",
    "#     print(num_para)\n",
    "    b_wx, u_wx, a_wx = W_wx[0], W_wx[2], W_wx[3]\n",
    "    b_b, u_b, a_b = W_b[0], W_b[2], W_b[3]\n",
    "        \n",
    "    \n",
    "    u_combine = np.concatenate((u_wx, np.reshape(u_b,(u_b.shape[0], 1, batch_size))), axis=1) # (32, 76, 50)\n",
    "    b_combine = np.concatenate((b_wx, np.reshape(b_b,(b_b.shape[0], 1, batch_size))), axis=1) # (32, 76, 50)\n",
    "    u_combine_recip = (np.zeros(u_combine.shape)+1)/u_combine\n",
    "    \n",
    "    numerator = np.sum(b_combine * u_combine_recip, axis = 1) # (32, 50)\n",
    "    denominator = np.sum(u_combine_recip, axis = (1))\n",
    "    \n",
    "    b_fusion = numerator / denominator\n",
    "    u_fusion = (num_para+1) / denominator\n",
    "    a_fusion = (np.sum(a_wx, axis=(1)) + a_b) / (num_para+1)\n",
    "    \n",
    "    return np.array([b_fusion, 1-b_fusion-u_fusion, u_fusion, a_fusion]).astype(np.float32), (b_fusion + u_fusion * a_fusion).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7lg8hAmI7fZa"
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W_w, W_b):\n",
    "    \"\"\"\n",
    "        A_prev - (batch_size 500, image_size 32， image_size 32，channel 3, 4) \n",
    "        W_w - (kernel_size, kernel_size, 3, filter_num, 4)\n",
    "        W_b - (filter_num, 4)\n",
    "        return Z - (28, 28, filter_num, 4)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #获取来自上一层数据的基本信息\n",
    "#     print(A_prev.shape,W_w.shape)\n",
    "    (batch_size, n_H_prev, n_W_prev, _, _) = A_prev.shape\n",
    "    \n",
    "    #获取权重矩阵的基本信息\n",
    "    ( f , f , _, n_C, _) = W_w.shape\n",
    "\n",
    "\n",
    "    #获取超参数hparameters的值\n",
    "    stride = 1\n",
    "    pad = 1\n",
    "    \n",
    "    # padding opinion\n",
    "    A_prev_pad = padding(A_prev,f)\n",
    "    \n",
    "    #计算卷积后的图像的宽度高度，参考上面的公式，使用int()来进行板除\n",
    "    n_H = int(( n_H_prev - f + 2 * pad )/ stride) + 1\n",
    "    n_W = int(( n_W_prev - f + 2 * pad )/ stride) + 1\n",
    " \n",
    "    \n",
    "    #使用0来初始化卷积输出Z\n",
    "    Z = np.zeros((batch_size, n_H, n_W, n_C, 4)) \n",
    "    Z_trust = np.zeros((batch_size, n_H, n_W, n_C))\n",
    "\n",
    "    for h in range(n_H):                       \n",
    "        for w in range(n_W):                              \n",
    "            vert_start = h * stride        \n",
    "            vert_end = vert_start + f       \n",
    "            horiz_start = w * stride        \n",
    "            horiz_end = horiz_start + f     \n",
    "            a_slice_prev = A_prev_pad[:,vert_start:vert_end,horiz_start:horiz_end,:,:]   # 500, 3, 3, 3, 4\n",
    "#             print(a_slice_prev)\n",
    "#             print('a_slice_prev :', np.isnan(np.min(a_slice_prev)))\n",
    "            opinion, trust = conv_single_step_trust(a_slice_prev,W_w,W_b)   \n",
    "#             print(opinion.shape)\n",
    "            Z[:,h,w,:,:] = opinion\n",
    "            Z_trust[:,h,w,:] = trust\n",
    "#     print(Z.shape,Z_trust.shape)\n",
    "    return Z, Z_trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HJbAPUO37fZY"
   },
   "outputs": [],
   "source": [
    "class MyLayerMaxTrust(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    max-trust fuction layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyLayerMaxTrust, self).__init__()\n",
    "\n",
    "\n",
    "    def call(self, x, opinion, trust):\n",
    "        \"\"\"\n",
    "        x - (50, 28, 28, 32)\n",
    "        opinion - (50, 28, 28, 32, 4)\n",
    "        trust - (50, 28, 28, 32)\n",
    "        \n",
    "        x, pooling_opinion1 = self.maxtrust(x, opinion1, trust1)\n",
    "\n",
    "        opinion_out - (50, 14, 14, 32, 4)\n",
    "        return - (50, 14, 14, 32, 4)\n",
    "\n",
    "        \"\"\"\n",
    "    #         print(inputs.shape,inputB.shape)\n",
    "        trust_mul = np.zeros(x.shape)\n",
    "        image_size = x.shape[1]\n",
    "        opinion_out = np.zeros((opinion.shape[0],int(opinion.shape[1]/2), int(opinion.shape[2]/2),opinion.shape[3], 4)).astype(np.float32)\n",
    "#         print(opinion_out.shape)\n",
    "        for i in range(trust.shape[0]):\n",
    "            for k in range(trust.shape[-1]):\n",
    "                input_max = trust[i,:,:,k].reshape((1,trust.shape[1],trust.shape[2],1))\n",
    "                _, argmax = tf.nn.max_pool_with_argmax(input = input_max, ksize = [1, 2, 2, 1],\n",
    "                                                    strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "                argmax_1d = argmax.numpy().flatten()\n",
    "#                 print(len(argmax_1d))\n",
    "            \n",
    "                for j in range(len(argmax_1d)):\n",
    "                    trust_mul[i,int(argmax_1d[j]/image_size),int(argmax_1d[j]%image_size),k] = 1\n",
    "                    opinion_out[i,int(j/int(image_size/2)), int(j%int(image_size/2)),k,:] = opinion[i,int(argmax_1d[j]/image_size),int(argmax_1d[j]%image_size),k,:] \n",
    "\n",
    "        x = x * trust_mul\n",
    "\n",
    "        return x, opinion_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "l0xqqyWB7fZZ"
   },
   "outputs": [],
   "source": [
    "class MyLayerCONV(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    max-trust fuction layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyLayerCONV, self).__init__()\n",
    "\n",
    "\n",
    "    def call(self, input1, input2, input3):\n",
    "        \n",
    "        Z, Z_trust = conv_forward(input1, input2, input3)\n",
    "           \n",
    "        return Z.astype(np.float32), Z_trust.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dLQTm3SOxVOF"
   },
   "outputs": [],
   "source": [
    "class MyLayerMaxPool(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    original max-pooling with trust\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyLayerMaxPool, self).__init__()\n",
    "\n",
    "\n",
    "    def call(self, x, opinion):\n",
    "        \"\"\"\n",
    "        x-(50,14,14,8)\n",
    "        \n",
    "        \"\"\"\n",
    "        image_size = x.shape[1]\n",
    "        opinion_out = np.zeros((opinion.shape[0],int(opinion.shape[1]/2), \n",
    "                                int(opinion.shape[2]/2),opinion.shape[3], 4)).astype(np.float32)\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            for k in range(x.shape[-1]):\n",
    "                input_max = x[i,:,:,k].numpy().reshape((1,x.shape[1],x.shape[2],1))\n",
    "                _, argmax = tf.nn.max_pool_with_argmax(input = input_max, ksize = [1, 2, 2, 1],\n",
    "                                                    strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "                argmax_1d = argmax.numpy().flatten()\n",
    "                for j in range(len(argmax_1d)):\n",
    "                    opinion_out[i,int(j/int(image_size/2)), int(j%int(image_size/2)),k,:] = opinion[i,int(argmax_1d[j]/image_size),\n",
    "                                                                                                    int(argmax_1d[j]%image_size),k,:] \n",
    "        return opinion_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayerComputeTrust(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    input- (50,14,14,8)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyLayerComputeTrust, self).__init__()\n",
    "\n",
    "\n",
    "    def call(self, input1):\n",
    "        \n",
    "        opinion, trust = intitial_opinion_trust(input1)\n",
    "           \n",
    "        return opinion, trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "15za74YC7fZa"
   },
   "outputs": [],
   "source": [
    "def get_update_matrix(grads_w, grads_b, rs):\n",
    "    \"\"\"\n",
    "    grads_list_w1 = grads_list_w + tf.where(abs(tf.squeeze(grads[0]))<0.01, 1, 0)  #  shape=(5, 5, 3, 32)\n",
    "    grads_list_b1 = grads_list_b + tf.where(abs(tf.squeeze(grads[1]))<0.01, 1, 0)  #  shape=(32,)\n",
    "    grads_list_w2 = grads_list_w + tf.where(abs(tf.squeeze(grads[0]))<0.01, 1, 0)  #  shape=(5, 5, 32, 64)\n",
    "    grads_list_b2 = grads_list_b + tf.where(abs(tf.squeeze(grads[1]))<0.01, 1, 0)  #  shape=(64,)\n",
    "    grads_list_w3 = grads_list_w + tf.where(abs(tf.squeeze(grads[0]))<0.01, 1, 0)  #  shape=(5, 5, 64, 64)\n",
    "    grads_list_b3 = grads_list_b + tf.where(abs(tf.squeeze(grads[1]))<0.01, 1, 0)  #  shape=(64,)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    W_w = np.array([grads_w/(rs+2),(rs-grads_w)/(rs+2),np.full(grads_w.shape, 2/(rs+2)),\n",
    "                    np.full(grads_w.shape, 0.5)]).astype(np.float32)\n",
    "    W_b = np.array([grads_b/(rs+2), (rs-grads_b)/(rs+2), np.full(grads_b.shape, 2/(rs+2)), \n",
    "                    np.full(grads_b.shape, 0.5)]).astype(np.float32)\n",
    "    W_w = np.moveaxis(W_w, 0, -1)\n",
    "    \n",
    "    return W_w, W_b.swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "yby1VsHD5SGv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_num1 = 4\n",
    "filter_num2 = 8\n",
    "filter_num3 = 8\n",
    "kernelsize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convopinion1 = MyLayerCONV()\n",
    "        self.convopinion2 = MyLayerCONV()\n",
    "        self.convopinion3 = MyLayerCONV()\n",
    "        self.compute_trust = MyLayerComputeTrust()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=filter_num1,             \n",
    "            kernel_size=[kernelsize, kernelsize],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu   \n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=filter_num2,           \n",
    "            kernel_size=[kernelsize, kernelsize],    \n",
    "            padding='same',\n",
    "            activation=tf.nn.relu   \n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=filter_num3,            \n",
    "            kernel_size=[kernelsize, kernelsize],     \n",
    "            padding='same',         \n",
    "            activation=tf.nn.relu   \n",
    "        )\n",
    "\n",
    "        self.maxtrust1 = MyLayerMaxTrust()\n",
    "        self.maxtrust2 = MyLayerMaxTrust()\n",
    "        self.maxfeature1 = MyLayerMaxPool()\n",
    "        self.maxfeature2 = MyLayerMaxPool()\n",
    "        \n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "       \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        x - 50,32,32,3\n",
    "        W_x - 50,32,32,3,4\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        X, W_x, W_w1, W_b1, W_w2, W_b2 = inputs\n",
    " \n",
    "        opinion1, trust1 = self.convopinion1(W_x, W_w1, W_b1)\n",
    "        x = self.conv1(X)   \n",
    "#         print(np.isnan(np.min(opinion1)))\n",
    "#         print('Underflow or not: ',np.isnan(np.min(trust1)))\n",
    "#         print(opinion1.shape,trust1.shape)\n",
    "#         x, pooling_opinion1 = self.maxtrust1(x, opinion1, trust1)\n",
    "        pooling_opinion1 = self.maxfeature1(x, opinion1)\n",
    "        x = self.pool1(x) \n",
    "#         print('First 2 layers end:',(time.time()-start))\n",
    "#         print(np.isnan(np.min(x)))\n",
    "#         print(np.isnan(np.min(pooling_opinion1)))\n",
    "        opinion2, _ = self.convopinion2(pooling_opinion1, W_w2, W_b2)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        pooling_opinion2 = self.maxfeature2(x, opinion2)\n",
    "        x = self.pool2(x)\n",
    "#         print(np.isnan(np.min(opinion2)))\n",
    "#         print(np.isnan(np.min(x)))\n",
    "#         print('Second 2 layers end:',(time.time()-start))\n",
    "        \n",
    "        x = self.flatten(x) \n",
    "        x = self.dense1(x)    \n",
    "        output = self.dense2(x)                    \n",
    "        return output,opinion1,opinion2,pooling_opinion2\n",
    "    \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2mYRIhMy7fZY"
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 50\n",
    "threshold1 = 3.0\n",
    "threshold2 = 5.0\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ZMt5J6p-7fZb"
   },
   "outputs": [],
   "source": [
    "# initial opinion\n",
    "# W_x = np.array([[[[0.0, 0.0, 1.0, 0.5] for _ in range(3)] for _ in range(32)] for _ in range(32)]).astype(np.float32)\n",
    "\n",
    "w_initial1 = np.array([[[[[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num1)] \n",
    "                         for _ in range(1)] for _ in range(kernelsize)] for _ in range(kernelsize)]).astype(np.float32)\n",
    "b_initial1 = np.array([[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num1)]).astype(np.float32)\n",
    "\n",
    "w_initial2 = np.array([[[[[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num2)] \n",
    "                         for _ in range(filter_num1)] for _ in range(kernelsize)] for _ in range(kernelsize)]).astype(np.float32)\n",
    "b_initial2 = np.array([[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num2)]).astype(np.float32)\n",
    "\n",
    "# w_initial3 = np.array([[[[[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num3)] \n",
    "#                          for _ in range(filter_num2)] for _ in range(kernelsize)] for _ in range(kernelsize)]).astype(np.float32)\n",
    "# b_initial3 = np.array([[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num3)]).astype(np.float32)\n",
    "\n",
    "# w_initial4 = np.array([[[0.0, 0.0, 1.0, 0.5] for _ in range(64)] for _ in range(784)]).astype(np.float32)\n",
    "# b_initial4 = np.array([[0.0, 0.0, 1.0, 0.5] for _ in range(64)]).astype(np.float32)\n",
    "\n",
    "# w_initial5 = np.array([[[0.0, 0.0, 1.0, 0.5] for _ in range(10)] for _ in range(64)]).astype(np.float32)\n",
    "# b_initial5 = np.array([[0.0, 0.0, 1.0, 0.5] for _ in range(10)]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lgpZvsdWxVOG"
   },
   "outputs": [],
   "source": [
    "def evidence_collect(y, y_pred):\n",
    "    r = 0\n",
    "    s = 0\n",
    "    r_list = [0]*10\n",
    "    s_list = [0]*10\n",
    "    \n",
    "    for j in range(len(y_pred)):\n",
    "        for i in range(len(y_pred[0])):\n",
    "            if i == y[j]:\n",
    "                if y_pred[j][i] > 0.8:\n",
    "                    r_list[i]+=1\n",
    "                    r+=1\n",
    "                else:\n",
    "                    s+=1\n",
    "                    s_list[i]+=1\n",
    "            else:\n",
    "                if y_pred[j][i] < 0.2:\n",
    "                    r_list[i]+=1\n",
    "                else:\n",
    "                    s_list[i]+=1\n",
    "                    \n",
    "    y_N_op = []\n",
    "    for i in range(len(r_list)):\n",
    "        y_N_op.append([r_list[i]/(r_list[i]+s_list[i]+2), \n",
    "                       s_list[i]/(r_list[i]+s_list[i]+2), 2/(r_list[i]+s_list[i]+2), 0.5])\n",
    "\n",
    "    \n",
    "    return [r/(r+s+2), s/(r+s+2), 2/(r+s+2), 0.5], y_N_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "LsdJSsfTxVOH"
   },
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "train_loss = []\n",
    "opinion_convlist = []\n",
    "y_update_wb = []\n",
    "y_N_opinion = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1677183,
     "status": "ok",
     "timestamp": 1615854665861,
     "user": {
      "displayName": "Mingxi Cheng",
      "photoUrl": "",
      "userId": "11732313048760842335"
     },
     "user_tz": 420
    },
    "id": "_Cc0BUeq7fZb",
    "outputId": "79e7baeb-3245-47e1-e8cb-7f9f01ab3e02",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batch: 0\n",
      "train accuracy: 0.06\n",
      "One batch end: 2.780397653579712\n",
      "# of batch: 1\n",
      "train accuracy: 0.05\n",
      "One batch end: 3.5443973541259766\n",
      "# of batch: 2\n",
      "train accuracy: 0.1\n",
      "One batch end: 4.306440830230713\n",
      "# of batch: 3\n",
      "train accuracy: 0.12\n",
      "One batch end: 5.072985649108887\n",
      "# of batch: 4\n",
      "train accuracy: 0.116\n",
      "One batch end: 5.8425068855285645\n",
      "# of batch: 5\n",
      "train accuracy: 0.13\n",
      "One batch end: 6.641559839248657\n",
      "# of batch: 6\n",
      "train accuracy: 0.13428572\n",
      "One batch end: 7.6015589237213135\n",
      "# of batch: 7\n",
      "train accuracy: 0.1475\n",
      "One batch end: 8.372765302658081\n",
      "# of batch: 8\n",
      "train accuracy: 0.15777777\n",
      "One batch end: 9.31676459312439\n",
      "# of batch: 9\n",
      "train accuracy: 0.184\n",
      "One batch end: 10.269825220108032\n",
      "# of batch: 10\n",
      "train accuracy: 0.18727273\n",
      "One batch end: 11.164836883544922\n",
      "# of batch: 11\n",
      "train accuracy: 0.20166667\n",
      "One batch end: 11.940836429595947\n",
      "# of batch: 12\n",
      "train accuracy: 0.22153845\n",
      "One batch end: 12.72896146774292\n",
      "# of batch: 13\n",
      "train accuracy: 0.22285715\n",
      "One batch end: 13.499016284942627\n",
      "# of batch: 14\n",
      "train accuracy: 0.23066667\n",
      "One batch end: 14.276039838790894\n",
      "# of batch: 15\n",
      "train accuracy: 0.2325\n",
      "One batch end: 15.048755645751953\n",
      "# of batch: 16\n",
      "train accuracy: 0.23882353\n",
      "One batch end: 15.824755191802979\n",
      "# of batch: 17\n",
      "train accuracy: 0.24555555\n",
      "One batch end: 16.609812259674072\n",
      "# of batch: 18\n",
      "train accuracy: 0.25157896\n",
      "One batch end: 17.380875825881958\n",
      "# of batch: 19\n",
      "train accuracy: 0.256\n",
      "One batch end: 18.153093576431274\n",
      "# of batch: 20\n",
      "train accuracy: 0.25904763\n",
      "One batch end: 18.923675060272217\n",
      "# of batch: 21\n",
      "train accuracy: 0.25636363\n",
      "One batch end: 19.695674896240234\n",
      "# of batch: 22\n",
      "train accuracy: 0.25652173\n",
      "One batch end: 20.45867419242859\n",
      "# of batch: 23\n",
      "train accuracy: 0.27\n",
      "One batch end: 21.23067355155945\n",
      "# of batch: 24\n",
      "train accuracy: 0.2824\n",
      "One batch end: 21.996805667877197\n",
      "# of batch: 25\n",
      "train accuracy: 0.29615384\n",
      "One batch end: 22.75880527496338\n",
      "# of batch: 26\n",
      "train accuracy: 0.30666667\n",
      "One batch end: 23.52580451965332\n",
      "# of batch: 27\n",
      "train accuracy: 0.31071427\n",
      "One batch end: 24.283804178237915\n",
      "# of batch: 28\n",
      "train accuracy: 0.31793103\n",
      "One batch end: 25.0478036403656\n",
      "# of batch: 29\n",
      "train accuracy: 0.324\n",
      "One batch end: 25.811803102493286\n",
      "# of batch: 30\n",
      "train accuracy: 0.3303226\n",
      "One batch end: 26.577802419662476\n",
      "# of batch: 31\n",
      "train accuracy: 0.33875\n",
      "One batch end: 27.339802026748657\n",
      "# of batch: 32\n",
      "train accuracy: 0.34666666\n",
      "One batch end: 28.1031334400177\n",
      "# of batch: 33\n",
      "train accuracy: 0.35411766\n",
      "One batch end: 28.859654664993286\n",
      "# of batch: 34\n",
      "train accuracy: 0.36114284\n",
      "One batch end: 29.623175859451294\n",
      "# of batch: 35\n",
      "train accuracy: 0.36722222\n",
      "One batch end: 30.514175176620483\n",
      "# of batch: 36\n",
      "train accuracy: 0.37297297\n",
      "One batch end: 31.378694534301758\n",
      "# of batch: 37\n",
      "train accuracy: 0.37842104\n",
      "One batch end: 32.276211977005005\n",
      "# of batch: 38\n",
      "train accuracy: 0.3851282\n",
      "One batch end: 33.23521423339844\n",
      "# of batch: 39\n",
      "train accuracy: 0.3885\n",
      "One batch end: 34.107213497161865\n",
      "# of batch: 40\n",
      "train accuracy: 0.39756098\n",
      "One batch end: 34.96120882034302\n",
      "# of batch: 41\n",
      "train accuracy: 0.40333334\n",
      "One batch end: 35.772212743759155\n",
      "# of batch: 42\n",
      "train accuracy: 0.4060465\n",
      "One batch end: 36.583353757858276\n",
      "# of batch: 43\n",
      "train accuracy: 0.41045454\n",
      "One batch end: 37.35735321044922\n",
      "# of batch: 44\n",
      "train accuracy: 0.41777778\n",
      "One batch end: 38.13635277748108\n",
      "# of batch: 45\n",
      "train accuracy: 0.4247826\n",
      "One batch end: 38.904703855514526\n",
      "# of batch: 46\n",
      "train accuracy: 0.43276596\n",
      "One batch end: 39.66670322418213\n",
      "# of batch: 47\n",
      "train accuracy: 0.43875\n",
      "One batch end: 40.430824756622314\n",
      "# of batch: 48\n",
      "train accuracy: 0.44612244\n",
      "One batch end: 41.19582414627075\n",
      "# of batch: 49\n",
      "train accuracy: 0.452\n",
      "One batch end: 41.95783615112305\n",
      "# of batch: 50\n",
      "train accuracy: 0.45725492\n",
      "One batch end: 42.7258358001709\n",
      "# of batch: 51\n",
      "train accuracy: 0.46\n",
      "One batch end: 43.483835220336914\n",
      "# of batch: 52\n",
      "train accuracy: 0.46415094\n",
      "One batch end: 44.262346267700195\n",
      "# of batch: 53\n",
      "train accuracy: 0.46703705\n",
      "One batch end: 45.05034565925598\n",
      "# of batch: 54\n",
      "train accuracy: 0.472\n",
      "One batch end: 45.81534504890442\n",
      "# of batch: 55\n",
      "train accuracy: 0.47535715\n",
      "One batch end: 46.58334493637085\n",
      "# of batch: 56\n",
      "train accuracy: 0.48105264\n",
      "One batch end: 47.30134415626526\n",
      "# of batch: 57\n",
      "train accuracy: 0.48448277\n",
      "One batch end: 48.23689818382263\n",
      "# of batch: 58\n",
      "train accuracy: 0.4881356\n",
      "One batch end: 48.94741415977478\n",
      "# of batch: 59\n",
      "train accuracy: 0.49266666\n",
      "One batch end: 49.652413845062256\n",
      "# of batch: 60\n",
      "train accuracy: 0.49704918\n",
      "One batch end: 50.353920221328735\n",
      "# of batch: 61\n",
      "train accuracy: 0.50096774\n",
      "One batch end: 51.052976846694946\n",
      "# of batch: 62\n",
      "train accuracy: 0.50539684\n",
      "One batch end: 51.80397629737854\n",
      "# of batch: 63\n",
      "train accuracy: 0.5103125\n",
      "One batch end: 52.504976987838745\n",
      "# of batch: 64\n",
      "train accuracy: 0.5135385\n",
      "One batch end: 53.221497535705566\n",
      "# of batch: 65\n",
      "train accuracy: 0.5181818\n",
      "One batch end: 53.951497077941895\n",
      "# of batch: 66\n",
      "train accuracy: 0.52268654\n",
      "One batch end: 54.65158700942993\n",
      "# of batch: 67\n",
      "train accuracy: 0.525\n",
      "One batch end: 55.354586601257324\n",
      "# of batch: 68\n",
      "train accuracy: 0.5295652\n",
      "One batch end: 56.0597460269928\n",
      "# of batch: 69\n",
      "train accuracy: 0.534\n",
      "One batch end: 56.75894808769226\n",
      "# of batch: 70\n",
      "train accuracy: 0.5380282\n",
      "One batch end: 57.46794772148132\n",
      "# of batch: 71\n",
      "train accuracy: 0.54194444\n",
      "One batch end: 58.17232894897461\n",
      "# of batch: 72\n",
      "train accuracy: 0.5460274\n",
      "One batch end: 58.877535343170166\n",
      "# of batch: 73\n",
      "train accuracy: 0.5489189\n",
      "One batch end: 59.57548904418945\n",
      "# of batch: 74\n",
      "train accuracy: 0.5517333\n",
      "One batch end: 60.27748894691467\n",
      "# of batch: 75\n",
      "train accuracy: 0.5539474\n",
      "One batch end: 60.98000955581665\n",
      "# of batch: 76\n",
      "train accuracy: 0.55792207\n",
      "One batch end: 61.68100905418396\n",
      "# of batch: 77\n",
      "train accuracy: 0.5617949\n",
      "One batch end: 62.39300847053528\n",
      "# of batch: 78\n",
      "train accuracy: 0.5637975\n",
      "One batch end: 63.099008083343506\n",
      "# of batch: 79\n",
      "train accuracy: 0.5675\n",
      "One batch end: 63.80407357215881\n",
      "# of batch: 80\n",
      "train accuracy: 0.57061726\n",
      "One batch end: 64.50307321548462\n",
      "# of batch: 81\n",
      "train accuracy: 0.5726829\n",
      "One batch end: 65.2165846824646\n",
      "# of batch: 82\n",
      "train accuracy: 0.5761446\n",
      "One batch end: 65.93258428573608\n",
      "# of batch: 83\n",
      "train accuracy: 0.57857144\n",
      "One batch end: 66.63326334953308\n",
      "# of batch: 84\n",
      "train accuracy: 0.58164704\n",
      "One batch end: 67.33526277542114\n",
      "# of batch: 85\n",
      "train accuracy: 0.58348835\n",
      "One batch end: 68.04626226425171\n",
      "# of batch: 86\n",
      "train accuracy: 0.5862069\n",
      "One batch end: 68.7512617111206\n",
      "# of batch: 87\n",
      "train accuracy: 0.58931816\n",
      "One batch end: 69.46026134490967\n",
      "# of batch: 88\n",
      "train accuracy: 0.59146065\n",
      "One batch end: 70.16426181793213\n",
      "# of batch: 89\n",
      "train accuracy: 0.5937778\n",
      "One batch end: 70.86878228187561\n",
      "# of batch: 90\n",
      "train accuracy: 0.5953846\n",
      "One batch end: 71.56786799430847\n",
      "# of batch: 91\n",
      "train accuracy: 0.59782606\n",
      "One batch end: 72.26986742019653\n",
      "# of batch: 92\n",
      "train accuracy: 0.6\n",
      "One batch end: 72.97186708450317\n",
      "# of batch: 93\n",
      "train accuracy: 0.6021277\n",
      "One batch end: 73.6758668422699\n",
      "# of batch: 94\n",
      "train accuracy: 0.6046316\n",
      "One batch end: 74.3768663406372\n",
      "# of batch: 95\n",
      "train accuracy: 0.6075\n",
      "One batch end: 75.07686591148376\n",
      "# of batch: 96\n",
      "train accuracy: 0.6096907\n",
      "One batch end: 75.77638649940491\n",
      "# of batch: 97\n",
      "train accuracy: 0.6122449\n",
      "One batch end: 76.47338628768921\n",
      "# of batch: 98\n",
      "train accuracy: 0.61474746\n",
      "One batch end: 77.17838549613953\n",
      "# of batch: 99\n",
      "train accuracy: 0.6164\n",
      "One batch end: 77.883385181427\n",
      "# of batch: 100\n",
      "train accuracy: 0.6182178\n",
      "One batch end: 78.58190655708313\n",
      "# of batch: 101\n",
      "train accuracy: 0.62058824\n",
      "One batch end: 79.28602504730225\n",
      "# of batch: 102\n",
      "train accuracy: 0.6231068\n",
      "One batch end: 79.99102473258972\n",
      "# of batch: 103\n",
      "train accuracy: 0.6255769\n",
      "One batch end: 80.69302415847778\n",
      "# of batch: 104\n",
      "train accuracy: 0.6272381\n",
      "One batch end: 81.39954805374146\n",
      "# of batch: 105\n",
      "train accuracy: 0.62905663\n",
      "One batch end: 82.20954513549805\n",
      "# of batch: 106\n",
      "train accuracy: 0.63102806\n",
      "One batch end: 83.01884341239929\n",
      "# of batch: 107\n",
      "train accuracy: 0.6324074\n",
      "One batch end: 83.75484275817871\n",
      "# of batch: 108\n",
      "train accuracy: 0.6350459\n",
      "One batch end: 84.57091999053955\n",
      "# of batch: 109\n",
      "train accuracy: 0.63618183\n",
      "One batch end: 85.33843755722046\n",
      "# of batch: 110\n",
      "train accuracy: 0.6381982\n",
      "One batch end: 86.05805492401123\n",
      "# of batch: 111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.6398214\n",
      "One batch end: 86.78405046463013\n",
      "# of batch: 112\n",
      "train accuracy: 0.6421239\n",
      "One batch end: 87.50957608222961\n",
      "# of batch: 113\n",
      "train accuracy: 0.6433333\n",
      "One batch end: 88.36857533454895\n",
      "# of batch: 114\n",
      "train accuracy: 0.6450435\n",
      "One batch end: 89.20257496833801\n",
      "# of batch: 115\n",
      "train accuracy: 0.64689654\n",
      "One batch end: 89.95157551765442\n",
      "# of batch: 116\n",
      "train accuracy: 0.64871794\n",
      "One batch end: 90.68857026100159\n",
      "# of batch: 117\n",
      "train accuracy: 0.65118647\n",
      "One batch end: 91.4152626991272\n",
      "# of batch: 118\n",
      "train accuracy: 0.65260506\n",
      "One batch end: 92.19126200675964\n",
      "# of batch: 119\n",
      "train accuracy: 0.6545\n",
      "One batch end: 92.98376822471619\n",
      "# of batch: 120\n",
      "train accuracy: 0.65504134\n",
      "One batch end: 93.7617678642273\n",
      "# of batch: 121\n",
      "train accuracy: 0.65737706\n",
      "One batch end: 94.55876731872559\n",
      "# of batch: 122\n",
      "train accuracy: 0.6593496\n",
      "One batch end: 95.34976696968079\n",
      "# of batch: 123\n",
      "train accuracy: 0.6608065\n",
      "One batch end: 96.13276648521423\n",
      "# of batch: 124\n",
      "train accuracy: 0.66304\n",
      "One batch end: 96.97076630592346\n",
      "# of batch: 125\n",
      "train accuracy: 0.6649206\n",
      "One batch end: 97.7907612323761\n",
      "# of batch: 126\n",
      "train accuracy: 0.66677165\n",
      "One batch end: 98.5632255077362\n",
      "# of batch: 127\n",
      "train accuracy: 0.66875\n",
      "One batch end: 99.42346024513245\n",
      "# of batch: 128\n",
      "train accuracy: 0.6699225\n",
      "One batch end: 100.18645930290222\n",
      "# of batch: 129\n",
      "train accuracy: 0.6716923\n",
      "One batch end: 100.95245456695557\n",
      "# of batch: 130\n",
      "train accuracy: 0.6731298\n",
      "One batch end: 101.7315776348114\n",
      "# of batch: 131\n",
      "train accuracy: 0.67454547\n",
      "One batch end: 102.61057686805725\n",
      "# of batch: 132\n",
      "train accuracy: 0.67593986\n",
      "One batch end: 103.48357605934143\n",
      "# of batch: 133\n",
      "train accuracy: 0.67731345\n",
      "One batch end: 104.3890974521637\n",
      "# of batch: 134\n",
      "train accuracy: 0.67896295\n",
      "One batch end: 105.30917096138\n",
      "# of batch: 135\n",
      "train accuracy: 0.67985296\n",
      "One batch end: 106.21017026901245\n",
      "# of batch: 136\n",
      "train accuracy: 0.680438\n",
      "One batch end: 106.97116994857788\n",
      "# of batch: 137\n",
      "train accuracy: 0.6821739\n",
      "One batch end: 107.80916953086853\n",
      "# of batch: 138\n",
      "train accuracy: 0.6833094\n",
      "One batch end: 108.66168284416199\n",
      "# of batch: 139\n",
      "train accuracy: 0.68457144\n",
      "One batch end: 109.54177188873291\n",
      "# of batch: 140\n",
      "train accuracy: 0.68638295\n",
      "One batch end: 110.40377163887024\n",
      "# of batch: 141\n",
      "train accuracy: 0.6878873\n",
      "One batch end: 111.21902680397034\n",
      "# of batch: 142\n",
      "train accuracy: 0.6888112\n",
      "One batch end: 112.0246148109436\n",
      "# of batch: 143\n",
      "train accuracy: 0.6901389\n",
      "One batch end: 112.852614402771\n",
      "# of batch: 144\n",
      "train accuracy: 0.6917241\n",
      "One batch end: 113.76213598251343\n",
      "# of batch: 145\n",
      "train accuracy: 0.6928767\n",
      "One batch end: 114.55913519859314\n",
      "# of batch: 146\n",
      "train accuracy: 0.6941497\n",
      "One batch end: 115.36665678024292\n",
      "# of batch: 147\n",
      "train accuracy: 0.69486487\n",
      "One batch end: 116.21416163444519\n",
      "# of batch: 148\n",
      "train accuracy: 0.69637585\n",
      "One batch end: 117.12516641616821\n",
      "# of batch: 149\n",
      "train accuracy: 0.69813335\n",
      "One batch end: 117.97519898414612\n",
      "# of batch: 150\n",
      "train accuracy: 0.69960266\n",
      "One batch end: 118.86151099205017\n",
      "# of batch: 151\n",
      "train accuracy: 0.7010526\n",
      "One batch end: 119.67435073852539\n",
      "# of batch: 152\n",
      "train accuracy: 0.70248365\n",
      "One batch end: 120.55587267875671\n",
      "# of batch: 153\n",
      "train accuracy: 0.70363635\n",
      "One batch end: 121.37191462516785\n",
      "# of batch: 154\n",
      "train accuracy: 0.7047742\n",
      "One batch end: 122.18391418457031\n",
      "# of batch: 155\n",
      "train accuracy: 0.70602566\n",
      "One batch end: 122.98142409324646\n",
      "# of batch: 156\n",
      "train accuracy: 0.7070064\n",
      "One batch end: 123.84952425956726\n",
      "# of batch: 157\n",
      "train accuracy: 0.7082279\n",
      "One batch end: 124.69763278961182\n",
      "# of batch: 158\n",
      "train accuracy: 0.7090566\n",
      "One batch end: 125.49363207817078\n",
      "# of batch: 159\n",
      "train accuracy: 0.709875\n",
      "One batch end: 126.26063561439514\n",
      "# of batch: 160\n",
      "train accuracy: 0.71080744\n",
      "One batch end: 127.03229427337646\n",
      "# of batch: 161\n",
      "train accuracy: 0.71185184\n",
      "One batch end: 127.81348252296448\n",
      "# of batch: 162\n",
      "train accuracy: 0.7131288\n",
      "One batch end: 128.59450244903564\n",
      "# of batch: 163\n",
      "train accuracy: 0.71402436\n",
      "One batch end: 129.38050174713135\n",
      "# of batch: 164\n",
      "train accuracy: 0.71539396\n",
      "One batch end: 130.15550136566162\n",
      "# of batch: 165\n",
      "train accuracy: 0.71686745\n",
      "One batch end: 130.92450070381165\n",
      "# of batch: 166\n",
      "train accuracy: 0.71736526\n",
      "One batch end: 131.683500289917\n",
      "# of batch: 167\n",
      "train accuracy: 0.71869045\n",
      "One batch end: 132.4510154724121\n",
      "# of batch: 168\n",
      "train accuracy: 0.71976334\n",
      "One batch end: 133.21001505851746\n",
      "# of batch: 169\n",
      "train accuracy: 0.7209412\n",
      "One batch end: 134.006014585495\n",
      "# of batch: 170\n",
      "train accuracy: 0.7218713\n",
      "One batch end: 134.80710244178772\n",
      "# of batch: 171\n",
      "train accuracy: 0.72244185\n",
      "One batch end: 135.59110164642334\n",
      "# of batch: 172\n",
      "train accuracy: 0.72369945\n",
      "One batch end: 136.37970066070557\n",
      "# of batch: 173\n",
      "train accuracy: 0.7245977\n",
      "One batch end: 137.16470050811768\n",
      "# of batch: 174\n",
      "train accuracy: 0.7253714\n",
      "One batch end: 137.88377332687378\n",
      "# of batch: 175\n",
      "train accuracy: 0.72647727\n",
      "One batch end: 138.60380864143372\n",
      "# of batch: 176\n",
      "train accuracy: 0.7272316\n",
      "One batch end: 139.33533000946045\n",
      "# of batch: 177\n",
      "train accuracy: 0.72808987\n",
      "One batch end: 140.03833055496216\n",
      "# of batch: 178\n",
      "train accuracy: 0.7293855\n",
      "One batch end: 140.73833012580872\n",
      "# of batch: 179\n",
      "train accuracy: 0.7301111\n",
      "One batch end: 141.44432854652405\n",
      "# of batch: 180\n",
      "train accuracy: 0.7308287\n",
      "One batch end: 142.14538550376892\n",
      "# of batch: 181\n",
      "train accuracy: 0.7315385\n",
      "One batch end: 142.84338474273682\n",
      "# of batch: 182\n",
      "train accuracy: 0.73224044\n",
      "One batch end: 143.54157137870789\n",
      "# of batch: 183\n",
      "train accuracy: 0.7328261\n",
      "One batch end: 144.2965714931488\n",
      "# of batch: 184\n",
      "train accuracy: 0.7336216\n",
      "One batch end: 145.07962822914124\n",
      "# of batch: 185\n",
      "train accuracy: 0.7348387\n",
      "One batch end: 145.84862756729126\n",
      "# of batch: 186\n",
      "train accuracy: 0.73497325\n",
      "One batch end: 146.65869283676147\n",
      "# of batch: 187\n",
      "train accuracy: 0.73574466\n",
      "One batch end: 147.3946933746338\n",
      "# of batch: 188\n",
      "train accuracy: 0.73671955\n",
      "One batch end: 148.10066890716553\n",
      "# of batch: 189\n",
      "train accuracy: 0.7375789\n",
      "One batch end: 148.79966807365417\n",
      "# of batch: 190\n",
      "train accuracy: 0.7383246\n",
      "One batch end: 149.51166772842407\n",
      "# of batch: 191\n",
      "train accuracy: 0.7391667\n",
      "One batch end: 150.2546672821045\n",
      "# of batch: 192\n",
      "train accuracy: 0.73979276\n",
      "One batch end: 150.98966693878174\n",
      "# of batch: 193\n",
      "train accuracy: 0.7406185\n",
      "One batch end: 151.7206666469574\n",
      "# of batch: 194\n",
      "train accuracy: 0.74174356\n",
      "One batch end: 152.42977643013\n",
      "# of batch: 195\n",
      "train accuracy: 0.74214286\n",
      "One batch end: 153.25176858901978\n",
      "# of batch: 196\n",
      "train accuracy: 0.7431472\n",
      "One batch end: 154.09709978103638\n",
      "# of batch: 197\n",
      "train accuracy: 0.74383837\n",
      "One batch end: 154.9416229724884\n",
      "# of batch: 198\n",
      "train accuracy: 0.74442214\n",
      "One batch end: 155.705144405365\n",
      "# of batch: 199\n",
      "train accuracy: 0.745\n",
      "One batch end: 156.43114376068115\n",
      "# of batch: 200\n",
      "train accuracy: 0.74587065\n",
      "One batch end: 157.18114352226257\n",
      "# of batch: 201\n",
      "train accuracy: 0.74633664\n",
      "One batch end: 157.9041428565979\n",
      "# of batch: 202\n",
      "train accuracy: 0.74719214\n",
      "One batch end: 158.62914276123047\n",
      "# of batch: 203\n",
      "train accuracy: 0.7479412\n",
      "One batch end: 159.37414169311523\n",
      "# of batch: 204\n",
      "train accuracy: 0.74887806\n",
      "One batch end: 160.1071412563324\n",
      "# of batch: 205\n",
      "train accuracy: 0.7495146\n",
      "One batch end: 160.85113835334778\n",
      "# of batch: 206\n",
      "train accuracy: 0.7501449\n",
      "One batch end: 161.68866300582886\n",
      "# of batch: 207\n",
      "train accuracy: 0.75076926\n",
      "One batch end: 162.5052673816681\n",
      "# of batch: 208\n",
      "train accuracy: 0.75167465\n",
      "One batch end: 163.4282670021057\n",
      "# of batch: 209\n",
      "train accuracy: 0.75238097\n",
      "One batch end: 164.29226565361023\n",
      "# of batch: 210\n",
      "train accuracy: 0.752891\n",
      "One batch end: 165.04826593399048\n",
      "# of batch: 211\n",
      "train accuracy: 0.7539623\n",
      "One batch end: 165.8972659111023\n",
      "# of batch: 212\n",
      "train accuracy: 0.7541784\n",
      "One batch end: 166.81126427650452\n",
      "# of batch: 213\n",
      "train accuracy: 0.75523365\n",
      "One batch end: 167.72426342964172\n",
      "# of batch: 214\n",
      "train accuracy: 0.75618607\n",
      "One batch end: 168.642263174057\n",
      "# of batch: 215\n",
      "train accuracy: 0.7567593\n",
      "One batch end: 169.40926218032837\n",
      "# of batch: 216\n",
      "train accuracy: 0.7573272\n",
      "One batch end: 170.15126180648804\n",
      "# of batch: 217\n",
      "train accuracy: 0.75798166\n",
      "One batch end: 170.96726202964783\n",
      "# of batch: 218\n",
      "train accuracy: 0.75853884\n",
      "One batch end: 171.70526146888733\n",
      "# of batch: 219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.759\n",
      "One batch end: 172.4312608242035\n",
      "# of batch: 220\n",
      "train accuracy: 0.7591855\n",
      "One batch end: 173.14133262634277\n",
      "# of batch: 221\n",
      "train accuracy: 0.7598198\n",
      "One batch end: 173.86585426330566\n",
      "# of batch: 222\n",
      "train accuracy: 0.7606278\n",
      "One batch end: 174.62885355949402\n",
      "# of batch: 223\n",
      "train accuracy: 0.7608036\n",
      "One batch end: 175.34985303878784\n",
      "# of batch: 224\n",
      "train accuracy: 0.76115555\n",
      "One batch end: 176.05985260009766\n",
      "# of batch: 225\n",
      "train accuracy: 0.76203537\n",
      "One batch end: 176.78485226631165\n",
      "# of batch: 226\n",
      "train accuracy: 0.76255506\n",
      "One batch end: 177.4998517036438\n",
      "# of batch: 227\n",
      "train accuracy: 0.76324564\n",
      "One batch end: 178.2028512954712\n",
      "# of batch: 228\n",
      "train accuracy: 0.7638428\n",
      "One batch end: 178.90585064888\n",
      "# of batch: 229\n",
      "train accuracy: 0.7646087\n",
      "One batch end: 179.60785055160522\n",
      "# of batch: 230\n",
      "train accuracy: 0.76545453\n",
      "One batch end: 180.31984996795654\n",
      "# of batch: 231\n",
      "train accuracy: 0.76586205\n",
      "One batch end: 181.02584958076477\n",
      "# of batch: 232\n",
      "train accuracy: 0.76669526\n",
      "One batch end: 181.73284888267517\n",
      "# of batch: 233\n",
      "train accuracy: 0.7674359\n",
      "One batch end: 182.44384837150574\n",
      "# of batch: 234\n",
      "train accuracy: 0.7682553\n",
      "One batch end: 183.20236945152283\n",
      "# of batch: 235\n",
      "train accuracy: 0.7691525\n",
      "One batch end: 183.96136903762817\n",
      "# of batch: 236\n",
      "train accuracy: 0.7694515\n",
      "One batch end: 184.73037004470825\n",
      "# of batch: 237\n",
      "train accuracy: 0.7702521\n",
      "One batch end: 185.50136804580688\n",
      "# of batch: 238\n",
      "train accuracy: 0.77096236\n",
      "One batch end: 186.272367477417\n",
      "# of batch: 239\n",
      "train accuracy: 0.77141666\n",
      "One batch end: 187.16636276245117\n",
      "# of batch: 240\n",
      "train accuracy: 0.7720332\n",
      "One batch end: 188.0158886909485\n",
      "# of batch: 241\n",
      "train accuracy: 0.7723967\n",
      "One batch end: 188.81496906280518\n",
      "# of batch: 242\n",
      "train accuracy: 0.7725103\n",
      "One batch end: 189.69596481323242\n",
      "# of batch: 243\n",
      "train accuracy: 0.77311474\n",
      "One batch end: 190.60521340370178\n",
      "# of batch: 244\n",
      "train accuracy: 0.7737143\n",
      "One batch end: 191.49421286582947\n",
      "# of batch: 245\n",
      "train accuracy: 0.7743902\n",
      "One batch end: 192.30721259117126\n",
      "# of batch: 246\n",
      "train accuracy: 0.77522266\n",
      "One batch end: 193.08621191978455\n",
      "# of batch: 247\n",
      "train accuracy: 0.7758064\n",
      "One batch end: 193.8442485332489\n",
      "# of batch: 248\n",
      "train accuracy: 0.7763052\n",
      "One batch end: 194.604248046875\n",
      "# of batch: 249\n",
      "train accuracy: 0.77672\n",
      "One batch end: 195.38124775886536\n",
      "# of batch: 250\n",
      "train accuracy: 0.7772111\n",
      "One batch end: 196.16024661064148\n",
      "# of batch: 251\n",
      "train accuracy: 0.77761906\n",
      "One batch end: 197.00424647331238\n",
      "# of batch: 252\n",
      "train accuracy: 0.77810276\n",
      "One batch end: 197.8406982421875\n",
      "# of batch: 253\n",
      "train accuracy: 0.7784252\n",
      "One batch end: 198.6806981563568\n",
      "# of batch: 254\n",
      "train accuracy: 0.7790588\n",
      "One batch end: 199.52622056007385\n",
      "# of batch: 255\n",
      "train accuracy: 0.7796875\n",
      "One batch end: 200.43178343772888\n",
      "# of batch: 256\n",
      "train accuracy: 0.78038913\n",
      "One batch end: 201.24078273773193\n",
      "# of batch: 257\n",
      "train accuracy: 0.7807752\n",
      "One batch end: 202.08530473709106\n",
      "# of batch: 258\n",
      "train accuracy: 0.78138995\n",
      "One batch end: 202.90830421447754\n",
      "# of batch: 259\n",
      "train accuracy: 0.78184617\n",
      "One batch end: 203.76630401611328\n",
      "# of batch: 260\n",
      "train accuracy: 0.7824521\n",
      "One batch end: 204.6074185371399\n",
      "# of batch: 261\n",
      "train accuracy: 0.78305346\n",
      "One batch end: 205.40141820907593\n",
      "# of batch: 262\n",
      "train accuracy: 0.7836502\n",
      "One batch end: 206.18749690055847\n",
      "# of batch: 263\n",
      "train accuracy: 0.7840152\n",
      "One batch end: 207.0475368499756\n",
      "# of batch: 264\n",
      "train accuracy: 0.78445286\n",
      "One batch end: 207.89304900169373\n",
      "# of batch: 265\n",
      "train accuracy: 0.7850376\n",
      "One batch end: 208.70804858207703\n",
      "# of batch: 266\n",
      "train accuracy: 0.7855431\n",
      "One batch end: 209.51464295387268\n",
      "# of batch: 267\n",
      "train accuracy: 0.7861194\n",
      "One batch end: 210.4236421585083\n",
      "# of batch: 268\n",
      "train accuracy: 0.7866914\n",
      "One batch end: 211.21716213226318\n",
      "# of batch: 269\n",
      "train accuracy: 0.7873333\n",
      "One batch end: 212.00916194915771\n",
      "# of batch: 270\n",
      "train accuracy: 0.7878229\n",
      "One batch end: 212.78068280220032\n",
      "# of batch: 271\n",
      "train accuracy: 0.7884559\n",
      "One batch end: 213.57168173789978\n",
      "# of batch: 272\n",
      "train accuracy: 0.7885714\n",
      "One batch end: 214.34868144989014\n",
      "# of batch: 273\n",
      "train accuracy: 0.7890511\n",
      "One batch end: 215.15468072891235\n",
      "# of batch: 274\n",
      "train accuracy: 0.7893091\n",
      "One batch end: 215.97320222854614\n",
      "# of batch: 275\n",
      "train accuracy: 0.78992754\n",
      "One batch end: 216.80972361564636\n",
      "# of batch: 276\n",
      "train accuracy: 0.7904693\n",
      "One batch end: 217.6977195739746\n",
      "# of batch: 277\n",
      "train accuracy: 0.7910072\n",
      "One batch end: 218.54779434204102\n",
      "# of batch: 278\n",
      "train accuracy: 0.7915412\n",
      "One batch end: 219.42979383468628\n",
      "# of batch: 279\n",
      "train accuracy: 0.7918571\n",
      "One batch end: 220.20979285240173\n",
      "# of batch: 280\n",
      "train accuracy: 0.792242\n",
      "One batch end: 221.108300447464\n",
      "# of batch: 281\n",
      "train accuracy: 0.79269505\n",
      "One batch end: 221.91082167625427\n",
      "# of batch: 282\n",
      "train accuracy: 0.7930742\n",
      "One batch end: 222.6828212738037\n",
      "# of batch: 283\n",
      "train accuracy: 0.79359156\n",
      "One batch end: 223.56721758842468\n",
      "# of batch: 284\n",
      "train accuracy: 0.79382455\n",
      "One batch end: 224.46621680259705\n",
      "# of batch: 285\n",
      "train accuracy: 0.79412585\n",
      "One batch end: 225.29127049446106\n",
      "# of batch: 286\n",
      "train accuracy: 0.79470384\n",
      "One batch end: 226.11527061462402\n",
      "# of batch: 287\n",
      "train accuracy: 0.7952778\n",
      "One batch end: 226.93935632705688\n",
      "# of batch: 288\n",
      "train accuracy: 0.7957094\n",
      "One batch end: 227.80435943603516\n",
      "# of batch: 289\n",
      "train accuracy: 0.7961379\n",
      "One batch end: 228.665358543396\n",
      "# of batch: 290\n",
      "train accuracy: 0.79649484\n",
      "One batch end: 229.46735835075378\n",
      "# of batch: 291\n",
      "train accuracy: 0.7971233\n",
      "One batch end: 230.1833577156067\n",
      "# of batch: 292\n",
      "train accuracy: 0.7971331\n",
      "One batch end: 231.01935744285583\n",
      "# of batch: 293\n",
      "train accuracy: 0.79768705\n",
      "One batch end: 231.91835737228394\n",
      "# of batch: 294\n",
      "train accuracy: 0.79823726\n",
      "One batch end: 232.8302092552185\n",
      "# of batch: 295\n",
      "train accuracy: 0.79858106\n",
      "One batch end: 233.74520897865295\n",
      "# of batch: 296\n",
      "train accuracy: 0.79885525\n",
      "One batch end: 234.6547396183014\n",
      "# of batch: 297\n",
      "train accuracy: 0.7994631\n",
      "One batch end: 235.5037407875061\n",
      "# of batch: 298\n",
      "train accuracy: 0.7998662\n",
      "One batch end: 236.34626126289368\n",
      "# of batch: 299\n",
      "train accuracy: 0.8002\n",
      "One batch end: 237.0562605857849\n",
      "# of batch: 300\n",
      "train accuracy: 0.80039865\n",
      "One batch end: 237.8404221534729\n",
      "# of batch: 301\n",
      "train accuracy: 0.8007285\n",
      "One batch end: 238.54242181777954\n",
      "# of batch: 302\n",
      "train accuracy: 0.8011221\n",
      "One batch end: 239.2469425201416\n",
      "# of batch: 303\n",
      "train accuracy: 0.8013158\n",
      "One batch end: 239.94499969482422\n",
      "# of batch: 304\n",
      "train accuracy: 0.80137706\n",
      "One batch end: 240.74052095413208\n",
      "# of batch: 305\n",
      "train accuracy: 0.80189544\n",
      "One batch end: 241.45352005958557\n",
      "# of batch: 306\n",
      "train accuracy: 0.8023453\n",
      "One batch end: 242.16251945495605\n",
      "# of batch: 307\n",
      "train accuracy: 0.80285716\n",
      "One batch end: 242.86960315704346\n",
      "# of batch: 308\n",
      "train accuracy: 0.8031715\n",
      "One batch end: 243.66060256958008\n",
      "# of batch: 309\n",
      "train accuracy: 0.8035484\n",
      "One batch end: 244.4076018333435\n",
      "# of batch: 310\n",
      "train accuracy: 0.80405146\n",
      "One batch end: 245.10760116577148\n",
      "# of batch: 311\n",
      "train accuracy: 0.80435896\n",
      "One batch end: 245.82012248039246\n",
      "# of batch: 312\n",
      "train accuracy: 0.80479234\n",
      "One batch end: 246.52212238311768\n",
      "# of batch: 313\n",
      "train accuracy: 0.8052229\n",
      "One batch end: 247.24364304542542\n",
      "# of batch: 314\n",
      "train accuracy: 0.8057778\n",
      "One batch end: 247.96464252471924\n",
      "# of batch: 315\n",
      "train accuracy: 0.80632913\n",
      "One batch end: 248.694641828537\n",
      "# of batch: 316\n",
      "train accuracy: 0.80643535\n",
      "One batch end: 249.42864179611206\n",
      "# of batch: 317\n",
      "train accuracy: 0.80679244\n",
      "One batch end: 250.1346414089203\n",
      "# of batch: 318\n",
      "train accuracy: 0.8071473\n",
      "One batch end: 250.838641166687\n",
      "# of batch: 319\n",
      "train accuracy: 0.8075\n",
      "One batch end: 251.5501627922058\n",
      "# of batch: 320\n",
      "train accuracy: 0.80778813\n",
      "One batch end: 252.25868391990662\n",
      "# of batch: 321\n",
      "train accuracy: 0.80832297\n",
      "One batch end: 252.9657483100891\n",
      "# of batch: 322\n",
      "train accuracy: 0.8086068\n",
      "One batch end: 253.67779183387756\n",
      "# of batch: 323\n",
      "train accuracy: 0.8087654\n",
      "One batch end: 254.38084959983826\n",
      "# of batch: 324\n",
      "train accuracy: 0.8091077\n",
      "One batch end: 255.08392477035522\n",
      "# of batch: 325\n",
      "train accuracy: 0.8093865\n",
      "One batch end: 255.79792380332947\n",
      "# of batch: 326\n",
      "train accuracy: 0.80978596\n",
      "One batch end: 256.5079233646393\n",
      "# of batch: 327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.810061\n",
      "One batch end: 257.2130515575409\n",
      "# of batch: 328\n",
      "train accuracy: 0.8103343\n",
      "One batch end: 257.9575881958008\n",
      "# of batch: 329\n",
      "train accuracy: 0.81078786\n",
      "One batch end: 258.67319774627686\n",
      "# of batch: 330\n",
      "train accuracy: 0.8108157\n",
      "One batch end: 259.3908817768097\n",
      "# of batch: 331\n",
      "train accuracy: 0.8111446\n",
      "One batch end: 260.25196528434753\n",
      "# of batch: 332\n",
      "train accuracy: 0.81147146\n",
      "One batch end: 260.9584903717041\n",
      "# of batch: 333\n",
      "train accuracy: 0.8117365\n",
      "One batch end: 261.6684923171997\n",
      "# of batch: 334\n",
      "train accuracy: 0.8118806\n",
      "One batch end: 262.3744852542877\n",
      "# of batch: 335\n",
      "train accuracy: 0.8120238\n",
      "One batch end: 263.1860806941986\n",
      "# of batch: 336\n",
      "train accuracy: 0.81216615\n",
      "One batch end: 264.04660081863403\n",
      "# of batch: 337\n",
      "train accuracy: 0.8123077\n",
      "One batch end: 264.79059624671936\n",
      "# of batch: 338\n",
      "train accuracy: 0.81262535\n",
      "One batch end: 265.51060009002686\n",
      "# of batch: 339\n",
      "train accuracy: 0.813\n",
      "One batch end: 266.23759961128235\n",
      "# of batch: 340\n",
      "train accuracy: 0.8133138\n",
      "One batch end: 266.99659991264343\n",
      "# of batch: 341\n",
      "train accuracy: 0.8134503\n",
      "One batch end: 267.70059871673584\n",
      "# of batch: 342\n",
      "train accuracy: 0.8138775\n",
      "One batch end: 268.5031201839447\n",
      "# of batch: 343\n",
      "train accuracy: 0.81395346\n",
      "One batch end: 269.2814416885376\n",
      "# of batch: 344\n",
      "train accuracy: 0.81443477\n",
      "One batch end: 270.12620210647583\n",
      "# of batch: 345\n",
      "train accuracy: 0.81468207\n",
      "One batch end: 270.85120129585266\n",
      "# of batch: 346\n",
      "train accuracy: 0.8149856\n",
      "One batch end: 271.5591959953308\n",
      "# of batch: 347\n",
      "train accuracy: 0.81528735\n",
      "One batch end: 272.2752540111542\n",
      "# of batch: 348\n",
      "train accuracy: 0.8155874\n",
      "One batch end: 272.9997749328613\n",
      "# of batch: 349\n",
      "train accuracy: 0.81605715\n",
      "One batch end: 273.84877467155457\n",
      "# of batch: 350\n",
      "train accuracy: 0.8165812\n",
      "One batch end: 274.7197732925415\n",
      "# of batch: 351\n",
      "train accuracy: 0.816875\n",
      "One batch end: 275.57029914855957\n",
      "# of batch: 352\n",
      "train accuracy: 0.81716716\n",
      "One batch end: 276.49429845809937\n",
      "# of batch: 353\n",
      "train accuracy: 0.8174011\n",
      "One batch end: 277.43980646133423\n",
      "# of batch: 354\n",
      "train accuracy: 0.81752115\n",
      "One batch end: 278.2069163322449\n",
      "# of batch: 355\n",
      "train accuracy: 0.81775284\n",
      "One batch end: 279.1404230594635\n",
      "# of batch: 356\n",
      "train accuracy: 0.8179832\n",
      "One batch end: 279.93242263793945\n",
      "# of batch: 357\n",
      "train accuracy: 0.8179888\n",
      "One batch end: 280.77193331718445\n",
      "# of batch: 358\n",
      "train accuracy: 0.818273\n",
      "One batch end: 281.55093264579773\n",
      "# of batch: 359\n",
      "train accuracy: 0.81866664\n",
      "One batch end: 282.4054515361786\n",
      "# of batch: 360\n",
      "train accuracy: 0.8189474\n",
      "One batch end: 283.269837141037\n",
      "# of batch: 361\n",
      "train accuracy: 0.81917125\n",
      "One batch end: 284.052903175354\n",
      "# of batch: 362\n",
      "train accuracy: 0.819449\n",
      "One batch end: 284.82490277290344\n",
      "# of batch: 363\n",
      "train accuracy: 0.81978023\n",
      "One batch end: 285.5949022769928\n",
      "# of batch: 364\n",
      "train accuracy: 0.82021916\n",
      "One batch end: 286.36090183258057\n",
      "# of batch: 365\n",
      "train accuracy: 0.8206011\n",
      "One batch end: 287.1284236907959\n",
      "# of batch: 366\n",
      "train accuracy: 0.82103544\n",
      "One batch end: 287.89097476005554\n",
      "# of batch: 367\n",
      "train accuracy: 0.82119566\n",
      "One batch end: 288.6589741706848\n",
      "# of batch: 368\n",
      "train accuracy: 0.821355\n",
      "One batch end: 289.4291207790375\n",
      "# of batch: 369\n",
      "train accuracy: 0.8216216\n",
      "One batch end: 290.191641330719\n",
      "# of batch: 370\n",
      "train accuracy: 0.82177895\n",
      "One batch end: 290.95664072036743\n",
      "# of batch: 371\n",
      "train accuracy: 0.8221505\n",
      "One batch end: 291.7556400299072\n",
      "# of batch: 372\n",
      "train accuracy: 0.82235926\n",
      "One batch end: 292.5237441062927\n",
      "# of batch: 373\n",
      "train accuracy: 0.8226738\n",
      "One batch end: 293.2937436103821\n",
      "# of batch: 374\n",
      "train accuracy: 0.8229333\n",
      "One batch end: 294.07974314689636\n",
      "# of batch: 375\n",
      "train accuracy: 0.82319146\n",
      "One batch end: 294.8412518501282\n",
      "# of batch: 376\n",
      "train accuracy: 0.82350135\n",
      "One batch end: 295.607328414917\n",
      "# of batch: 377\n",
      "train accuracy: 0.82375664\n",
      "One batch end: 296.3713972568512\n",
      "# of batch: 378\n",
      "train accuracy: 0.8241161\n",
      "One batch end: 297.13747930526733\n",
      "# of batch: 379\n",
      "train accuracy: 0.82442105\n",
      "One batch end: 297.90847873687744\n",
      "# of batch: 380\n",
      "train accuracy: 0.8246719\n",
      "One batch end: 298.6780812740326\n",
      "# of batch: 381\n",
      "train accuracy: 0.8246073\n",
      "One batch end: 299.4431691169739\n",
      "# of batch: 382\n",
      "train accuracy: 0.824752\n",
      "One batch end: 300.2061679363251\n",
      "# of batch: 383\n",
      "train accuracy: 0.82484376\n",
      "One batch end: 300.9741668701172\n",
      "# of batch: 384\n",
      "train accuracy: 0.824987\n",
      "One batch end: 301.74016666412354\n",
      "# of batch: 385\n",
      "train accuracy: 0.8253368\n",
      "One batch end: 302.50916719436646\n",
      "# of batch: 386\n",
      "train accuracy: 0.8255297\n",
      "One batch end: 303.2721657752991\n",
      "# of batch: 387\n",
      "train accuracy: 0.825567\n",
      "One batch end: 304.03416538238525\n",
      "# of batch: 388\n",
      "train accuracy: 0.82586116\n",
      "One batch end: 304.80268001556396\n",
      "# of batch: 389\n",
      "train accuracy: 0.826\n",
      "One batch end: 305.5646798610687\n",
      "# of batch: 390\n",
      "train accuracy: 0.82608694\n",
      "One batch end: 306.3296790122986\n",
      "# of batch: 391\n",
      "train accuracy: 0.8262755\n",
      "One batch end: 307.2806787490845\n",
      "# of batch: 392\n",
      "train accuracy: 0.82671756\n",
      "One batch end: 308.04783368110657\n",
      "# of batch: 393\n",
      "train accuracy: 0.8270558\n",
      "One batch end: 308.81485652923584\n",
      "# of batch: 394\n",
      "train accuracy: 0.8272405\n",
      "One batch end: 309.58251547813416\n",
      "# of batch: 395\n",
      "train accuracy: 0.8276263\n",
      "One batch end: 310.35057950019836\n",
      "# of batch: 396\n",
      "train accuracy: 0.8279093\n",
      "One batch end: 311.11157870292664\n",
      "# of batch: 397\n",
      "train accuracy: 0.8280402\n",
      "One batch end: 311.87462544441223\n",
      "# of batch: 398\n",
      "train accuracy: 0.8281203\n",
      "One batch end: 312.6386249065399\n",
      "# of batch: 399\n",
      "train accuracy: 0.8284\n",
      "One batch end: 313.41062450408936\n",
      "# of batch: 400\n",
      "train accuracy: 0.8287282\n",
      "One batch end: 314.17966747283936\n",
      "# of batch: 401\n",
      "train accuracy: 0.82890546\n",
      "One batch end: 314.94720911979675\n",
      "# of batch: 402\n",
      "train accuracy: 0.8290819\n",
      "One batch end: 315.70920848846436\n",
      "# of batch: 403\n",
      "train accuracy: 0.82930696\n",
      "One batch end: 316.48020815849304\n",
      "# of batch: 404\n",
      "train accuracy: 0.8293827\n",
      "One batch end: 317.2482075691223\n",
      "# of batch: 405\n",
      "train accuracy: 0.82955664\n",
      "One batch end: 318.01720690727234\n",
      "# of batch: 406\n",
      "train accuracy: 0.82953316\n",
      "One batch end: 318.75325775146484\n",
      "# of batch: 407\n",
      "train accuracy: 0.82965684\n",
      "One batch end: 319.457257270813\n",
      "# of batch: 408\n",
      "train accuracy: 0.82982886\n",
      "One batch end: 320.16125679016113\n",
      "# of batch: 409\n",
      "train accuracy: 0.8301463\n",
      "One batch end: 320.8683376312256\n",
      "# of batch: 410\n",
      "train accuracy: 0.8305596\n",
      "One batch end: 321.5803802013397\n",
      "# of batch: 411\n",
      "train accuracy: 0.8307767\n",
      "One batch end: 322.286425113678\n",
      "# of batch: 412\n",
      "train accuracy: 0.83099276\n",
      "One batch end: 322.9894917011261\n",
      "# of batch: 413\n",
      "train accuracy: 0.83120775\n",
      "One batch end: 323.6925766468048\n",
      "# of batch: 414\n",
      "train accuracy: 0.8313253\n",
      "One batch end: 324.39357686042786\n",
      "# of batch: 415\n",
      "train accuracy: 0.8314423\n",
      "One batch end: 325.1006634235382\n",
      "# of batch: 416\n",
      "train accuracy: 0.8317506\n",
      "One batch end: 325.82295322418213\n",
      "# of batch: 417\n",
      "train accuracy: 0.83200955\n",
      "One batch end: 326.531183719635\n",
      "# of batch: 418\n",
      "train accuracy: 0.8324105\n",
      "One batch end: 327.2351830005646\n",
      "# of batch: 419\n",
      "train accuracy: 0.8327619\n",
      "One batch end: 327.9411826133728\n",
      "# of batch: 420\n",
      "train accuracy: 0.8329691\n",
      "One batch end: 328.6456985473633\n",
      "# of batch: 421\n",
      "train accuracy: 0.83317536\n",
      "One batch end: 329.3522958755493\n",
      "# of batch: 422\n",
      "train accuracy: 0.83356977\n",
      "One batch end: 330.0652952194214\n",
      "# of batch: 423\n",
      "train accuracy: 0.83367926\n",
      "One batch end: 330.774295091629\n",
      "# of batch: 424\n",
      "train accuracy: 0.83397645\n",
      "One batch end: 331.4802963733673\n",
      "# of batch: 425\n",
      "train accuracy: 0.8341315\n",
      "One batch end: 332.1862943172455\n",
      "# of batch: 426\n",
      "train accuracy: 0.83428574\n",
      "One batch end: 332.8948154449463\n",
      "# of batch: 427\n",
      "train accuracy: 0.83453274\n",
      "One batch end: 333.60581493377686\n",
      "# of batch: 428\n",
      "train accuracy: 0.83477855\n",
      "One batch end: 334.3128147125244\n",
      "# of batch: 429\n",
      "train accuracy: 0.83493024\n",
      "One batch end: 335.01833605766296\n",
      "# of batch: 430\n",
      "train accuracy: 0.8352204\n",
      "One batch end: 335.72239422798157\n",
      "# of batch: 431\n",
      "train accuracy: 0.83532405\n",
      "One batch end: 336.4263937473297\n",
      "# of batch: 432\n",
      "train accuracy: 0.835612\n",
      "One batch end: 337.1259150505066\n",
      "# of batch: 433\n",
      "train accuracy: 0.83589864\n",
      "One batch end: 337.83591508865356\n",
      "# of batch: 434\n",
      "train accuracy: 0.836046\n",
      "One batch end: 338.5419147014618\n",
      "# of batch: 435\n",
      "train accuracy: 0.83619267\n",
      "One batch end: 339.2459135055542\n",
      "# of batch: 436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.83647597\n",
      "One batch end: 339.95542454719543\n",
      "# of batch: 437\n",
      "train accuracy: 0.83684933\n",
      "One batch end: 340.6624240875244\n",
      "# of batch: 438\n",
      "train accuracy: 0.8370843\n",
      "One batch end: 341.3724238872528\n",
      "# of batch: 439\n",
      "train accuracy: 0.8373182\n",
      "One batch end: 342.0834228992462\n",
      "# of batch: 440\n",
      "train accuracy: 0.83746034\n",
      "One batch end: 342.79142236709595\n",
      "# of batch: 441\n",
      "train accuracy: 0.8377828\n",
      "One batch end: 343.5069432258606\n",
      "# of batch: 442\n",
      "train accuracy: 0.8379232\n",
      "One batch end: 344.23894357681274\n",
      "# of batch: 443\n",
      "train accuracy: 0.8381081\n",
      "One batch end: 344.9509425163269\n",
      "# of batch: 444\n",
      "train accuracy: 0.83833706\n",
      "One batch end: 345.6635401248932\n",
      "# of batch: 445\n",
      "train accuracy: 0.8386099\n",
      "One batch end: 346.3685395717621\n",
      "# of batch: 446\n",
      "train accuracy: 0.8387472\n",
      "One batch end: 347.09053897857666\n",
      "# of batch: 447\n",
      "train accuracy: 0.8389286\n",
      "One batch end: 347.8022356033325\n",
      "# of batch: 448\n",
      "train accuracy: 0.8391091\n",
      "One batch end: 348.51023530960083\n",
      "# of batch: 449\n",
      "train accuracy: 0.8392889\n",
      "One batch end: 349.2207567691803\n",
      "# of batch: 450\n",
      "train accuracy: 0.8393348\n",
      "One batch end: 349.9387559890747\n",
      "# of batch: 451\n",
      "train accuracy: 0.83960176\n",
      "One batch end: 350.64626240730286\n",
      "# of batch: 452\n",
      "train accuracy: 0.8397351\n",
      "One batch end: 351.3532621860504\n",
      "# of batch: 453\n",
      "train accuracy: 0.8398238\n",
      "One batch end: 352.064261674881\n",
      "# of batch: 454\n",
      "train accuracy: 0.8400879\n",
      "One batch end: 352.77626156806946\n",
      "# of batch: 455\n",
      "train accuracy: 0.84039474\n",
      "One batch end: 353.4853117465973\n",
      "# of batch: 456\n",
      "train accuracy: 0.8405689\n",
      "One batch end: 354.1963119506836\n",
      "# of batch: 457\n",
      "train accuracy: 0.84074235\n",
      "One batch end: 354.9053108692169\n",
      "# of batch: 458\n",
      "train accuracy: 0.8409586\n",
      "One batch end: 355.61531019210815\n",
      "# of batch: 459\n",
      "train accuracy: 0.84126085\n",
      "One batch end: 356.32230973243713\n",
      "# of batch: 460\n",
      "train accuracy: 0.8414317\n",
      "One batch end: 357.03430938720703\n",
      "# of batch: 461\n",
      "train accuracy: 0.8415151\n",
      "One batch end: 357.7443091869354\n",
      "# of batch: 462\n",
      "train accuracy: 0.8418143\n",
      "One batch end: 358.45330905914307\n",
      "# of batch: 463\n",
      "train accuracy: 0.8420259\n",
      "One batch end: 359.15730810165405\n",
      "# of batch: 464\n",
      "train accuracy: 0.84210753\n",
      "One batch end: 359.86841678619385\n",
      "# of batch: 465\n",
      "train accuracy: 0.84227467\n",
      "One batch end: 360.5755457878113\n",
      "# of batch: 466\n",
      "train accuracy: 0.84244114\n",
      "One batch end: 361.2815456390381\n",
      "# of batch: 467\n",
      "train accuracy: 0.8426496\n",
      "One batch end: 361.9940514564514\n",
      "# of batch: 468\n",
      "train accuracy: 0.8428145\n",
      "One batch end: 362.70505237579346\n",
      "# of batch: 469\n",
      "train accuracy: 0.8429787\n",
      "One batch end: 363.44505167007446\n",
      "# of batch: 470\n",
      "train accuracy: 0.84301484\n",
      "One batch end: 364.2122871875763\n",
      "# of batch: 471\n",
      "train accuracy: 0.8430932\n",
      "One batch end: 364.976286649704\n",
      "# of batch: 472\n",
      "train accuracy: 0.84317124\n",
      "One batch end: 365.741286277771\n",
      "# of batch: 473\n",
      "train accuracy: 0.8435021\n",
      "One batch end: 366.50336146354675\n",
      "# of batch: 474\n",
      "train accuracy: 0.84366316\n",
      "One batch end: 367.2674894332886\n",
      "# of batch: 475\n",
      "train accuracy: 0.84382355\n",
      "One batch end: 368.03699803352356\n",
      "# of batch: 476\n",
      "train accuracy: 0.844109\n",
      "One batch end: 368.80407643318176\n",
      "# of batch: 477\n",
      "train accuracy: 0.8443933\n",
      "One batch end: 369.57516145706177\n",
      "# of batch: 478\n",
      "train accuracy: 0.8445094\n",
      "One batch end: 370.3441619873047\n",
      "# of batch: 479\n",
      "train accuracy: 0.84479165\n",
      "One batch end: 371.1121606826782\n",
      "# of batch: 480\n",
      "train accuracy: 0.8450312\n",
      "One batch end: 371.8816819190979\n",
      "# of batch: 481\n",
      "train accuracy: 0.8453112\n",
      "One batch end: 372.6456823348999\n",
      "# of batch: 482\n",
      "train accuracy: 0.84550726\n",
      "One batch end: 373.4136805534363\n",
      "# of batch: 483\n",
      "train accuracy: 0.84561986\n",
      "One batch end: 374.1876800060272\n",
      "# of batch: 484\n",
      "train accuracy: 0.8458969\n",
      "One batch end: 374.95193362236023\n",
      "# of batch: 485\n",
      "train accuracy: 0.84617287\n",
      "One batch end: 375.7234890460968\n",
      "# of batch: 486\n",
      "train accuracy: 0.84632444\n",
      "One batch end: 376.4884383678436\n",
      "# of batch: 487\n",
      "train accuracy: 0.8465164\n",
      "One batch end: 377.2524380683899\n",
      "# of batch: 488\n",
      "train accuracy: 0.84683025\n",
      "One batch end: 378.02243757247925\n",
      "# of batch: 489\n",
      "train accuracy: 0.8470204\n",
      "One batch end: 378.7874321937561\n",
      "# of batch: 490\n",
      "train accuracy: 0.84712833\n",
      "One batch end: 379.55643582344055\n",
      "# of batch: 491\n",
      "train accuracy: 0.84727645\n",
      "One batch end: 380.3254351615906\n",
      "# of batch: 492\n",
      "train accuracy: 0.8475051\n",
      "One batch end: 381.0896339416504\n",
      "# of batch: 493\n",
      "train accuracy: 0.84777325\n",
      "One batch end: 381.86673069000244\n",
      "# of batch: 494\n",
      "train accuracy: 0.8478788\n",
      "One batch end: 382.63369035720825\n",
      "# of batch: 495\n",
      "train accuracy: 0.8481855\n",
      "One batch end: 383.3956892490387\n",
      "# of batch: 496\n",
      "train accuracy: 0.8483702\n",
      "One batch end: 384.1602101325989\n",
      "# of batch: 497\n",
      "train accuracy: 0.84839356\n",
      "One batch end: 384.9242219924927\n",
      "# of batch: 498\n",
      "train accuracy: 0.848497\n",
      "One batch end: 385.6987521648407\n",
      "# of batch: 499\n",
      "train accuracy: 0.84868\n",
      "One batch end: 386.4627513885498\n",
      "# of batch: 500\n",
      "train accuracy: 0.84882236\n",
      "One batch end: 387.22380018234253\n",
      "# of batch: 501\n",
      "train accuracy: 0.849004\n",
      "One batch end: 387.98783445358276\n",
      "# of batch: 502\n",
      "train accuracy: 0.8491451\n",
      "One batch end: 388.7479717731476\n",
      "# of batch: 503\n",
      "train accuracy: 0.84936506\n",
      "One batch end: 389.51105785369873\n",
      "# of batch: 504\n",
      "train accuracy: 0.84962374\n",
      "One batch end: 390.2793197631836\n",
      "# of batch: 505\n",
      "train accuracy: 0.84976286\n",
      "One batch end: 391.04031896591187\n",
      "# of batch: 506\n",
      "train accuracy: 0.84994084\n",
      "One batch end: 391.8070149421692\n",
      "# of batch: 507\n",
      "train accuracy: 0.8501575\n",
      "One batch end: 392.5800714492798\n",
      "# of batch: 508\n",
      "train accuracy: 0.850334\n",
      "One batch end: 393.344126701355\n",
      "# of batch: 509\n",
      "train accuracy: 0.85058826\n",
      "One batch end: 394.11312532424927\n",
      "# of batch: 510\n",
      "train accuracy: 0.8508806\n",
      "One batch end: 394.8761248588562\n",
      "# of batch: 511\n",
      "train accuracy: 0.8510156\n",
      "One batch end: 395.6409339904785\n",
      "# of batch: 512\n",
      "train accuracy: 0.8511891\n",
      "One batch end: 396.40456461906433\n",
      "# of batch: 513\n",
      "train accuracy: 0.8514397\n",
      "One batch end: 397.1735649108887\n",
      "# of batch: 514\n",
      "train accuracy: 0.8516117\n",
      "One batch end: 397.94061613082886\n",
      "# of batch: 515\n",
      "train accuracy: 0.8517829\n",
      "One batch end: 398.7047073841095\n",
      "# of batch: 516\n",
      "train accuracy: 0.8519149\n",
      "One batch end: 399.4707899093628\n",
      "# of batch: 517\n",
      "train accuracy: 0.85200775\n",
      "One batch end: 400.2337894439697\n",
      "# of batch: 518\n",
      "train accuracy: 0.8522158\n",
      "One batch end: 401.0013382434845\n",
      "# of batch: 519\n",
      "train accuracy: 0.8525\n",
      "One batch end: 401.77033853530884\n",
      "# of batch: 520\n",
      "train accuracy: 0.8525528\n",
      "One batch end: 402.5333375930786\n",
      "# of batch: 521\n",
      "train accuracy: 0.852682\n",
      "One batch end: 403.2989077568054\n",
      "# of batch: 522\n",
      "train accuracy: 0.852696\n",
      "One batch end: 404.0715026855469\n",
      "# of batch: 523\n",
      "train accuracy: 0.85278624\n",
      "One batch end: 404.8355016708374\n",
      "# of batch: 524\n",
      "train accuracy: 0.85295236\n",
      "One batch end: 405.59850096702576\n",
      "# of batch: 525\n",
      "train accuracy: 0.8530418\n",
      "One batch end: 406.36750054359436\n",
      "# of batch: 526\n",
      "train accuracy: 0.8532448\n",
      "One batch end: 407.0794999599457\n",
      "# of batch: 527\n",
      "train accuracy: 0.8533712\n",
      "One batch end: 407.7885048389435\n",
      "# of batch: 528\n",
      "train accuracy: 0.8535728\n",
      "One batch end: 408.50059843063354\n",
      "# of batch: 529\n",
      "train accuracy: 0.85369813\n",
      "One batch end: 409.2105987071991\n",
      "# of batch: 530\n",
      "train accuracy: 0.85378534\n",
      "One batch end: 409.9227156639099\n",
      "# of batch: 531\n",
      "train accuracy: 0.8538722\n",
      "One batch end: 410.6377639770508\n",
      "# of batch: 532\n",
      "train accuracy: 0.8539587\n",
      "One batch end: 411.34682846069336\n",
      "# of batch: 533\n",
      "train accuracy: 0.8540824\n",
      "One batch end: 412.0958869457245\n",
      "# of batch: 534\n",
      "train accuracy: 0.8543178\n",
      "One batch end: 412.8188855648041\n",
      "# of batch: 535\n",
      "train accuracy: 0.8544776\n",
      "One batch end: 413.53088545799255\n",
      "# of batch: 536\n",
      "train accuracy: 0.8546741\n",
      "One batch end: 414.25688457489014\n",
      "# of batch: 537\n",
      "train accuracy: 0.8549442\n",
      "One batch end: 414.96788454055786\n",
      "# of batch: 538\n",
      "train accuracy: 0.85513914\n",
      "One batch end: 415.67497277259827\n",
      "# of batch: 539\n",
      "train accuracy: 0.8552222\n",
      "One batch end: 416.383971452713\n",
      "# of batch: 540\n",
      "train accuracy: 0.85545284\n",
      "One batch end: 417.0959713459015\n",
      "# of batch: 541\n",
      "train accuracy: 0.8556458\n",
      "One batch end: 417.81097078323364\n",
      "# of batch: 542\n",
      "train accuracy: 0.85565376\n",
      "One batch end: 418.51596999168396\n",
      "# of batch: 543\n",
      "train accuracy: 0.85588235\n",
      "One batch end: 419.2265725135803\n",
      "# of batch: 544\n",
      "train accuracy: 0.8560734\n",
      "One batch end: 419.9355719089508\n",
      "# of batch: 545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8561905\n",
      "One batch end: 420.64757108688354\n",
      "# of batch: 546\n",
      "train accuracy: 0.85627055\n",
      "One batch end: 421.3535714149475\n",
      "# of batch: 547\n",
      "train accuracy: 0.85638684\n",
      "One batch end: 422.070570230484\n",
      "# of batch: 548\n",
      "train accuracy: 0.8565756\n",
      "One batch end: 422.7895655632019\n",
      "# of batch: 549\n",
      "train accuracy: 0.85676366\n",
      "One batch end: 423.4965693950653\n",
      "# of batch: 550\n",
      "train accuracy: 0.8569873\n",
      "One batch end: 424.2066149711609\n",
      "# of batch: 551\n",
      "train accuracy: 0.85721016\n",
      "One batch end: 424.9166696071625\n",
      "# of batch: 552\n",
      "train accuracy: 0.8573237\n",
      "One batch end: 425.624835729599\n",
      "# of batch: 553\n",
      "train accuracy: 0.85732853\n",
      "One batch end: 426.33083486557007\n",
      "# of batch: 554\n",
      "train accuracy: 0.8574775\n",
      "One batch end: 427.03983449935913\n",
      "# of batch: 555\n",
      "train accuracy: 0.8575899\n",
      "One batch end: 427.74435591697693\n",
      "# of batch: 556\n",
      "train accuracy: 0.8577379\n",
      "One batch end: 428.48571848869324\n",
      "# of batch: 557\n",
      "train accuracy: 0.8577778\n",
      "One batch end: 429.1952395439148\n",
      "# of batch: 558\n",
      "train accuracy: 0.85796064\n",
      "One batch end: 429.9072937965393\n",
      "# of batch: 559\n",
      "train accuracy: 0.85807145\n",
      "One batch end: 430.6142933368683\n",
      "# of batch: 560\n",
      "train accuracy: 0.85828876\n",
      "One batch end: 431.32231736183167\n",
      "# of batch: 561\n",
      "train accuracy: 0.8584697\n",
      "One batch end: 432.0358238220215\n",
      "# of batch: 562\n",
      "train accuracy: 0.8586501\n",
      "One batch end: 432.7478232383728\n",
      "# of batch: 563\n",
      "train accuracy: 0.85879433\n",
      "One batch end: 433.45487809181213\n",
      "# of batch: 564\n",
      "train accuracy: 0.8588672\n",
      "One batch end: 434.17087745666504\n",
      "# of batch: 565\n",
      "train accuracy: 0.85908127\n",
      "One batch end: 434.8779377937317\n",
      "# of batch: 566\n",
      "train accuracy: 0.85918874\n",
      "One batch end: 435.5839366912842\n",
      "# of batch: 567\n",
      "train accuracy: 0.8592958\n",
      "One batch end: 436.28793692588806\n",
      "# of batch: 568\n",
      "train accuracy: 0.8594376\n",
      "One batch end: 436.9940013885498\n",
      "# of batch: 569\n",
      "train accuracy: 0.85950875\n",
      "One batch end: 437.69811749458313\n",
      "# of batch: 570\n",
      "train accuracy: 0.85961473\n",
      "One batch end: 438.4046609401703\n",
      "# of batch: 571\n",
      "train accuracy: 0.8598601\n",
      "One batch end: 439.11366081237793\n",
      "# of batch: 572\n",
      "train accuracy: 0.8598953\n",
      "One batch end: 439.82574462890625\n",
      "# of batch: 573\n",
      "train accuracy: 0.8601045\n",
      "One batch end: 440.541344165802\n",
      "# of batch: 574\n",
      "train accuracy: 0.86017394\n",
      "One batch end: 441.255343914032\n",
      "# of batch: 575\n",
      "train accuracy: 0.8602431\n",
      "One batch end: 441.97286462783813\n",
      "# of batch: 576\n",
      "train accuracy: 0.86041594\n",
      "One batch end: 442.68545961380005\n",
      "# of batch: 577\n",
      "train accuracy: 0.86058825\n",
      "One batch end: 443.3964686393738\n",
      "# of batch: 578\n",
      "train accuracy: 0.8607945\n",
      "One batch end: 444.10346841812134\n",
      "# of batch: 579\n",
      "train accuracy: 0.861\n",
      "One batch end: 444.81346893310547\n",
      "# of batch: 580\n",
      "train accuracy: 0.8612048\n",
      "One batch end: 445.5199964046478\n",
      "# of batch: 581\n",
      "train accuracy: 0.86137456\n",
      "One batch end: 446.23299169540405\n",
      "# of batch: 582\n",
      "train accuracy: 0.86157805\n",
      "One batch end: 446.9439949989319\n",
      "# of batch: 583\n",
      "train accuracy: 0.86167806\n",
      "One batch end: 447.6549949645996\n",
      "# of batch: 584\n",
      "train accuracy: 0.8617778\n",
      "One batch end: 448.3659942150116\n",
      "# of batch: 585\n",
      "train accuracy: 0.86197954\n",
      "One batch end: 449.07958340644836\n",
      "# of batch: 586\n",
      "train accuracy: 0.8621465\n",
      "One batch end: 449.79484033584595\n",
      "# of batch: 587\n",
      "train accuracy: 0.86227894\n",
      "One batch end: 450.503839969635\n",
      "# of batch: 588\n",
      "train accuracy: 0.8624448\n",
      "One batch end: 451.2264347076416\n",
      "# of batch: 589\n",
      "train accuracy: 0.862678\n",
      "One batch end: 451.9924883842468\n",
      "# of batch: 590\n",
      "train accuracy: 0.8628088\n",
      "One batch end: 452.7575829029083\n",
      "# of batch: 591\n",
      "train accuracy: 0.8629054\n",
      "One batch end: 453.5245826244354\n",
      "# of batch: 592\n",
      "train accuracy: 0.8631366\n",
      "One batch end: 454.2995820045471\n",
      "# of batch: 593\n",
      "train accuracy: 0.86329967\n",
      "One batch end: 455.0725815296173\n",
      "# of batch: 594\n",
      "train accuracy: 0.8634286\n",
      "One batch end: 455.84162282943726\n",
      "# of batch: 595\n",
      "train accuracy: 0.8635906\n",
      "One batch end: 456.61762166023254\n",
      "# of batch: 596\n",
      "train accuracy: 0.86375207\n",
      "One batch end: 457.3887574672699\n",
      "# of batch: 597\n",
      "train accuracy: 0.8638796\n",
      "One batch end: 458.15875792503357\n",
      "# of batch: 598\n",
      "train accuracy: 0.8640401\n",
      "One batch end: 458.9237561225891\n",
      "# of batch: 599\n",
      "train accuracy: 0.8642333\n",
      "One batch end: 459.68775606155396\n",
      "# of batch: 600\n",
      "train accuracy: 0.8643261\n",
      "One batch end: 460.45427656173706\n",
      "# of batch: 601\n",
      "train accuracy: 0.86438537\n",
      "One batch end: 461.2201979160309\n",
      "# of batch: 602\n",
      "train accuracy: 0.86447763\n",
      "One batch end: 461.98670768737793\n",
      "# of batch: 603\n",
      "train accuracy: 0.86466885\n",
      "One batch end: 462.75122928619385\n",
      "# of batch: 604\n",
      "train accuracy: 0.86472726\n",
      "One batch end: 463.52234172821045\n",
      "# of batch: 605\n",
      "train accuracy: 0.86481845\n",
      "One batch end: 464.28934144973755\n",
      "# of batch: 606\n",
      "train accuracy: 0.86487645\n",
      "One batch end: 465.0596046447754\n",
      "# of batch: 607\n",
      "train accuracy: 0.8649671\n",
      "One batch end: 465.82877111434937\n",
      "# of batch: 608\n",
      "train accuracy: 0.8649918\n",
      "One batch end: 466.59877133369446\n",
      "# of batch: 609\n",
      "train accuracy: 0.86514753\n",
      "One batch end: 467.36684107780457\n",
      "# of batch: 610\n",
      "train accuracy: 0.8652701\n",
      "One batch end: 468.13863372802734\n",
      "# of batch: 611\n",
      "train accuracy: 0.8654902\n",
      "One batch end: 468.90863394737244\n",
      "# of batch: 612\n",
      "train accuracy: 0.865677\n",
      "One batch end: 469.67863368988037\n",
      "# of batch: 613\n",
      "train accuracy: 0.8658632\n",
      "One batch end: 470.44663190841675\n",
      "# of batch: 614\n",
      "train accuracy: 0.86601627\n",
      "One batch end: 471.21055459976196\n",
      "# of batch: 615\n",
      "train accuracy: 0.8661039\n",
      "One batch end: 471.9745502471924\n",
      "# of batch: 616\n",
      "train accuracy: 0.866094\n",
      "One batch end: 472.74370527267456\n",
      "# of batch: 617\n",
      "train accuracy: 0.8662136\n",
      "One batch end: 473.5067148208618\n",
      "# of batch: 618\n",
      "train accuracy: 0.86636513\n",
      "One batch end: 474.27171516418457\n",
      "# of batch: 619\n",
      "train accuracy: 0.8664516\n",
      "One batch end: 475.03871393203735\n",
      "# of batch: 620\n",
      "train accuracy: 0.8665056\n",
      "One batch end: 475.80823373794556\n",
      "# of batch: 621\n",
      "train accuracy: 0.8665595\n",
      "One batch end: 476.57323384284973\n",
      "# of batch: 622\n",
      "train accuracy: 0.8667095\n",
      "One batch end: 477.33623242378235\n",
      "# of batch: 623\n",
      "train accuracy: 0.8669231\n",
      "One batch end: 478.1042318344116\n",
      "# of batch: 624\n",
      "train accuracy: 0.86704\n",
      "One batch end: 478.8722324371338\n",
      "# of batch: 625\n",
      "train accuracy: 0.8671885\n",
      "One batch end: 479.6382312774658\n",
      "# of batch: 626\n",
      "train accuracy: 0.8673046\n",
      "One batch end: 480.4022305011749\n",
      "# of batch: 627\n",
      "train accuracy: 0.8674522\n",
      "One batch end: 481.16575145721436\n",
      "# of batch: 628\n",
      "train accuracy: 0.86756754\n",
      "One batch end: 481.9317512512207\n",
      "# of batch: 629\n",
      "train accuracy: 0.8677143\n",
      "One batch end: 482.69975113868713\n",
      "# of batch: 630\n",
      "train accuracy: 0.86779714\n",
      "One batch end: 483.46375036239624\n",
      "# of batch: 631\n",
      "train accuracy: 0.86787975\n",
      "One batch end: 484.2247495651245\n",
      "# of batch: 632\n",
      "train accuracy: 0.86799365\n",
      "One batch end: 484.99183416366577\n",
      "# of batch: 633\n",
      "train accuracy: 0.8681703\n",
      "One batch end: 485.7519631385803\n",
      "# of batch: 634\n",
      "train accuracy: 0.86828345\n",
      "One batch end: 486.5255627632141\n",
      "# of batch: 635\n",
      "train accuracy: 0.8684277\n",
      "One batch end: 487.29407143592834\n",
      "# of batch: 636\n",
      "train accuracy: 0.86854005\n",
      "One batch end: 488.0640711784363\n",
      "# of batch: 637\n",
      "train accuracy: 0.86871475\n",
      "One batch end: 488.842577457428\n",
      "# of batch: 638\n",
      "train accuracy: 0.86885756\n",
      "One batch end: 489.61457681655884\n",
      "# of batch: 639\n",
      "train accuracy: 0.8689687\n",
      "One batch end: 490.38157629966736\n",
      "# of batch: 640\n",
      "train accuracy: 0.8690796\n",
      "One batch end: 491.15499353408813\n",
      "# of batch: 641\n",
      "train accuracy: 0.86912775\n",
      "One batch end: 491.9271352291107\n",
      "# of batch: 642\n",
      "train accuracy: 0.86920685\n",
      "One batch end: 492.691134929657\n",
      "# of batch: 643\n",
      "train accuracy: 0.86937886\n",
      "One batch end: 493.45318126678467\n",
      "# of batch: 644\n",
      "train accuracy: 0.8693954\n",
      "One batch end: 494.22618079185486\n",
      "# of batch: 645\n",
      "train accuracy: 0.86950463\n",
      "One batch end: 494.96418046951294\n",
      "# of batch: 646\n",
      "train accuracy: 0.8696754\n",
      "One batch end: 495.6681799888611\n",
      "# of batch: 647\n",
      "train accuracy: 0.8698148\n",
      "One batch end: 496.37426137924194\n",
      "# of batch: 648\n",
      "train accuracy: 0.86998457\n",
      "One batch end: 497.0842614173889\n",
      "# of batch: 649\n",
      "train accuracy: 0.87\n",
      "One batch end: 497.7912607192993\n",
      "# of batch: 650\n",
      "train accuracy: 0.8700154\n",
      "One batch end: 498.5009684562683\n",
      "# of batch: 651\n",
      "train accuracy: 0.87015337\n",
      "One batch end: 499.2059669494629\n",
      "# of batch: 652\n",
      "train accuracy: 0.87026036\n",
      "One batch end: 499.9139678478241\n",
      "# of batch: 653\n",
      "train accuracy: 0.87039757\n",
      "One batch end: 500.61796617507935\n",
      "# of batch: 654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.87053436\n",
      "One batch end: 501.3240530490875\n",
      "# of batch: 655\n",
      "train accuracy: 0.87067074\n",
      "One batch end: 502.0360531806946\n",
      "# of batch: 656\n",
      "train accuracy: 0.87077624\n",
      "One batch end: 502.7435736656189\n",
      "# of batch: 657\n",
      "train accuracy: 0.87091184\n",
      "One batch end: 503.44857358932495\n",
      "# of batch: 658\n",
      "train accuracy: 0.870956\n",
      "One batch end: 504.1565728187561\n",
      "# of batch: 659\n",
      "train accuracy: 0.87103033\n",
      "One batch end: 504.86457228660583\n",
      "# of batch: 660\n",
      "train accuracy: 0.87104386\n",
      "One batch end: 505.57113456726074\n",
      "# of batch: 661\n",
      "train accuracy: 0.87114805\n",
      "One batch end: 506.2821674346924\n",
      "# of batch: 662\n",
      "train accuracy: 0.8712519\n",
      "One batch end: 506.99321150779724\n",
      "# of batch: 663\n",
      "train accuracy: 0.8713855\n",
      "One batch end: 507.7032108306885\n",
      "# of batch: 664\n",
      "train accuracy: 0.8715188\n",
      "One batch end: 508.4102113246918\n",
      "# of batch: 665\n",
      "train accuracy: 0.87165165\n",
      "One batch end: 509.1182110309601\n",
      "# of batch: 666\n",
      "train accuracy: 0.8717241\n",
      "One batch end: 509.82635617256165\n",
      "# of batch: 667\n",
      "train accuracy: 0.8718563\n",
      "One batch end: 510.5343565940857\n",
      "# of batch: 668\n",
      "train accuracy: 0.8719283\n",
      "One batch end: 511.2449355125427\n",
      "# of batch: 669\n",
      "train accuracy: 0.8719702\n",
      "One batch end: 511.9529845714569\n",
      "# of batch: 670\n",
      "train accuracy: 0.872161\n",
      "One batch end: 512.6599838733673\n",
      "# of batch: 671\n",
      "train accuracy: 0.8722917\n",
      "One batch end: 513.3639838695526\n",
      "# of batch: 672\n",
      "train accuracy: 0.87236255\n",
      "One batch end: 514.0790934562683\n",
      "# of batch: 673\n",
      "train accuracy: 0.87249255\n",
      "One batch end: 514.7920928001404\n",
      "# of batch: 674\n",
      "train accuracy: 0.8724741\n",
      "One batch end: 515.4986145496368\n",
      "# of batch: 675\n",
      "train accuracy: 0.872574\n",
      "One batch end: 516.2082171440125\n",
      "# of batch: 676\n",
      "train accuracy: 0.8727031\n",
      "One batch end: 516.9182169437408\n",
      "# of batch: 677\n",
      "train accuracy: 0.87289083\n",
      "One batch end: 517.6282162666321\n",
      "# of batch: 678\n",
      "train accuracy: 0.87301916\n",
      "One batch end: 518.339736700058\n",
      "# of batch: 679\n",
      "train accuracy: 0.87308824\n",
      "One batch end: 519.0537362098694\n",
      "# of batch: 680\n",
      "train accuracy: 0.87324524\n",
      "One batch end: 519.7617318630219\n",
      "# of batch: 681\n",
      "train accuracy: 0.8732258\n",
      "One batch end: 520.4697351455688\n",
      "# of batch: 682\n",
      "train accuracy: 0.8731772\n",
      "One batch end: 521.180341720581\n",
      "# of batch: 683\n",
      "train accuracy: 0.8731871\n",
      "One batch end: 521.8933413028717\n",
      "# of batch: 684\n",
      "train accuracy: 0.8732555\n",
      "One batch end: 522.6013414859772\n",
      "# of batch: 685\n",
      "train accuracy: 0.87341106\n",
      "One batch end: 523.3098616600037\n",
      "# of batch: 686\n",
      "train accuracy: 0.8735371\n",
      "One batch end: 524.0238609313965\n",
      "# of batch: 687\n",
      "train accuracy: 0.87360466\n",
      "One batch end: 524.7312371730804\n",
      "# of batch: 688\n",
      "train accuracy: 0.87373006\n",
      "One batch end: 525.5152406692505\n",
      "# of batch: 689\n",
      "train accuracy: 0.87371016\n",
      "One batch end: 526.2842407226562\n",
      "# of batch: 690\n",
      "train accuracy: 0.8737482\n",
      "One batch end: 527.0137627124786\n",
      "# of batch: 691\n",
      "train accuracy: 0.8738439\n",
      "One batch end: 527.7317624092102\n",
      "# of batch: 692\n",
      "train accuracy: 0.87396824\n",
      "One batch end: 528.4402849674225\n",
      "# of batch: 693\n",
      "train accuracy: 0.8740922\n",
      "One batch end: 529.149284362793\n",
      "# of batch: 694\n",
      "train accuracy: 0.87407196\n",
      "One batch end: 529.8562841415405\n",
      "# of batch: 695\n",
      "train accuracy: 0.87413794\n",
      "One batch end: 530.5718054771423\n",
      "# of batch: 696\n",
      "train accuracy: 0.87420374\n",
      "One batch end: 531.2810015678406\n",
      "# of batch: 697\n",
      "train accuracy: 0.87426937\n",
      "One batch end: 532.017523765564\n",
      "# of batch: 698\n",
      "train accuracy: 0.87436336\n",
      "One batch end: 532.740522146225\n",
      "# of batch: 699\n",
      "train accuracy: 0.8744571\n",
      "One batch end: 533.4485216140747\n",
      "# of batch: 700\n",
      "train accuracy: 0.87446505\n",
      "One batch end: 534.1795213222504\n",
      "# of batch: 701\n",
      "train accuracy: 0.8745869\n",
      "One batch end: 534.8960554599762\n",
      "# of batch: 702\n",
      "train accuracy: 0.87459457\n",
      "One batch end: 535.6070549488068\n",
      "# of batch: 703\n",
      "train accuracy: 0.87465906\n",
      "One batch end: 536.3196318149567\n",
      "# of batch: 704\n",
      "train accuracy: 0.87475175\n",
      "One batch end: 537.0321521759033\n",
      "# of batch: 705\n",
      "train accuracy: 0.8749009\n",
      "One batch end: 537.7376730442047\n",
      "# of batch: 706\n",
      "train accuracy: 0.8749929\n",
      "One batch end: 538.4486727714539\n",
      "# of batch: 707\n",
      "train accuracy: 0.875\n",
      "One batch end: 539.1556723117828\n",
      "# of batch: 708\n",
      "train accuracy: 0.8750917\n",
      "One batch end: 539.9251892566681\n",
      "# of batch: 709\n",
      "train accuracy: 0.8752676\n",
      "One batch end: 540.723192691803\n",
      "# of batch: 710\n",
      "train accuracy: 0.8754149\n",
      "One batch end: 541.5101878643036\n",
      "# of batch: 711\n",
      "train accuracy: 0.8755056\n",
      "One batch end: 542.31418800354\n",
      "# of batch: 712\n",
      "train accuracy: 0.8756522\n",
      "One batch end: 543.1161870956421\n",
      "# of batch: 713\n",
      "train accuracy: 0.8757423\n",
      "One batch end: 543.9001863002777\n",
      "# of batch: 714\n",
      "train accuracy: 0.8758881\n",
      "One batch end: 544.7041854858398\n",
      "# of batch: 715\n",
      "train accuracy: 0.8760056\n",
      "One batch end: 545.493189573288\n",
      "# of batch: 716\n",
      "train accuracy: 0.8761506\n",
      "One batch end: 546.2741899490356\n",
      "# of batch: 717\n",
      "train accuracy: 0.8762117\n",
      "One batch end: 547.052188873291\n",
      "# of batch: 718\n",
      "train accuracy: 0.87621695\n",
      "One batch end: 547.832186460495\n",
      "# of batch: 719\n",
      "train accuracy: 0.8763889\n",
      "One batch end: 548.6181831359863\n",
      "# of batch: 720\n",
      "train accuracy: 0.8765326\n",
      "One batch end: 549.4266970157623\n",
      "# of batch: 721\n",
      "train accuracy: 0.8766759\n",
      "One batch end: 550.2376959323883\n",
      "# of batch: 722\n",
      "train accuracy: 0.8767911\n",
      "One batch end: 551.0546956062317\n",
      "# of batch: 723\n",
      "train accuracy: 0.8769337\n",
      "One batch end: 551.8666951656342\n",
      "# of batch: 724\n",
      "train accuracy: 0.87704825\n",
      "One batch end: 552.6597034931183\n",
      "# of batch: 725\n",
      "train accuracy: 0.87719005\n",
      "One batch end: 553.4627079963684\n",
      "# of batch: 726\n",
      "train accuracy: 0.87724894\n",
      "One batch end: 554.250702381134\n",
      "# of batch: 727\n",
      "train accuracy: 0.8773352\n",
      "One batch end: 555.0387020111084\n",
      "# of batch: 728\n",
      "train accuracy: 0.87744856\n",
      "One batch end: 555.8287055492401\n",
      "# of batch: 729\n",
      "train accuracy: 0.8775343\n",
      "One batch end: 556.6172225475311\n",
      "# of batch: 730\n",
      "train accuracy: 0.8776197\n",
      "One batch end: 557.3992266654968\n",
      "# of batch: 731\n",
      "train accuracy: 0.8777596\n",
      "One batch end: 558.1865854263306\n",
      "# of batch: 732\n",
      "train accuracy: 0.87784445\n",
      "One batch end: 558.978598356247\n",
      "# of batch: 733\n",
      "train accuracy: 0.8779836\n",
      "One batch end: 559.765597820282\n",
      "# of batch: 734\n",
      "train accuracy: 0.87806803\n",
      "One batch end: 560.5535976886749\n",
      "# of batch: 735\n",
      "train accuracy: 0.87807065\n",
      "One batch end: 561.3415968418121\n",
      "# of batch: 736\n",
      "train accuracy: 0.87820894\n",
      "One batch end: 562.1255919933319\n",
      "# of batch: 737\n",
      "train accuracy: 0.8783198\n",
      "One batch end: 562.9157102108002\n",
      "# of batch: 738\n",
      "train accuracy: 0.87845737\n",
      "One batch end: 563.7097096443176\n",
      "# of batch: 739\n",
      "train accuracy: 0.8785405\n",
      "One batch end: 564.4911499023438\n",
      "# of batch: 740\n",
      "train accuracy: 0.8786235\n",
      "One batch end: 565.2781488895416\n",
      "# of batch: 741\n",
      "train accuracy: 0.8786792\n",
      "One batch end: 566.0616703033447\n",
      "# of batch: 742\n",
      "train accuracy: 0.8787887\n",
      "One batch end: 566.8481910228729\n",
      "# of batch: 743\n",
      "train accuracy: 0.8789247\n",
      "One batch end: 567.6431903839111\n",
      "# of batch: 744\n",
      "train accuracy: 0.87895304\n",
      "One batch end: 568.4291903972626\n",
      "# of batch: 745\n",
      "train accuracy: 0.87898123\n",
      "One batch end: 569.227189540863\n",
      "# of batch: 746\n",
      "train accuracy: 0.8790897\n",
      "One batch end: 570.0151891708374\n",
      "# of batch: 747\n",
      "train accuracy: 0.8792246\n",
      "One batch end: 570.8025186061859\n",
      "# of batch: 748\n",
      "train accuracy: 0.879279\n",
      "One batch end: 571.5815625190735\n",
      "# of batch: 749\n",
      "train accuracy: 0.87928\n",
      "One batch end: 572.365565776825\n",
      "# of batch: 750\n",
      "train accuracy: 0.8793875\n",
      "One batch end: 573.1185657978058\n",
      "# of batch: 751\n",
      "train accuracy: 0.8794415\n",
      "One batch end: 573.90656042099\n",
      "# of batch: 752\n",
      "train accuracy: 0.8794953\n",
      "One batch end: 574.6915600299835\n",
      "# of batch: 753\n",
      "train accuracy: 0.87949604\n",
      "One batch end: 575.4715640544891\n",
      "# of batch: 754\n",
      "train accuracy: 0.87957615\n",
      "One batch end: 576.2535631656647\n",
      "# of batch: 755\n",
      "train accuracy: 0.8796296\n",
      "One batch end: 577.0507879257202\n",
      "# of batch: 756\n",
      "train accuracy: 0.87976223\n",
      "One batch end: 577.8867859840393\n",
      "# of batch: 757\n",
      "train accuracy: 0.8798681\n",
      "One batch end: 578.7517855167389\n",
      "# of batch: 758\n",
      "train accuracy: 0.8799473\n",
      "One batch end: 579.5667893886566\n",
      "# of batch: 759\n",
      "train accuracy: 0.8800526\n",
      "One batch end: 580.3582315444946\n",
      "# of batch: 760\n",
      "train accuracy: 0.8801314\n",
      "One batch end: 581.1453900337219\n",
      "# of batch: 761\n",
      "train accuracy: 0.8801575\n",
      "One batch end: 581.9123892784119\n",
      "# of batch: 762\n",
      "train accuracy: 0.88028836\n",
      "One batch end: 582.7026703357697\n",
      "# of batch: 763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.880445\n",
      "One batch end: 583.4891874790192\n",
      "# of batch: 764\n",
      "train accuracy: 0.88052285\n",
      "One batch end: 584.2917122840881\n",
      "# of batch: 765\n",
      "train accuracy: 0.8806527\n",
      "One batch end: 585.0497121810913\n",
      "# of batch: 766\n",
      "train accuracy: 0.88078225\n",
      "One batch end: 585.7847113609314\n",
      "# of batch: 767\n",
      "train accuracy: 0.8809375\n",
      "One batch end: 586.5275635719299\n",
      "# of batch: 768\n",
      "train accuracy: 0.8809883\n",
      "One batch end: 587.2650856971741\n",
      "# of batch: 769\n",
      "train accuracy: 0.88106495\n",
      "One batch end: 588.0290842056274\n",
      "# of batch: 770\n",
      "train accuracy: 0.88114136\n",
      "One batch end: 588.7830836772919\n",
      "# of batch: 771\n",
      "train accuracy: 0.8812435\n",
      "One batch end: 589.5235981941223\n",
      "# of batch: 772\n",
      "train accuracy: 0.8813454\n",
      "One batch end: 590.2625935077667\n",
      "# of batch: 773\n",
      "train accuracy: 0.8814212\n",
      "One batch end: 591.0075969696045\n",
      "# of batch: 774\n",
      "train accuracy: 0.8815484\n",
      "One batch end: 591.7405970096588\n",
      "# of batch: 775\n",
      "train accuracy: 0.8816495\n",
      "One batch end: 592.480596780777\n",
      "# of batch: 776\n",
      "train accuracy: 0.8817246\n",
      "One batch end: 593.2105967998505\n",
      "# of batch: 777\n",
      "train accuracy: 0.88169664\n",
      "One batch end: 593.9415912628174\n",
      "# of batch: 778\n",
      "train accuracy: 0.8818228\n",
      "One batch end: 594.6759350299835\n",
      "# of batch: 779\n",
      "train accuracy: 0.88189745\n",
      "One batch end: 595.4139301776886\n",
      "# of batch: 780\n",
      "train accuracy: 0.88202304\n",
      "One batch end: 596.1569306850433\n",
      "# of batch: 781\n",
      "train accuracy: 0.88212276\n",
      "One batch end: 596.8840177059174\n",
      "# of batch: 782\n",
      "train accuracy: 0.88224775\n",
      "One batch end: 597.6515426635742\n",
      "# of batch: 783\n",
      "train accuracy: 0.8823214\n",
      "One batch end: 598.3930599689484\n",
      "# of batch: 784\n",
      "train accuracy: 0.88242036\n",
      "One batch end: 599.1240637302399\n",
      "# of batch: 785\n",
      "train accuracy: 0.88251907\n",
      "One batch end: 599.857063293457\n",
      "# of batch: 786\n",
      "train accuracy: 0.88261753\n",
      "One batch end: 600.5950629711151\n",
      "# of batch: 787\n",
      "train accuracy: 0.882665\n",
      "One batch end: 601.327062368393\n",
      "# of batch: 788\n",
      "train accuracy: 0.88278836\n",
      "One batch end: 602.0590622425079\n",
      "# of batch: 789\n",
      "train accuracy: 0.8827848\n",
      "One batch end: 602.8005831241608\n",
      "# of batch: 790\n",
      "train accuracy: 0.88285714\n",
      "One batch end: 603.5307068824768\n",
      "# of batch: 791\n",
      "train accuracy: 0.88290405\n",
      "One batch end: 604.2672290802002\n",
      "# of batch: 792\n",
      "train accuracy: 0.88295084\n",
      "One batch end: 605.0087492465973\n",
      "# of batch: 793\n",
      "train accuracy: 0.88299745\n",
      "One batch end: 605.7357497215271\n",
      "# of batch: 794\n",
      "train accuracy: 0.88306916\n",
      "One batch end: 606.4997494220734\n",
      "# of batch: 795\n",
      "train accuracy: 0.88316584\n",
      "One batch end: 607.2577483654022\n",
      "# of batch: 796\n",
      "train accuracy: 0.8832371\n",
      "One batch end: 608.0042541027069\n",
      "# of batch: 797\n",
      "train accuracy: 0.8833333\n",
      "One batch end: 608.7462539672852\n",
      "# of batch: 798\n",
      "train accuracy: 0.8833792\n",
      "One batch end: 609.4824137687683\n",
      "# of batch: 799\n",
      "train accuracy: 0.8835\n",
      "One batch end: 610.222418308258\n",
      "# of batch: 800\n",
      "train accuracy: 0.8836454\n",
      "One batch end: 610.9664127826691\n",
      "# of batch: 801\n",
      "train accuracy: 0.8836908\n",
      "One batch end: 611.7033560276031\n",
      "# of batch: 802\n",
      "train accuracy: 0.8837858\n",
      "One batch end: 612.4363603591919\n",
      "# of batch: 803\n",
      "train accuracy: 0.8838806\n",
      "One batch end: 613.1778769493103\n",
      "# of batch: 804\n",
      "train accuracy: 0.884\n",
      "One batch end: 613.9314024448395\n",
      "# of batch: 805\n",
      "train accuracy: 0.8840943\n",
      "One batch end: 614.6733980178833\n",
      "# of batch: 806\n",
      "train accuracy: 0.884114\n",
      "One batch end: 615.412401676178\n",
      "# of batch: 807\n",
      "train accuracy: 0.8842327\n",
      "One batch end: 616.1483969688416\n",
      "# of batch: 808\n",
      "train accuracy: 0.88432634\n",
      "One batch end: 616.8919155597687\n",
      "# of batch: 809\n",
      "train accuracy: 0.88441974\n",
      "One batch end: 617.6384222507477\n",
      "# of batch: 810\n",
      "train accuracy: 0.8844636\n",
      "One batch end: 618.3764178752899\n",
      "# of batch: 811\n",
      "train accuracy: 0.8846059\n",
      "One batch end: 619.1239285469055\n",
      "# of batch: 812\n",
      "train accuracy: 0.8846741\n",
      "One batch end: 619.858922958374\n",
      "# of batch: 813\n",
      "train accuracy: 0.88479114\n",
      "One batch end: 620.5994431972504\n",
      "# of batch: 814\n",
      "train accuracy: 0.8848098\n",
      "One batch end: 621.3434474468231\n",
      "# of batch: 815\n",
      "train accuracy: 0.88490194\n",
      "One batch end: 622.077446937561\n",
      "# of batch: 816\n",
      "train accuracy: 0.8849204\n",
      "One batch end: 622.8164472579956\n",
      "# of batch: 817\n",
      "train accuracy: 0.88496333\n",
      "One batch end: 623.5594437122345\n",
      "# of batch: 818\n",
      "train accuracy: 0.88505495\n",
      "One batch end: 624.3006975650787\n",
      "# of batch: 819\n",
      "train accuracy: 0.88512194\n",
      "One batch end: 625.0446963310242\n",
      "# of batch: 820\n",
      "train accuracy: 0.88521314\n",
      "One batch end: 625.8002183437347\n",
      "# of batch: 821\n",
      "train accuracy: 0.8853285\n",
      "One batch end: 626.5554871559143\n",
      "# of batch: 822\n",
      "train accuracy: 0.8853949\n",
      "One batch end: 627.3244826793671\n",
      "# of batch: 823\n",
      "train accuracy: 0.8854369\n",
      "One batch end: 628.0764865875244\n",
      "# of batch: 824\n",
      "train accuracy: 0.88552725\n",
      "One batch end: 628.8129954338074\n",
      "# of batch: 825\n",
      "train accuracy: 0.88564163\n",
      "One batch end: 629.5469989776611\n",
      "# of batch: 826\n",
      "train accuracy: 0.8857074\n",
      "One batch end: 630.3429989814758\n",
      "# of batch: 827\n",
      "train accuracy: 0.8858213\n",
      "One batch end: 631.1249940395355\n",
      "# of batch: 828\n",
      "train accuracy: 0.88591075\n",
      "One batch end: 631.9359934329987\n",
      "# of batch: 829\n",
      "train accuracy: 0.8860482\n",
      "One batch end: 632.7229971885681\n",
      "# of batch: 830\n",
      "train accuracy: 0.8861372\n",
      "One batch end: 633.5095086097717\n",
      "# of batch: 831\n",
      "train accuracy: 0.88627404\n",
      "One batch end: 634.3085126876831\n",
      "# of batch: 832\n",
      "train accuracy: 0.88636255\n",
      "One batch end: 635.0985133647919\n",
      "# of batch: 833\n",
      "train accuracy: 0.8864508\n",
      "One batch end: 635.888512134552\n",
      "# of batch: 834\n",
      "train accuracy: 0.88651496\n",
      "One batch end: 636.6775498390198\n",
      "# of batch: 835\n",
      "train accuracy: 0.8866029\n",
      "One batch end: 637.4610612392426\n",
      "# of batch: 836\n",
      "train accuracy: 0.88669056\n",
      "One batch end: 638.2450609207153\n",
      "# of batch: 837\n",
      "train accuracy: 0.8868019\n",
      "One batch end: 639.0304152965546\n",
      "# of batch: 838\n",
      "train accuracy: 0.8868653\n",
      "One batch end: 639.8194105625153\n",
      "# of batch: 839\n",
      "train accuracy: 0.88688093\n",
      "One batch end: 640.6054146289825\n",
      "# of batch: 840\n",
      "train accuracy: 0.8869441\n",
      "One batch end: 641.3939440250397\n",
      "# of batch: 841\n",
      "train accuracy: 0.8869596\n",
      "One batch end: 642.1854555606842\n",
      "# of batch: 842\n",
      "train accuracy: 0.8869751\n",
      "One batch end: 642.9794590473175\n",
      "# of batch: 843\n",
      "train accuracy: 0.8870142\n",
      "One batch end: 643.7710404396057\n",
      "# of batch: 844\n",
      "train accuracy: 0.8870769\n",
      "One batch end: 644.5545575618744\n",
      "# of batch: 845\n",
      "train accuracy: 0.88718677\n",
      "One batch end: 645.3455607891083\n",
      "# of batch: 846\n",
      "train accuracy: 0.8872255\n",
      "One batch end: 646.1485605239868\n",
      "# of batch: 847\n",
      "train accuracy: 0.8873585\n",
      "One batch end: 646.9185557365417\n",
      "# of batch: 848\n",
      "train accuracy: 0.8874441\n",
      "One batch end: 647.695559501648\n",
      "# of batch: 849\n",
      "train accuracy: 0.8875059\n",
      "One batch end: 648.487555027008\n",
      "# of batch: 850\n",
      "train accuracy: 0.88752055\n",
      "One batch end: 649.2665586471558\n",
      "# of batch: 851\n",
      "train accuracy: 0.8875587\n",
      "One batch end: 650.0465543270111\n",
      "# of batch: 852\n",
      "train accuracy: 0.88766706\n",
      "One batch end: 650.8175532817841\n",
      "# of batch: 853\n",
      "train accuracy: 0.8877752\n",
      "One batch end: 651.6205534934998\n",
      "# of batch: 854\n",
      "train accuracy: 0.88788307\n",
      "One batch end: 652.4260678291321\n",
      "# of batch: 855\n",
      "train accuracy: 0.8879439\n",
      "One batch end: 653.2080671787262\n",
      "# of batch: 856\n",
      "train accuracy: 0.888028\n",
      "One batch end: 654.010066986084\n",
      "# of batch: 857\n",
      "train accuracy: 0.8880886\n",
      "One batch end: 654.7940664291382\n",
      "# of batch: 858\n",
      "train accuracy: 0.8881257\n",
      "One batch end: 655.5810699462891\n",
      "# of batch: 859\n",
      "train accuracy: 0.88825583\n",
      "One batch end: 656.3571541309357\n",
      "# of batch: 860\n",
      "train accuracy: 0.88836235\n",
      "One batch end: 657.1356751918793\n",
      "# of batch: 861\n",
      "train accuracy: 0.88842225\n",
      "One batch end: 657.9356739521027\n",
      "# of batch: 862\n",
      "train accuracy: 0.8885052\n",
      "One batch end: 658.7176737785339\n",
      "# of batch: 863\n",
      "train accuracy: 0.88861114\n",
      "One batch end: 659.5246732234955\n",
      "# of batch: 864\n",
      "train accuracy: 0.88869363\n",
      "One batch end: 660.2946727275848\n",
      "# of batch: 865\n",
      "train accuracy: 0.8888222\n",
      "One batch end: 661.0986721515656\n",
      "# of batch: 866\n",
      "train accuracy: 0.8888351\n",
      "One batch end: 661.9056715965271\n",
      "# of batch: 867\n",
      "train accuracy: 0.8889401\n",
      "One batch end: 662.6981744766235\n",
      "# of batch: 868\n",
      "train accuracy: 0.8890219\n",
      "One batch end: 663.5328245162964\n",
      "# of batch: 869\n",
      "train accuracy: 0.8891035\n",
      "One batch end: 664.3359830379486\n",
      "# of batch: 870\n",
      "train accuracy: 0.88913894\n",
      "One batch end: 665.1429805755615\n",
      "# of batch: 871\n",
      "train accuracy: 0.8892431\n",
      "One batch end: 665.9350442886353\n",
      "# of batch: 872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8893242\n",
      "One batch end: 666.732043504715\n",
      "# of batch: 873\n",
      "train accuracy: 0.889405\n",
      "One batch end: 667.5150427818298\n",
      "# of batch: 874\n",
      "train accuracy: 0.88950855\n",
      "One batch end: 668.2935636043549\n",
      "# of batch: 875\n",
      "train accuracy: 0.8896119\n",
      "One batch end: 669.0685634613037\n",
      "# of batch: 876\n",
      "train accuracy: 0.88964653\n",
      "One batch end: 669.8485624790192\n",
      "# of batch: 877\n",
      "train accuracy: 0.88972664\n",
      "One batch end: 670.6266090869904\n",
      "# of batch: 878\n",
      "train accuracy: 0.88982934\n",
      "One batch end: 671.4006087779999\n",
      "# of batch: 879\n",
      "train accuracy: 0.8898636\n",
      "One batch end: 672.1711204051971\n",
      "# of batch: 880\n",
      "train accuracy: 0.8898751\n",
      "One batch end: 672.9461197853088\n",
      "# of batch: 881\n",
      "train accuracy: 0.88986397\n",
      "One batch end: 673.7311193943024\n",
      "# of batch: 882\n",
      "train accuracy: 0.8898754\n",
      "One batch end: 674.5271186828613\n",
      "# of batch: 883\n",
      "train accuracy: 0.8899095\n",
      "One batch end: 675.2771182060242\n",
      "# of batch: 884\n",
      "train accuracy: 0.8899435\n",
      "One batch end: 675.9931178092957\n",
      "# of batch: 885\n",
      "train accuracy: 0.8900226\n",
      "One batch end: 676.7101173400879\n",
      "# of batch: 886\n",
      "train accuracy: 0.890124\n",
      "One batch end: 677.4301171302795\n",
      "# of batch: 887\n",
      "train accuracy: 0.8901802\n",
      "One batch end: 678.1441164016724\n",
      "# of batch: 888\n",
      "train accuracy: 0.89030373\n",
      "One batch end: 678.858638048172\n",
      "# of batch: 889\n",
      "train accuracy: 0.89033705\n",
      "One batch end: 679.5736367702484\n",
      "# of batch: 890\n",
      "train accuracy: 0.89037037\n",
      "One batch end: 680.2901465892792\n",
      "# of batch: 891\n",
      "train accuracy: 0.89044845\n",
      "One batch end: 681.0091462135315\n",
      "# of batch: 892\n",
      "train accuracy: 0.89050394\n",
      "One batch end: 681.7271456718445\n",
      "# of batch: 893\n",
      "train accuracy: 0.8905369\n",
      "One batch end: 682.4486563205719\n",
      "# of batch: 894\n",
      "train accuracy: 0.8905698\n",
      "One batch end: 683.1666557788849\n",
      "# of batch: 895\n",
      "train accuracy: 0.8906473\n",
      "One batch end: 683.8826553821564\n",
      "# of batch: 896\n",
      "train accuracy: 0.8906577\n",
      "One batch end: 684.6011617183685\n",
      "# of batch: 897\n",
      "train accuracy: 0.89066815\n",
      "One batch end: 685.3191611766815\n",
      "# of batch: 898\n",
      "train accuracy: 0.8907675\n",
      "One batch end: 686.0371608734131\n",
      "# of batch: 899\n",
      "train accuracy: 0.89084446\n",
      "One batch end: 686.7581601142883\n",
      "# of batch: 900\n",
      "train accuracy: 0.8909212\n",
      "One batch end: 687.4731597900391\n",
      "# of batch: 901\n",
      "train accuracy: 0.8909534\n",
      "One batch end: 688.1891593933105\n",
      "# of batch: 902\n",
      "train accuracy: 0.8910078\n",
      "One batch end: 688.9092199802399\n",
      "# of batch: 903\n",
      "train accuracy: 0.8911062\n",
      "One batch end: 689.6223027706146\n",
      "# of batch: 904\n",
      "train accuracy: 0.891116\n",
      "One batch end: 690.3463032245636\n",
      "# of batch: 905\n",
      "train accuracy: 0.8911921\n",
      "One batch end: 691.0643026828766\n",
      "# of batch: 906\n",
      "train accuracy: 0.89128995\n",
      "One batch end: 691.7883458137512\n",
      "# of batch: 907\n",
      "train accuracy: 0.8913216\n",
      "One batch end: 692.5034370422363\n",
      "# of batch: 908\n",
      "train accuracy: 0.8913971\n",
      "One batch end: 693.2214360237122\n",
      "# of batch: 909\n",
      "train accuracy: 0.8914725\n",
      "One batch end: 693.9364356994629\n",
      "# of batch: 910\n",
      "train accuracy: 0.8915258\n",
      "One batch end: 694.6565589904785\n",
      "# of batch: 911\n",
      "train accuracy: 0.8916228\n",
      "One batch end: 695.3695585727692\n",
      "# of batch: 912\n",
      "train accuracy: 0.8916977\n",
      "One batch end: 696.0805580615997\n",
      "# of batch: 913\n",
      "train accuracy: 0.89172864\n",
      "One batch end: 696.7936518192291\n",
      "# of batch: 914\n",
      "train accuracy: 0.89180326\n",
      "One batch end: 697.5116515159607\n",
      "# of batch: 915\n",
      "train accuracy: 0.8918996\n",
      "One batch end: 698.2311725616455\n",
      "# of batch: 916\n",
      "train accuracy: 0.89195204\n",
      "One batch end: 698.947259426117\n",
      "# of batch: 917\n",
      "train accuracy: 0.89204794\n",
      "One batch end: 699.6622588634491\n",
      "# of batch: 918\n",
      "train accuracy: 0.89212185\n",
      "One batch end: 700.3722543716431\n",
      "# of batch: 919\n",
      "train accuracy: 0.8922174\n",
      "One batch end: 701.0832579135895\n",
      "# of batch: 920\n",
      "train accuracy: 0.892291\n",
      "One batch end: 701.794864654541\n",
      "# of batch: 921\n",
      "train accuracy: 0.89238614\n",
      "One batch end: 702.5144667625427\n",
      "# of batch: 922\n",
      "train accuracy: 0.8924594\n",
      "One batch end: 703.2234659194946\n",
      "# of batch: 923\n",
      "train accuracy: 0.89253247\n",
      "One batch end: 703.9384653568268\n",
      "# of batch: 924\n",
      "train accuracy: 0.89264864\n",
      "One batch end: 704.6494650840759\n",
      "# of batch: 925\n",
      "train accuracy: 0.892743\n",
      "One batch end: 705.3629713058472\n",
      "# of batch: 926\n",
      "train accuracy: 0.8928587\n",
      "One batch end: 706.0751271247864\n",
      "# of batch: 927\n",
      "train accuracy: 0.89288795\n",
      "One batch end: 706.7911269664764\n",
      "# of batch: 928\n",
      "train accuracy: 0.8929171\n",
      "One batch end: 707.5047640800476\n",
      "# of batch: 929\n",
      "train accuracy: 0.89303225\n",
      "One batch end: 708.2168464660645\n",
      "# of batch: 930\n",
      "train accuracy: 0.89312565\n",
      "One batch end: 708.9288463592529\n",
      "# of batch: 931\n",
      "train accuracy: 0.8931974\n",
      "One batch end: 709.6398975849152\n",
      "# of batch: 932\n",
      "train accuracy: 0.89322615\n",
      "One batch end: 710.3629400730133\n",
      "# of batch: 933\n",
      "train accuracy: 0.8932762\n",
      "One batch end: 711.0750167369843\n",
      "# of batch: 934\n",
      "train accuracy: 0.89336896\n",
      "One batch end: 711.7900166511536\n",
      "# of batch: 935\n",
      "train accuracy: 0.8934402\n",
      "One batch end: 712.4990162849426\n",
      "# of batch: 936\n",
      "train accuracy: 0.89344716\n",
      "One batch end: 713.2080993652344\n",
      "# of batch: 937\n",
      "train accuracy: 0.8935394\n",
      "One batch end: 713.9171032905579\n",
      "# of batch: 938\n",
      "train accuracy: 0.89361024\n",
      "One batch end: 714.6326098442078\n",
      "# of batch: 939\n",
      "train accuracy: 0.8936596\n",
      "One batch end: 715.3442139625549\n",
      "# of batch: 940\n",
      "train accuracy: 0.8937726\n",
      "One batch end: 716.0562131404877\n",
      "# of batch: 941\n",
      "train accuracy: 0.8938641\n",
      "One batch end: 716.7672131061554\n",
      "# of batch: 942\n",
      "train accuracy: 0.8939767\n",
      "One batch end: 717.4782228469849\n",
      "# of batch: 943\n",
      "train accuracy: 0.89398307\n",
      "One batch end: 718.1882226467133\n",
      "# of batch: 944\n",
      "train accuracy: 0.8940106\n",
      "One batch end: 718.9043228626251\n",
      "# of batch: 945\n",
      "train accuracy: 0.89408034\n",
      "One batch end: 719.6203224658966\n",
      "# of batch: 946\n",
      "train accuracy: 0.89414996\n",
      "One batch end: 720.3623218536377\n",
      "# of batch: 947\n",
      "train accuracy: 0.8941983\n",
      "One batch end: 721.139321565628\n",
      "# of batch: 948\n",
      "train accuracy: 0.8942677\n",
      "One batch end: 721.9153208732605\n",
      "# of batch: 949\n",
      "train accuracy: 0.8943368\n",
      "One batch end: 722.6924042701721\n",
      "# of batch: 950\n",
      "train accuracy: 0.8944059\n",
      "One batch end: 723.4655590057373\n",
      "# of batch: 951\n",
      "train accuracy: 0.89445376\n",
      "One batch end: 724.2405586242676\n",
      "# of batch: 952\n",
      "train accuracy: 0.8945435\n",
      "One batch end: 725.0145578384399\n",
      "# of batch: 953\n",
      "train accuracy: 0.89454925\n",
      "One batch end: 725.792557477951\n",
      "# of batch: 954\n",
      "train accuracy: 0.894555\n",
      "One batch end: 726.5705568790436\n",
      "# of batch: 955\n",
      "train accuracy: 0.89462346\n",
      "One batch end: 727.3455564975739\n",
      "# of batch: 956\n",
      "train accuracy: 0.89469177\n",
      "One batch end: 728.1255559921265\n",
      "# of batch: 957\n",
      "train accuracy: 0.89471817\n",
      "One batch end: 728.9015552997589\n",
      "# of batch: 958\n",
      "train accuracy: 0.89478624\n",
      "One batch end: 729.6806976795197\n",
      "# of batch: 959\n",
      "train accuracy: 0.894875\n",
      "One batch end: 730.4557783603668\n",
      "# of batch: 960\n",
      "train accuracy: 0.89494276\n",
      "One batch end: 731.2373766899109\n",
      "# of batch: 961\n",
      "train accuracy: 0.8950104\n",
      "One batch end: 732.012371301651\n",
      "# of batch: 962\n",
      "train accuracy: 0.8950779\n",
      "One batch end: 732.7899310588837\n",
      "# of batch: 963\n",
      "train accuracy: 0.895166\n",
      "One batch end: 733.5609307289124\n",
      "# of batch: 964\n",
      "train accuracy: 0.89523315\n",
      "One batch end: 734.3339805603027\n",
      "# of batch: 965\n",
      "train accuracy: 0.8952795\n",
      "One batch end: 735.1075801849365\n",
      "# of batch: 966\n",
      "train accuracy: 0.8953671\n",
      "One batch end: 735.8811016082764\n",
      "# of batch: 967\n",
      "train accuracy: 0.89539254\n",
      "One batch end: 736.6562566757202\n",
      "# of batch: 968\n",
      "train accuracy: 0.89545923\n",
      "One batch end: 737.4292559623718\n",
      "# of batch: 969\n",
      "train accuracy: 0.89552575\n",
      "One batch end: 738.2039000988007\n",
      "# of batch: 970\n",
      "train accuracy: 0.8956128\n",
      "One batch end: 738.9793758392334\n",
      "# of batch: 971\n",
      "train accuracy: 0.8957202\n",
      "One batch end: 739.7493710517883\n",
      "# of batch: 972\n",
      "train accuracy: 0.8957862\n",
      "One batch end: 740.5214660167694\n",
      "# of batch: 973\n",
      "train accuracy: 0.89583164\n",
      "One batch end: 741.2944672107697\n",
      "# of batch: 974\n",
      "train accuracy: 0.89589745\n",
      "One batch end: 742.0684666633606\n",
      "# of batch: 975\n",
      "train accuracy: 0.8959426\n",
      "One batch end: 742.8456134796143\n",
      "# of batch: 976\n",
      "train accuracy: 0.8960082\n",
      "One batch end: 743.6246130466461\n",
      "# of batch: 977\n",
      "train accuracy: 0.8960941\n",
      "One batch end: 744.4027004241943\n",
      "# of batch: 978\n",
      "train accuracy: 0.89611846\n",
      "One batch end: 745.175699710846\n",
      "# of batch: 979\n",
      "train accuracy: 0.89618367\n",
      "One batch end: 745.9516994953156\n",
      "# of batch: 980\n",
      "train accuracy: 0.89626914\n",
      "One batch end: 746.7306985855103\n",
      "# of batch: 981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.89631367\n",
      "One batch end: 747.5030341148376\n",
      "# of batch: 982\n",
      "train accuracy: 0.8963581\n",
      "One batch end: 748.2771210670471\n",
      "# of batch: 983\n",
      "train accuracy: 0.89642274\n",
      "One batch end: 749.0591206550598\n",
      "# of batch: 984\n",
      "train accuracy: 0.89652795\n",
      "One batch end: 749.8398215770721\n",
      "# of batch: 985\n",
      "train accuracy: 0.8965517\n",
      "One batch end: 750.611820936203\n",
      "# of batch: 986\n",
      "train accuracy: 0.8965957\n",
      "One batch end: 751.3868203163147\n",
      "# of batch: 987\n",
      "train accuracy: 0.8966599\n",
      "One batch end: 752.1549108028412\n",
      "# of batch: 988\n",
      "train accuracy: 0.8967442\n",
      "One batch end: 752.9314866065979\n",
      "# of batch: 989\n",
      "train accuracy: 0.8968081\n",
      "One batch end: 753.7055523395538\n",
      "# of batch: 990\n",
      "train accuracy: 0.896892\n",
      "One batch end: 754.4815526008606\n",
      "# of batch: 991\n",
      "train accuracy: 0.89695567\n",
      "One batch end: 755.2576310634613\n",
      "# of batch: 992\n",
      "train accuracy: 0.896999\n",
      "One batch end: 756.0356307029724\n",
      "# of batch: 993\n",
      "train accuracy: 0.89700204\n",
      "One batch end: 756.8092505931854\n",
      "# of batch: 994\n",
      "train accuracy: 0.89704525\n",
      "One batch end: 757.5802500247955\n",
      "# of batch: 995\n",
      "train accuracy: 0.8971486\n",
      "One batch end: 758.351752281189\n",
      "# of batch: 996\n",
      "train accuracy: 0.8972317\n",
      "One batch end: 759.1317517757416\n",
      "# of batch: 997\n",
      "train accuracy: 0.89727455\n",
      "One batch end: 759.9107511043549\n",
      "# of batch: 998\n",
      "train accuracy: 0.8973774\n",
      "One batch end: 760.6867506504059\n",
      "# of batch: 999\n",
      "train accuracy: 0.89746\n",
      "One batch end: 761.4558393955231\n",
      "# of batch: 1000\n",
      "train accuracy: 0.8974825\n",
      "One batch end: 762.2288389205933\n",
      "# of batch: 1001\n",
      "train accuracy: 0.89758486\n",
      "One batch end: 763.0018382072449\n",
      "# of batch: 1002\n",
      "train accuracy: 0.8976471\n",
      "One batch end: 763.7768380641937\n",
      "# of batch: 1003\n",
      "train accuracy: 0.8977291\n",
      "One batch end: 764.5018374919891\n",
      "# of batch: 1004\n",
      "train accuracy: 0.89783084\n",
      "One batch end: 765.2138328552246\n",
      "# of batch: 1005\n",
      "train accuracy: 0.89789265\n",
      "One batch end: 765.9274408817291\n",
      "# of batch: 1006\n",
      "train accuracy: 0.8979742\n",
      "One batch end: 766.6414406299591\n",
      "# of batch: 1007\n",
      "train accuracy: 0.89801586\n",
      "One batch end: 767.3564400672913\n",
      "# of batch: 1008\n",
      "train accuracy: 0.8980971\n",
      "One batch end: 768.0690650939941\n",
      "# of batch: 1009\n",
      "train accuracy: 0.8981188\n",
      "One batch end: 768.7780628204346\n",
      "# of batch: 1010\n",
      "train accuracy: 0.8981602\n",
      "One batch end: 769.4951465129852\n",
      "# of batch: 1011\n",
      "train accuracy: 0.8981818\n",
      "One batch end: 770.2152214050293\n",
      "# of batch: 1012\n",
      "train accuracy: 0.89824283\n",
      "One batch end: 770.9412207603455\n",
      "# of batch: 1013\n",
      "train accuracy: 0.8983432\n",
      "One batch end: 771.7007417678833\n",
      "# of batch: 1014\n",
      "train accuracy: 0.8984237\n",
      "One batch end: 772.415741443634\n",
      "# of batch: 1015\n",
      "train accuracy: 0.89850396\n",
      "One batch end: 773.149261713028\n",
      "# of batch: 1016\n",
      "train accuracy: 0.8985447\n",
      "One batch end: 773.910261631012\n",
      "# of batch: 1017\n",
      "train accuracy: 0.89854616\n",
      "One batch end: 774.6337819099426\n",
      "# of batch: 1018\n",
      "train accuracy: 0.8986261\n",
      "One batch end: 775.3493025302887\n",
      "# of batch: 1019\n",
      "train accuracy: 0.89862746\n",
      "One batch end: 776.0693919658661\n",
      "# of batch: 1020\n",
      "train accuracy: 0.89870715\n",
      "One batch end: 776.7814674377441\n",
      "# of batch: 1021\n",
      "train accuracy: 0.89874756\n",
      "One batch end: 777.496550321579\n",
      "# of batch: 1022\n",
      "train accuracy: 0.8987683\n",
      "One batch end: 778.2105503082275\n",
      "# of batch: 1023\n",
      "train accuracy: 0.8988086\n",
      "One batch end: 778.9315495491028\n",
      "# of batch: 1024\n",
      "train accuracy: 0.8988488\n",
      "One batch end: 779.6445488929749\n",
      "# of batch: 1025\n",
      "train accuracy: 0.89892787\n",
      "One batch end: 780.3590550422668\n",
      "# of batch: 1026\n",
      "train accuracy: 0.89898735\n",
      "One batch end: 781.0750548839569\n",
      "# of batch: 1027\n",
      "train accuracy: 0.89906615\n",
      "One batch end: 781.7870538234711\n",
      "# of batch: 1028\n",
      "train accuracy: 0.89912534\n",
      "One batch end: 782.519134759903\n",
      "# of batch: 1029\n",
      "train accuracy: 0.8991456\n",
      "One batch end: 783.2331342697144\n",
      "# of batch: 1030\n",
      "train accuracy: 0.8992435\n",
      "One batch end: 783.9451336860657\n",
      "# of batch: 1031\n",
      "train accuracy: 0.89926356\n",
      "One batch end: 784.6601333618164\n",
      "# of batch: 1032\n",
      "train accuracy: 0.8993224\n",
      "One batch end: 785.3766431808472\n",
      "# of batch: 1033\n",
      "train accuracy: 0.89938104\n",
      "One batch end: 786.0932476520538\n",
      "# of batch: 1034\n",
      "train accuracy: 0.89940095\n",
      "One batch end: 786.8102471828461\n",
      "# of batch: 1035\n",
      "train accuracy: 0.8994595\n",
      "One batch end: 787.5242464542389\n",
      "# of batch: 1036\n",
      "train accuracy: 0.89953715\n",
      "One batch end: 788.2382464408875\n",
      "# of batch: 1037\n",
      "train accuracy: 0.8995761\n",
      "One batch end: 788.960245847702\n",
      "# of batch: 1038\n",
      "train accuracy: 0.899615\n",
      "One batch end: 789.6742453575134\n",
      "# of batch: 1039\n",
      "train accuracy: 0.8996923\n",
      "One batch end: 790.3862447738647\n",
      "# of batch: 1040\n",
      "train accuracy: 0.89973104\n",
      "One batch end: 791.1002440452576\n",
      "# of batch: 1041\n",
      "train accuracy: 0.89978886\n",
      "One batch end: 791.8142440319061\n",
      "# of batch: 1042\n",
      "train accuracy: 0.8998466\n",
      "One batch end: 792.5282435417175\n",
      "# of batch: 1043\n",
      "train accuracy: 0.8999234\n",
      "One batch end: 793.2443652153015\n",
      "# of batch: 1044\n",
      "train accuracy: 0.9\n",
      "One batch end: 793.957364320755\n",
      "# of batch: 1045\n",
      "train accuracy: 0.9000765\n",
      "One batch end: 794.6733641624451\n",
      "# of batch: 1046\n",
      "train accuracy: 0.9001146\n",
      "One batch end: 795.3838856220245\n",
      "# of batch: 1047\n",
      "train accuracy: 0.90017176\n",
      "One batch end: 796.0992493629456\n",
      "# of batch: 1048\n",
      "train accuracy: 0.9002097\n",
      "One batch end: 796.8142488002777\n",
      "# of batch: 1049\n",
      "train accuracy: 0.9002857\n",
      "One batch end: 797.5272486209869\n",
      "# of batch: 1050\n",
      "train accuracy: 0.9003806\n",
      "One batch end: 798.2422480583191\n",
      "# of batch: 1051\n",
      "train accuracy: 0.9004182\n",
      "One batch end: 798.9592478275299\n",
      "# of batch: 1052\n",
      "train accuracy: 0.90047485\n",
      "One batch end: 799.6742470264435\n",
      "# of batch: 1053\n",
      "train accuracy: 0.9005503\n",
      "One batch end: 800.3877692222595\n",
      "# of batch: 1054\n",
      "train accuracy: 0.90060663\n",
      "One batch end: 801.1047677993774\n",
      "# of batch: 1055\n",
      "train accuracy: 0.900625\n",
      "One batch end: 801.8202786445618\n",
      "# of batch: 1056\n",
      "train accuracy: 0.9006812\n",
      "One batch end: 802.537278175354\n",
      "# of batch: 1057\n",
      "train accuracy: 0.90068054\n",
      "One batch end: 803.2517995834351\n",
      "# of batch: 1058\n",
      "train accuracy: 0.9007366\n",
      "One batch end: 804.0123207569122\n",
      "# of batch: 1059\n",
      "train accuracy: 0.90073586\n",
      "One batch end: 804.7283201217651\n",
      "# of batch: 1060\n",
      "train accuracy: 0.90082943\n",
      "One batch end: 805.4463195800781\n",
      "# of batch: 1061\n",
      "train accuracy: 0.9008851\n",
      "One batch end: 806.1658351421356\n",
      "# of batch: 1062\n",
      "train accuracy: 0.9009407\n",
      "One batch end: 806.8818335533142\n",
      "# of batch: 1063\n",
      "train accuracy: 0.90101504\n",
      "One batch end: 807.5973546504974\n",
      "# of batch: 1064\n",
      "train accuracy: 0.90110797\n",
      "One batch end: 808.3148777484894\n",
      "# of batch: 1065\n",
      "train accuracy: 0.9011632\n",
      "One batch end: 809.0288770198822\n",
      "# of batch: 1066\n",
      "train accuracy: 0.90119964\n",
      "One batch end: 809.792876958847\n",
      "# of batch: 1067\n",
      "train accuracy: 0.9011985\n",
      "One batch end: 810.5723972320557\n",
      "# of batch: 1068\n",
      "train accuracy: 0.9012535\n",
      "One batch end: 811.3443970680237\n",
      "# of batch: 1069\n",
      "train accuracy: 0.9012897\n",
      "One batch end: 812.1183965206146\n",
      "# of batch: 1070\n",
      "train accuracy: 0.90134454\n",
      "One batch end: 812.8964600563049\n",
      "# of batch: 1071\n",
      "train accuracy: 0.9014179\n",
      "One batch end: 813.6714589595795\n",
      "# of batch: 1072\n",
      "train accuracy: 0.9014725\n",
      "One batch end: 814.4434585571289\n",
      "# of batch: 1073\n",
      "train accuracy: 0.90156424\n",
      "One batch end: 815.2224583625793\n",
      "# of batch: 1074\n",
      "train accuracy: 0.9016372\n",
      "One batch end: 815.9966731071472\n",
      "# of batch: 1075\n",
      "train accuracy: 0.90171003\n",
      "One batch end: 816.78067278862\n",
      "# of batch: 1076\n",
      "train accuracy: 0.90178275\n",
      "One batch end: 817.5556719303131\n",
      "# of batch: 1077\n",
      "train accuracy: 0.9018553\n",
      "One batch end: 818.3366713523865\n",
      "# of batch: 1078\n",
      "train accuracy: 0.9019092\n",
      "One batch end: 819.1176710128784\n",
      "# of batch: 1079\n",
      "train accuracy: 0.9019259\n",
      "One batch end: 819.8946704864502\n",
      "# of batch: 1080\n",
      "train accuracy: 0.90196115\n",
      "One batch end: 820.6706700325012\n",
      "# of batch: 1081\n",
      "train accuracy: 0.90205175\n",
      "One batch end: 821.4456696510315\n",
      "# of batch: 1082\n",
      "train accuracy: 0.9021422\n",
      "One batch end: 822.2216689586639\n",
      "# of batch: 1083\n",
      "train accuracy: 0.90221405\n",
      "One batch end: 823.0006685256958\n",
      "# of batch: 1084\n",
      "train accuracy: 0.90223044\n",
      "One batch end: 823.7796680927277\n",
      "# of batch: 1085\n",
      "train accuracy: 0.9022652\n",
      "One batch end: 824.5536677837372\n",
      "# of batch: 1086\n",
      "train accuracy: 0.9023367\n",
      "One batch end: 825.3281784057617\n",
      "# of batch: 1087\n",
      "train accuracy: 0.9023897\n",
      "One batch end: 826.1092646121979\n",
      "# of batch: 1088\n",
      "train accuracy: 0.90247935\n",
      "One batch end: 826.887264251709\n",
      "# of batch: 1089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.90251374\n",
      "One batch end: 827.6712639331818\n",
      "# of batch: 1090\n",
      "train accuracy: 0.90245646\n",
      "One batch end: 828.4482634067535\n",
      "# of batch: 1091\n",
      "train accuracy: 0.90249085\n",
      "One batch end: 829.225263595581\n",
      "# of batch: 1092\n",
      "train accuracy: 0.9025618\n",
      "One batch end: 830.0002620220184\n",
      "# of batch: 1093\n",
      "train accuracy: 0.90265083\n",
      "One batch end: 830.778261423111\n",
      "# of batch: 1094\n",
      "train accuracy: 0.9026849\n",
      "One batch end: 831.5542612075806\n",
      "# of batch: 1095\n",
      "train accuracy: 0.90277374\n",
      "One batch end: 832.3262605667114\n",
      "# of batch: 1096\n",
      "train accuracy: 0.90284413\n",
      "One batch end: 833.1058595180511\n",
      "# of batch: 1097\n",
      "train accuracy: 0.902878\n",
      "One batch end: 833.8828592300415\n",
      "# of batch: 1098\n",
      "train accuracy: 0.90294814\n",
      "One batch end: 834.6548583507538\n",
      "# of batch: 1099\n",
      "train accuracy: 0.9029091\n",
      "One batch end: 835.4291484355927\n",
      "# of batch: 1100\n",
      "train accuracy: 0.90296096\n",
      "One batch end: 836.2071475982666\n",
      "# of batch: 1101\n",
      "train accuracy: 0.9030309\n",
      "One batch end: 836.9841470718384\n",
      "# of batch: 1102\n",
      "train accuracy: 0.90306437\n",
      "One batch end: 837.7616696357727\n",
      "# of batch: 1103\n",
      "train accuracy: 0.90315217\n",
      "One batch end: 838.5446693897247\n",
      "# of batch: 1104\n",
      "train accuracy: 0.9032036\n",
      "One batch end: 839.3226690292358\n",
      "# of batch: 1105\n",
      "train accuracy: 0.90329117\n",
      "One batch end: 840.0976679325104\n",
      "# of batch: 1106\n",
      "train accuracy: 0.9033785\n",
      "One batch end: 840.8766674995422\n",
      "# of batch: 1107\n",
      "train accuracy: 0.9034296\n",
      "One batch end: 841.6526670455933\n",
      "# of batch: 1108\n",
      "train accuracy: 0.9034626\n",
      "One batch end: 842.4378716945648\n",
      "# of batch: 1109\n",
      "train accuracy: 0.9035135\n",
      "One batch end: 843.2183928489685\n",
      "# of batch: 1110\n",
      "train accuracy: 0.90354633\n",
      "One batch end: 843.9983921051025\n",
      "# of batch: 1111\n",
      "train accuracy: 0.9036151\n",
      "One batch end: 844.7723915576935\n",
      "# of batch: 1112\n",
      "train accuracy: 0.9036837\n",
      "One batch end: 845.5465738773346\n",
      "# of batch: 1113\n",
      "train accuracy: 0.90373427\n",
      "One batch end: 846.3235743045807\n",
      "# of batch: 1114\n",
      "train accuracy: 0.9038027\n",
      "One batch end: 847.1025726795197\n",
      "# of batch: 1115\n",
      "train accuracy: 0.90387094\n",
      "One batch end: 847.8835723400116\n",
      "# of batch: 1116\n",
      "train accuracy: 0.9039033\n",
      "One batch end: 848.6605718135834\n",
      "# of batch: 1117\n",
      "train accuracy: 0.9039714\n",
      "One batch end: 849.4325716495514\n",
      "# of batch: 1118\n",
      "train accuracy: 0.90402144\n",
      "One batch end: 850.2085709571838\n",
      "# of batch: 1119\n",
      "train accuracy: 0.90410715\n",
      "One batch end: 850.9895706176758\n",
      "# of batch: 1120\n",
      "train accuracy: 0.904157\n",
      "One batch end: 851.7631731033325\n",
      "# of batch: 1121\n",
      "train accuracy: 0.90420675\n",
      "One batch end: 852.534172296524\n",
      "# of batch: 1122\n",
      "train accuracy: 0.90420306\n",
      "One batch end: 853.3103015422821\n",
      "# of batch: 1123\n",
      "train accuracy: 0.90427047\n",
      "One batch end: 854.0413014888763\n",
      "# of batch: 1124\n",
      "train accuracy: 0.90430224\n",
      "One batch end: 854.7633013725281\n",
      "# of batch: 1125\n",
      "train accuracy: 0.9043162\n",
      "One batch end: 855.4813802242279\n",
      "# of batch: 1126\n",
      "train accuracy: 0.90434784\n",
      "One batch end: 856.1979839801788\n",
      "# of batch: 1127\n",
      "train accuracy: 0.9043972\n",
      "One batch end: 856.9139828681946\n",
      "# of batch: 1128\n",
      "train accuracy: 0.9044464\n",
      "One batch end: 857.6339824199677\n",
      "# of batch: 1129\n",
      "train accuracy: 0.9045133\n",
      "One batch end: 858.3509821891785\n",
      "# of batch: 1130\n",
      "train accuracy: 0.90458\n",
      "One batch end: 859.0699813365936\n",
      "# of batch: 1131\n",
      "train accuracy: 0.90464664\n",
      "One batch end: 859.7859809398651\n",
      "# of batch: 1132\n",
      "train accuracy: 0.9047308\n",
      "One batch end: 860.5059807300568\n",
      "# of batch: 1133\n",
      "train accuracy: 0.90474427\n",
      "One batch end: 861.2249798774719\n",
      "# of batch: 1134\n",
      "train accuracy: 0.90479296\n",
      "One batch end: 861.9419796466827\n",
      "# of batch: 1135\n",
      "train accuracy: 0.90484154\n",
      "One batch end: 862.6589789390564\n",
      "# of batch: 1136\n",
      "train accuracy: 0.90490764\n",
      "One batch end: 863.3766491413116\n",
      "# of batch: 1137\n",
      "train accuracy: 0.9049736\n",
      "One batch end: 864.0968952178955\n",
      "# of batch: 1138\n",
      "train accuracy: 0.9050395\n",
      "One batch end: 864.8128492832184\n",
      "# of batch: 1139\n",
      "train accuracy: 0.9051053\n",
      "One batch end: 865.5293703079224\n",
      "# of batch: 1140\n",
      "train accuracy: 0.9051709\n",
      "One batch end: 866.2479732036591\n",
      "# of batch: 1141\n",
      "train accuracy: 0.9052014\n",
      "One batch end: 866.9709722995758\n",
      "# of batch: 1142\n",
      "train accuracy: 0.90524936\n",
      "One batch end: 867.6899719238281\n",
      "# of batch: 1143\n",
      "train accuracy: 0.90524477\n",
      "One batch end: 868.4069712162018\n",
      "# of batch: 1144\n",
      "train accuracy: 0.9052926\n",
      "One batch end: 869.1229710578918\n",
      "# of batch: 1145\n",
      "train accuracy: 0.9052705\n",
      "One batch end: 869.8379707336426\n",
      "# of batch: 1146\n",
      "train accuracy: 0.9053008\n",
      "One batch end: 870.5519700050354\n",
      "# of batch: 1147\n",
      "train accuracy: 0.9053484\n",
      "One batch end: 871.2694909572601\n",
      "# of batch: 1148\n",
      "train accuracy: 0.9054134\n",
      "One batch end: 871.9874904155731\n",
      "# of batch: 1149\n",
      "train accuracy: 0.90547824\n",
      "One batch end: 872.7024903297424\n",
      "# of batch: 1150\n",
      "train accuracy: 0.9055083\n",
      "One batch end: 873.422091960907\n",
      "# of batch: 1151\n",
      "train accuracy: 0.9055382\n",
      "One batch end: 874.1546132564545\n",
      "# of batch: 1152\n",
      "train accuracy: 0.90560275\n",
      "One batch end: 874.877613067627\n",
      "# of batch: 1153\n",
      "train accuracy: 0.9056846\n",
      "One batch end: 875.5926125049591\n",
      "# of batch: 1154\n",
      "train accuracy: 0.9057143\n",
      "One batch end: 876.3146116733551\n",
      "# of batch: 1155\n",
      "train accuracy: 0.9057785\n",
      "One batch end: 877.0401296615601\n",
      "# of batch: 1156\n",
      "train accuracy: 0.90582544\n",
      "One batch end: 877.7671322822571\n",
      "# of batch: 1157\n",
      "train accuracy: 0.90587217\n",
      "One batch end: 878.4921319484711\n",
      "# of batch: 1158\n",
      "train accuracy: 0.9059016\n",
      "One batch end: 879.2161316871643\n",
      "# of batch: 1159\n",
      "train accuracy: 0.90589654\n",
      "One batch end: 879.946130990982\n",
      "# of batch: 1160\n",
      "train accuracy: 0.9059087\n",
      "One batch end: 880.6611306667328\n",
      "# of batch: 1161\n",
      "train accuracy: 0.9059725\n",
      "One batch end: 881.3946857452393\n",
      "# of batch: 1162\n",
      "train accuracy: 0.90603614\n",
      "One batch end: 882.1116852760315\n",
      "# of batch: 1163\n",
      "train accuracy: 0.9060481\n",
      "One batch end: 882.8346848487854\n",
      "# of batch: 1164\n",
      "train accuracy: 0.90609443\n",
      "One batch end: 883.5587272644043\n",
      "# of batch: 1165\n",
      "train accuracy: 0.9061406\n",
      "One batch end: 884.2967264652252\n",
      "# of batch: 1166\n",
      "train accuracy: 0.9062039\n",
      "One batch end: 885.0162479877472\n",
      "# of batch: 1167\n",
      "train accuracy: 0.90625\n",
      "One batch end: 885.7312436103821\n",
      "# of batch: 1168\n",
      "train accuracy: 0.90626174\n",
      "One batch end: 886.4493515491486\n",
      "# of batch: 1169\n",
      "train accuracy: 0.9063077\n",
      "One batch end: 887.1769473552704\n",
      "# of batch: 1170\n",
      "train accuracy: 0.90635353\n",
      "One batch end: 887.9000287055969\n",
      "# of batch: 1171\n",
      "train accuracy: 0.9063481\n",
      "One batch end: 888.6190280914307\n",
      "# of batch: 1172\n",
      "train accuracy: 0.9063939\n",
      "One batch end: 889.3370275497437\n",
      "# of batch: 1173\n",
      "train accuracy: 0.90640545\n",
      "One batch end: 890.0540266036987\n",
      "# of batch: 1174\n",
      "train accuracy: 0.9064681\n",
      "One batch end: 890.7790260314941\n",
      "# of batch: 1175\n",
      "train accuracy: 0.9064966\n",
      "One batch end: 891.5030267238617\n",
      "# of batch: 1176\n",
      "train accuracy: 0.9065251\n",
      "One batch end: 892.2420253753662\n",
      "# of batch: 1177\n",
      "train accuracy: 0.90657043\n",
      "One batch end: 892.982025384903\n",
      "# of batch: 1178\n",
      "train accuracy: 0.9066327\n",
      "One batch end: 893.7420244216919\n",
      "# of batch: 1179\n",
      "train accuracy: 0.9066949\n",
      "One batch end: 894.4630246162415\n",
      "# of batch: 1180\n",
      "train accuracy: 0.9067739\n",
      "One batch end: 895.1970233917236\n",
      "# of batch: 1181\n",
      "train accuracy: 0.90683585\n",
      "One batch end: 895.9200234413147\n",
      "# of batch: 1182\n",
      "train accuracy: 0.9068808\n",
      "One batch end: 896.6450228691101\n",
      "# of batch: 1183\n",
      "train accuracy: 0.9069088\n",
      "One batch end: 897.364022731781\n",
      "# of batch: 1184\n",
      "train accuracy: 0.90697044\n",
      "One batch end: 898.082022190094\n",
      "# of batch: 1185\n",
      "train accuracy: 0.9070152\n",
      "One batch end: 898.8040215969086\n",
      "# of batch: 1186\n",
      "train accuracy: 0.90700924\n",
      "One batch end: 899.5870203971863\n",
      "# of batch: 1187\n",
      "train accuracy: 0.9070707\n",
      "One batch end: 900.3672280311584\n",
      "# of batch: 1188\n",
      "train accuracy: 0.9070816\n",
      "One batch end: 901.1502275466919\n",
      "# of batch: 1189\n",
      "train accuracy: 0.90710926\n",
      "One batch end: 901.9232928752899\n",
      "# of batch: 1190\n",
      "train accuracy: 0.90713686\n",
      "One batch end: 902.7072925567627\n",
      "# of batch: 1191\n",
      "train accuracy: 0.9071812\n",
      "One batch end: 903.4849328994751\n",
      "# of batch: 1192\n",
      "train accuracy: 0.907259\n",
      "One batch end: 904.2634408473969\n",
      "# of batch: 1193\n",
      "train accuracy: 0.9073367\n",
      "One batch end: 905.0464403629303\n",
      "# of batch: 1194\n",
      "train accuracy: 0.9073975\n",
      "One batch end: 905.8284397125244\n",
      "# of batch: 1195\n",
      "train accuracy: 0.9074582\n",
      "One batch end: 906.6129505634308\n",
      "# of batch: 1196\n",
      "train accuracy: 0.9075021\n",
      "One batch end: 907.3959500789642\n",
      "# of batch: 1197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9075459\n",
      "One batch end: 908.1710093021393\n",
      "# of batch: 1198\n",
      "train accuracy: 0.9075897\n",
      "One batch end: 908.9520087242126\n",
      "# of batch: 1199\n",
      "train accuracy: 0.9076\n",
      "One batch end: 909.7250080108643\n",
      "# of batch: 1200\n",
      "train accuracy: 0.90767694\n",
      "One batch end: 910.498007774353\n",
      "# of batch: 1201\n",
      "train accuracy: 0.90772045\n",
      "One batch end: 911.2771198749542\n",
      "# of batch: 1202\n",
      "train accuracy: 0.90774727\n",
      "One batch end: 912.0541741847992\n",
      "# of batch: 1203\n",
      "train accuracy: 0.9077907\n",
      "One batch end: 912.8291738033295\n",
      "# of batch: 1204\n",
      "train accuracy: 0.9078174\n",
      "One batch end: 913.6101734638214\n",
      "# of batch: 1205\n",
      "train accuracy: 0.9078441\n",
      "One batch end: 914.3872277736664\n",
      "# of batch: 1206\n",
      "train accuracy: 0.90788734\n",
      "One batch end: 915.1662273406982\n",
      "# of batch: 1207\n",
      "train accuracy: 0.9079139\n",
      "One batch end: 915.9424636363983\n",
      "# of batch: 1208\n",
      "train accuracy: 0.9079901\n",
      "One batch end: 916.7129933834076\n",
      "# of batch: 1209\n",
      "train accuracy: 0.908\n",
      "One batch end: 917.4920642375946\n",
      "# of batch: 1210\n",
      "train accuracy: 0.9080595\n",
      "One batch end: 918.2660622596741\n",
      "# of batch: 1211\n",
      "train accuracy: 0.9081353\n",
      "One batch end: 919.0450618267059\n",
      "# of batch: 1212\n",
      "train accuracy: 0.90819454\n",
      "One batch end: 919.819061756134\n",
      "# of batch: 1213\n",
      "train accuracy: 0.9082537\n",
      "One batch end: 920.5920610427856\n",
      "# of batch: 1214\n",
      "train accuracy: 0.9082963\n",
      "One batch end: 921.3715815544128\n",
      "# of batch: 1215\n",
      "train accuracy: 0.90833884\n",
      "One batch end: 922.1531603336334\n",
      "# of batch: 1216\n",
      "train accuracy: 0.9083813\n",
      "One batch end: 922.9421601295471\n",
      "# of batch: 1217\n",
      "train accuracy: 0.9083908\n",
      "One batch end: 923.7241594791412\n",
      "# of batch: 1218\n",
      "train accuracy: 0.908466\n",
      "One batch end: 924.51122879982\n",
      "# of batch: 1219\n",
      "train accuracy: 0.9085082\n",
      "One batch end: 925.2918081283569\n",
      "# of batch: 1220\n",
      "train accuracy: 0.9085504\n",
      "One batch end: 926.067807674408\n",
      "# of batch: 1221\n",
      "train accuracy: 0.90855974\n",
      "One batch end: 926.8508689403534\n",
      "# of batch: 1222\n",
      "train accuracy: 0.9086018\n",
      "One batch end: 927.6278686523438\n",
      "# of batch: 1223\n",
      "train accuracy: 0.9086601\n",
      "One batch end: 928.4074819087982\n",
      "# of batch: 1224\n",
      "train accuracy: 0.90871835\n",
      "One batch end: 929.1844811439514\n",
      "# of batch: 1225\n",
      "train accuracy: 0.9087765\n",
      "One batch end: 929.9634807109833\n",
      "# of batch: 1226\n",
      "train accuracy: 0.908802\n",
      "One batch end: 930.7404804229736\n",
      "# of batch: 1227\n",
      "train accuracy: 0.9088599\n",
      "One batch end: 931.5210280418396\n",
      "# of batch: 1228\n",
      "train accuracy: 0.9089341\n",
      "One batch end: 932.3050274848938\n",
      "# of batch: 1229\n",
      "train accuracy: 0.9089756\n",
      "One batch end: 933.0976393222809\n",
      "# of batch: 1230\n",
      "train accuracy: 0.9090496\n",
      "One batch end: 933.8786609172821\n",
      "# of batch: 1231\n",
      "train accuracy: 0.90909094\n",
      "One batch end: 934.6576607227325\n",
      "# of batch: 1232\n",
      "train accuracy: 0.90916467\n",
      "One batch end: 935.4426600933075\n",
      "# of batch: 1233\n",
      "train accuracy: 0.90920585\n",
      "One batch end: 936.2206597328186\n",
      "# of batch: 1234\n",
      "train accuracy: 0.90926313\n",
      "One batch end: 937.0026590824127\n",
      "# of batch: 1235\n",
      "train accuracy: 0.9092557\n",
      "One batch end: 937.7826585769653\n",
      "# of batch: 1236\n",
      "train accuracy: 0.9092643\n",
      "One batch end: 938.5616581439972\n",
      "# of batch: 1237\n",
      "train accuracy: 0.90933764\n",
      "One batch end: 939.3486576080322\n",
      "# of batch: 1238\n",
      "train accuracy: 0.9093624\n",
      "One batch end: 940.1326577663422\n",
      "# of batch: 1239\n",
      "train accuracy: 0.90941936\n",
      "One batch end: 940.9221639633179\n",
      "# of batch: 1240\n",
      "train accuracy: 0.9094601\n",
      "One batch end: 941.7017254829407\n",
      "# of batch: 1241\n",
      "train accuracy: 0.909533\n",
      "One batch end: 942.4777982234955\n",
      "# of batch: 1242\n",
      "train accuracy: 0.9095897\n",
      "One batch end: 943.2583193778992\n",
      "# of batch: 1243\n",
      "train accuracy: 0.90961415\n",
      "One batch end: 943.9803190231323\n",
      "# of batch: 1244\n",
      "train accuracy: 0.90968674\n",
      "One batch end: 944.6983940601349\n",
      "# of batch: 1245\n",
      "train accuracy: 0.9097592\n",
      "One batch end: 945.4173924922943\n",
      "# of batch: 1246\n",
      "train accuracy: 0.90981555\n",
      "One batch end: 946.135986328125\n",
      "# of batch: 1247\n",
      "train accuracy: 0.9098718\n",
      "One batch end: 946.8559861183167\n",
      "# of batch: 1248\n",
      "train accuracy: 0.90991193\n",
      "One batch end: 947.5755889415741\n",
      "# of batch: 1249\n",
      "train accuracy: 0.909936\n",
      "One batch end: 948.294588804245\n",
      "# of batch: 1250\n",
      "train accuracy: 0.90996003\n",
      "One batch end: 949.0155878067017\n",
      "# of batch: 1251\n",
      "train accuracy: 0.90998405\n",
      "One batch end: 949.7365834712982\n",
      "# of batch: 1252\n",
      "train accuracy: 0.9100399\n",
      "One batch end: 950.4556272029877\n",
      "# of batch: 1253\n",
      "train accuracy: 0.9100957\n",
      "One batch end: 951.1786267757416\n",
      "# of batch: 1254\n",
      "train accuracy: 0.91013545\n",
      "One batch end: 951.8966264724731\n",
      "# of batch: 1255\n",
      "train accuracy: 0.91017514\n",
      "One batch end: 952.6136257648468\n",
      "# of batch: 1256\n",
      "train accuracy: 0.91019887\n",
      "One batch end: 953.3326163291931\n",
      "# of batch: 1257\n",
      "train accuracy: 0.9102067\n",
      "One batch end: 954.0506684780121\n",
      "# of batch: 1258\n",
      "train accuracy: 0.91024625\n",
      "One batch end: 954.7716677188873\n",
      "# of batch: 1259\n",
      "train accuracy: 0.9102857\n",
      "One batch end: 955.4856674671173\n",
      "# of batch: 1260\n",
      "train accuracy: 0.91027755\n",
      "One batch end: 956.2007315158844\n",
      "# of batch: 1261\n",
      "train accuracy: 0.91028523\n",
      "One batch end: 956.9182529449463\n",
      "# of batch: 1262\n",
      "train accuracy: 0.91032463\n",
      "One batch end: 957.6342523097992\n",
      "# of batch: 1263\n",
      "train accuracy: 0.9103481\n",
      "One batch end: 958.3537735939026\n",
      "# of batch: 1264\n",
      "train accuracy: 0.91037154\n",
      "One batch end: 959.071772813797\n",
      "# of batch: 1265\n",
      "train accuracy: 0.91042656\n",
      "One batch end: 959.7907721996307\n",
      "# of batch: 1266\n",
      "train accuracy: 0.91046566\n",
      "One batch end: 960.5087718963623\n",
      "# of batch: 1267\n",
      "train accuracy: 0.9105205\n",
      "One batch end: 961.2247712612152\n",
      "# of batch: 1268\n",
      "train accuracy: 0.9105595\n",
      "One batch end: 961.9402923583984\n",
      "# of batch: 1269\n",
      "train accuracy: 0.91058266\n",
      "One batch end: 962.6898987293243\n",
      "# of batch: 1270\n",
      "train accuracy: 0.9106216\n",
      "One batch end: 963.4178977012634\n",
      "# of batch: 1271\n",
      "train accuracy: 0.9106918\n",
      "One batch end: 964.1384193897247\n",
      "# of batch: 1272\n",
      "train accuracy: 0.91073054\n",
      "One batch end: 964.8564188480377\n",
      "# of batch: 1273\n",
      "train accuracy: 0.9107692\n",
      "One batch end: 965.5694181919098\n",
      "# of batch: 1274\n",
      "train accuracy: 0.9107765\n",
      "One batch end: 966.2854175567627\n",
      "# of batch: 1275\n",
      "train accuracy: 0.91081506\n",
      "One batch end: 967.011417388916\n",
      "# of batch: 1276\n",
      "train accuracy: 0.91086924\n",
      "One batch end: 967.7319386005402\n",
      "# of batch: 1277\n",
      "train accuracy: 0.9108764\n",
      "One batch end: 968.4514598846436\n",
      "# of batch: 1278\n",
      "train accuracy: 0.9109148\n",
      "One batch end: 969.1674592494965\n",
      "# of batch: 1279\n",
      "train accuracy: 0.9109531\n",
      "One batch end: 969.8864591121674\n",
      "# of batch: 1280\n",
      "train accuracy: 0.91102266\n",
      "One batch end: 970.6049799919128\n",
      "# of batch: 1281\n",
      "train accuracy: 0.91104525\n",
      "One batch end: 971.3259794712067\n",
      "# of batch: 1282\n",
      "train accuracy: 0.9111146\n",
      "One batch end: 972.0475652217865\n",
      "# of batch: 1283\n",
      "train accuracy: 0.9111682\n",
      "One batch end: 972.7655646800995\n",
      "# of batch: 1284\n",
      "train accuracy: 0.9111907\n",
      "One batch end: 973.482565164566\n",
      "# of batch: 1285\n",
      "train accuracy: 0.91121304\n",
      "One batch end: 974.2005636692047\n",
      "# of batch: 1286\n",
      "train accuracy: 0.9112665\n",
      "One batch end: 974.9225633144379\n",
      "# of batch: 1287\n",
      "train accuracy: 0.91130435\n",
      "One batch end: 975.6385626792908\n",
      "# of batch: 1288\n",
      "train accuracy: 0.9113266\n",
      "One batch end: 976.3550844192505\n",
      "# of batch: 1289\n",
      "train accuracy: 0.9113643\n",
      "One batch end: 977.0740847587585\n",
      "# of batch: 1290\n",
      "train accuracy: 0.9114175\n",
      "One batch end: 977.7940833568573\n",
      "# of batch: 1291\n",
      "train accuracy: 0.9114551\n",
      "One batch end: 978.5120828151703\n",
      "# of batch: 1292\n",
      "train accuracy: 0.91149265\n",
      "One batch end: 979.2320783138275\n",
      "# of batch: 1293\n",
      "train accuracy: 0.9115611\n",
      "One batch end: 979.9510827064514\n",
      "# of batch: 1294\n",
      "train accuracy: 0.91159844\n",
      "One batch end: 980.6680812835693\n",
      "# of batch: 1295\n",
      "train accuracy: 0.91165125\n",
      "One batch end: 981.388081073761\n",
      "# of batch: 1296\n",
      "train accuracy: 0.91170394\n",
      "One batch end: 982.1030802726746\n",
      "# of batch: 1297\n",
      "train accuracy: 0.9117566\n",
      "One batch end: 982.8170762062073\n",
      "# of batch: 1298\n",
      "train accuracy: 0.9117783\n",
      "One batch end: 983.5352559089661\n",
      "# of batch: 1299\n",
      "train accuracy: 0.91184616\n",
      "One batch end: 984.2572555541992\n",
      "# of batch: 1300\n",
      "train accuracy: 0.9118678\n",
      "One batch end: 984.9732549190521\n",
      "# of batch: 1301\n",
      "train accuracy: 0.91192013\n",
      "One batch end: 985.6897752285004\n",
      "# of batch: 1302\n",
      "train accuracy: 0.9119877\n",
      "One batch end: 986.405775308609\n",
      "# of batch: 1303\n",
      "train accuracy: 0.9120399\n",
      "One batch end: 987.1297745704651\n",
      "# of batch: 1304\n",
      "train accuracy: 0.91204596\n",
      "One batch end: 987.850832939148\n",
      "# of batch: 1305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.912098\n",
      "One batch end: 988.5669085979462\n",
      "# of batch: 1306\n",
      "train accuracy: 0.91213465\n",
      "One batch end: 989.343908071518\n",
      "# of batch: 1307\n",
      "train accuracy: 0.912156\n",
      "One batch end: 990.1239075660706\n",
      "# of batch: 1308\n",
      "train accuracy: 0.9122231\n",
      "One batch end: 990.8999073505402\n",
      "# of batch: 1309\n",
      "train accuracy: 0.91227484\n",
      "One batch end: 991.671906709671\n",
      "# of batch: 1310\n",
      "train accuracy: 0.9122807\n",
      "One batch end: 992.447906255722\n",
      "# of batch: 1311\n",
      "train accuracy: 0.9123171\n",
      "One batch end: 993.2229056358337\n",
      "# of batch: 1312\n",
      "train accuracy: 0.9123534\n",
      "One batch end: 994.0046174526215\n",
      "# of batch: 1313\n",
      "train accuracy: 0.91238964\n",
      "One batch end: 994.7826178073883\n",
      "# of batch: 1314\n",
      "train accuracy: 0.9124259\n",
      "One batch end: 995.5616166591644\n",
      "# of batch: 1315\n",
      "train accuracy: 0.9124164\n",
      "One batch end: 996.3391370773315\n",
      "# of batch: 1316\n",
      "train accuracy: 0.9124222\n",
      "One batch end: 997.1171362400055\n",
      "# of batch: 1317\n",
      "train accuracy: 0.91247344\n",
      "One batch end: 997.8937339782715\n",
      "# of batch: 1318\n",
      "train accuracy: 0.91252464\n",
      "One batch end: 998.6717383861542\n",
      "# of batch: 1319\n",
      "train accuracy: 0.9125758\n",
      "One batch end: 999.4527368545532\n",
      "# of batch: 1320\n",
      "train accuracy: 0.9126268\n",
      "One batch end: 1000.2277367115021\n",
      "# of batch: 1321\n",
      "train accuracy: 0.91267776\n",
      "One batch end: 1001.0017359256744\n",
      "# of batch: 1322\n",
      "train accuracy: 0.9127135\n",
      "One batch end: 1001.7772567272186\n",
      "# of batch: 1323\n",
      "train accuracy: 0.9127644\n",
      "One batch end: 1002.5778584480286\n",
      "# of batch: 1324\n",
      "train accuracy: 0.9128\n",
      "One batch end: 1003.3698618412018\n",
      "# of batch: 1325\n",
      "train accuracy: 0.9128205\n",
      "One batch end: 1004.1548614501953\n",
      "# of batch: 1326\n",
      "train accuracy: 0.91285604\n",
      "One batch end: 1004.9321134090424\n",
      "# of batch: 1327\n",
      "train accuracy: 0.91292167\n",
      "One batch end: 1005.7081127166748\n",
      "# of batch: 1328\n",
      "train accuracy: 0.91295713\n",
      "One batch end: 1006.4856357574463\n",
      "# of batch: 1329\n",
      "train accuracy: 0.9129624\n",
      "One batch end: 1007.2616353034973\n",
      "# of batch: 1330\n",
      "train accuracy: 0.9129977\n",
      "One batch end: 1008.0386345386505\n",
      "# of batch: 1331\n",
      "train accuracy: 0.91306305\n",
      "One batch end: 1008.8196339607239\n",
      "# of batch: 1332\n",
      "train accuracy: 0.9130983\n",
      "One batch end: 1009.5926332473755\n",
      "# of batch: 1333\n",
      "train accuracy: 0.9131184\n",
      "One batch end: 1010.3696329593658\n",
      "# of batch: 1334\n",
      "train accuracy: 0.91313857\n",
      "One batch end: 1011.1471536159515\n",
      "# of batch: 1335\n",
      "train accuracy: 0.9132036\n",
      "One batch end: 1011.9607503414154\n",
      "# of batch: 1336\n",
      "train accuracy: 0.91325355\n",
      "One batch end: 1012.744749546051\n",
      "# of batch: 1337\n",
      "train accuracy: 0.9132885\n",
      "One batch end: 1013.5232710838318\n",
      "# of batch: 1338\n",
      "train accuracy: 0.9133234\n",
      "One batch end: 1014.3112707138062\n",
      "# of batch: 1339\n",
      "train accuracy: 0.9133433\n",
      "One batch end: 1015.0962700843811\n",
      "# of batch: 1340\n",
      "train accuracy: 0.91337806\n",
      "One batch end: 1015.8777906894684\n",
      "# of batch: 1341\n",
      "train accuracy: 0.913383\n",
      "One batch end: 1016.6527903079987\n",
      "# of batch: 1342\n",
      "train accuracy: 0.9134177\n",
      "One batch end: 1017.4323892593384\n",
      "# of batch: 1343\n",
      "train accuracy: 0.91342264\n",
      "One batch end: 1018.2113893032074\n",
      "# of batch: 1344\n",
      "train accuracy: 0.9134424\n",
      "One batch end: 1018.9904310703278\n",
      "# of batch: 1345\n",
      "train accuracy: 0.9134621\n",
      "One batch end: 1019.7705030441284\n",
      "# of batch: 1346\n",
      "train accuracy: 0.9134967\n",
      "One batch end: 1020.5475022792816\n",
      "# of batch: 1347\n",
      "train accuracy: 0.91351634\n",
      "One batch end: 1021.3266186714172\n",
      "# of batch: 1348\n",
      "train accuracy: 0.91353595\n",
      "One batch end: 1022.1136181354523\n",
      "# of batch: 1349\n",
      "train accuracy: 0.9135407\n",
      "One batch end: 1022.903618812561\n",
      "# of batch: 1350\n",
      "train accuracy: 0.91356033\n",
      "One batch end: 1023.7166171073914\n",
      "# of batch: 1351\n",
      "train accuracy: 0.91360945\n",
      "One batch end: 1024.5836164951324\n",
      "# of batch: 1352\n",
      "train accuracy: 0.91365856\n",
      "One batch end: 1025.395616054535\n",
      "# of batch: 1353\n",
      "train accuracy: 0.91370755\n",
      "One batch end: 1026.233615398407\n",
      "# of batch: 1354\n",
      "train accuracy: 0.9137269\n",
      "One batch end: 1027.0556147098541\n",
      "# of batch: 1355\n",
      "train accuracy: 0.9137906\n",
      "One batch end: 1027.9036145210266\n",
      "# of batch: 1356\n",
      "train accuracy: 0.91383934\n",
      "One batch end: 1028.7956140041351\n",
      "# of batch: 1357\n",
      "train accuracy: 0.9138733\n",
      "One batch end: 1029.640613079071\n",
      "# of batch: 1358\n",
      "train accuracy: 0.913922\n",
      "One batch end: 1030.5606129169464\n",
      "# of batch: 1359\n",
      "train accuracy: 0.9139706\n",
      "One batch end: 1031.377611875534\n",
      "# of batch: 1360\n",
      "train accuracy: 0.9140191\n",
      "One batch end: 1032.2806115150452\n",
      "# of batch: 1361\n",
      "train accuracy: 0.91406757\n",
      "One batch end: 1033.0751218795776\n",
      "# of batch: 1362\n",
      "train accuracy: 0.9140866\n",
      "One batch end: 1033.8731215000153\n",
      "# of batch: 1363\n",
      "train accuracy: 0.9141056\n",
      "One batch end: 1034.6231207847595\n",
      "# of batch: 1364\n",
      "train accuracy: 0.91412455\n",
      "One batch end: 1035.3851203918457\n",
      "# of batch: 1365\n",
      "train accuracy: 0.91418743\n",
      "One batch end: 1036.1406314373016\n",
      "# of batch: 1366\n",
      "train accuracy: 0.914177\n",
      "One batch end: 1036.8726308345795\n",
      "# of batch: 1367\n",
      "train accuracy: 0.9142105\n",
      "One batch end: 1037.74063038826\n",
      "# of batch: 1368\n",
      "train accuracy: 0.91421473\n",
      "One batch end: 1038.6751403808594\n",
      "# of batch: 1369\n",
      "train accuracy: 0.9142774\n",
      "One batch end: 1039.6246626377106\n",
      "# of batch: 1370\n",
      "train accuracy: 0.9143107\n",
      "One batch end: 1040.472661972046\n",
      "# of batch: 1371\n",
      "train accuracy: 0.914344\n",
      "One batch end: 1041.3836615085602\n",
      "# of batch: 1372\n",
      "train accuracy: 0.9143918\n",
      "One batch end: 1042.2364225387573\n",
      "# of batch: 1373\n",
      "train accuracy: 0.9144396\n",
      "One batch end: 1043.1644220352173\n",
      "# of batch: 1374\n",
      "train accuracy: 0.9144727\n",
      "One batch end: 1043.9109332561493\n",
      "# of batch: 1375\n",
      "train accuracy: 0.9144913\n",
      "One batch end: 1044.6739330291748\n",
      "# of batch: 1376\n",
      "train accuracy: 0.91453886\n",
      "One batch end: 1045.4419326782227\n",
      "# of batch: 1377\n",
      "train accuracy: 0.91455734\n",
      "One batch end: 1046.2019319534302\n",
      "# of batch: 1378\n",
      "train accuracy: 0.9145468\n",
      "One batch end: 1046.9714171886444\n",
      "# of batch: 1379\n",
      "train accuracy: 0.9145797\n",
      "One batch end: 1047.8194165229797\n",
      "# of batch: 1380\n",
      "train accuracy: 0.91464156\n",
      "One batch end: 1048.7674157619476\n",
      "# of batch: 1381\n",
      "train accuracy: 0.9146599\n",
      "One batch end: 1049.7714154720306\n",
      "# of batch: 1382\n",
      "train accuracy: 0.9146782\n",
      "One batch end: 1050.786414861679\n",
      "# of batch: 1383\n",
      "train accuracy: 0.914711\n",
      "One batch end: 1051.7714142799377\n",
      "# of batch: 1384\n",
      "train accuracy: 0.91477257\n",
      "One batch end: 1052.8314139842987\n",
      "# of batch: 1385\n",
      "train accuracy: 0.9148052\n",
      "One batch end: 1053.8874127864838\n",
      "# of batch: 1386\n",
      "train accuracy: 0.91482335\n",
      "One batch end: 1054.8849189281464\n",
      "# of batch: 1387\n",
      "train accuracy: 0.9148271\n",
      "One batch end: 1055.8189187049866\n",
      "# of batch: 1388\n",
      "train accuracy: 0.914874\n",
      "One batch end: 1056.7099177837372\n",
      "# of batch: 1389\n",
      "train accuracy: 0.91492087\n",
      "One batch end: 1057.6974244117737\n",
      "# of batch: 1390\n",
      "train accuracy: 0.91496766\n",
      "One batch end: 1058.5829405784607\n",
      "# of batch: 1391\n",
      "train accuracy: 0.9150144\n",
      "One batch end: 1059.514930486679\n",
      "# of batch: 1392\n",
      "train accuracy: 0.91504663\n",
      "One batch end: 1060.4379305839539\n",
      "# of batch: 1393\n",
      "train accuracy: 0.9151076\n",
      "One batch end: 1061.3264513015747\n",
      "# of batch: 1394\n",
      "train accuracy: 0.9151398\n",
      "One batch end: 1062.2379581928253\n",
      "# of batch: 1395\n",
      "train accuracy: 0.9152006\n",
      "One batch end: 1063.132957458496\n",
      "# of batch: 1396\n",
      "train accuracy: 0.91524696\n",
      "One batch end: 1064.0219569206238\n",
      "# of batch: 1397\n",
      "train accuracy: 0.9152933\n",
      "One batch end: 1064.9669857025146\n",
      "# of batch: 1398\n",
      "train accuracy: 0.9152966\n",
      "One batch end: 1065.8689849376678\n",
      "# of batch: 1399\n",
      "train accuracy: 0.9153\n",
      "One batch end: 1066.784984588623\n",
      "# of batch: 1400\n",
      "train accuracy: 0.9153319\n",
      "One batch end: 1067.6979849338531\n",
      "# of batch: 1401\n",
      "train accuracy: 0.9153638\n",
      "One batch end: 1068.6180613040924\n",
      "# of batch: 1402\n",
      "train accuracy: 0.91540986\n",
      "One batch end: 1069.5220816135406\n",
      "# of batch: 1403\n",
      "train accuracy: 0.9154131\n",
      "One batch end: 1070.422081232071\n",
      "# of batch: 1404\n",
      "train accuracy: 0.9154591\n",
      "One batch end: 1071.3210804462433\n",
      "# of batch: 1405\n",
      "train accuracy: 0.915505\n",
      "One batch end: 1072.1950798034668\n",
      "# of batch: 1406\n",
      "train accuracy: 0.9155366\n",
      "One batch end: 1073.1025919914246\n",
      "# of batch: 1407\n",
      "train accuracy: 0.91552556\n",
      "One batch end: 1074.0210995674133\n",
      "# of batch: 1408\n",
      "train accuracy: 0.91557133\n",
      "One batch end: 1074.9036092758179\n",
      "# of batch: 1409\n",
      "train accuracy: 0.91563123\n",
      "One batch end: 1075.8196086883545\n",
      "# of batch: 1410\n",
      "train accuracy: 0.91567683\n",
      "One batch end: 1076.6891169548035\n",
      "# of batch: 1411\n",
      "train accuracy: 0.91572237\n",
      "One batch end: 1077.5761170387268\n",
      "# of batch: 1412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9157537\n",
      "One batch end: 1078.4731166362762\n",
      "# of batch: 1413\n",
      "train accuracy: 0.91579914\n",
      "One batch end: 1079.390115737915\n",
      "# of batch: 1414\n",
      "train accuracy: 0.9158304\n",
      "One batch end: 1080.3101155757904\n",
      "# of batch: 1415\n",
      "train accuracy: 0.91587573\n",
      "One batch end: 1081.21311378479\n",
      "# of batch: 1416\n",
      "train accuracy: 0.9159351\n",
      "One batch end: 1082.1171135902405\n",
      "# of batch: 1417\n",
      "train accuracy: 0.9159803\n",
      "One batch end: 1083.018473625183\n",
      "# of batch: 1418\n",
      "train accuracy: 0.9160113\n",
      "One batch end: 1083.912985086441\n",
      "# of batch: 1419\n",
      "train accuracy: 0.9160704\n",
      "One batch end: 1084.817497253418\n",
      "# of batch: 1420\n",
      "train accuracy: 0.91610134\n",
      "One batch end: 1085.711497783661\n",
      "# of batch: 1421\n",
      "train accuracy: 0.91611814\n",
      "One batch end: 1086.6344962120056\n",
      "# of batch: 1422\n",
      "train accuracy: 0.91613495\n",
      "One batch end: 1087.5364964008331\n",
      "# of batch: 1423\n",
      "train accuracy: 0.91619384\n",
      "One batch end: 1088.4630026817322\n",
      "# of batch: 1424\n",
      "train accuracy: 0.91619647\n",
      "One batch end: 1089.3955092430115\n",
      "# of batch: 1425\n",
      "train accuracy: 0.9162272\n",
      "One batch end: 1090.3105084896088\n",
      "# of batch: 1426\n",
      "train accuracy: 0.91628593\n",
      "One batch end: 1091.243507862091\n",
      "# of batch: 1427\n",
      "train accuracy: 0.9163025\n",
      "One batch end: 1092.2205073833466\n",
      "# of batch: 1428\n",
      "train accuracy: 0.9163051\n",
      "One batch end: 1093.237875699997\n",
      "# of batch: 1429\n",
      "train accuracy: 0.91633564\n",
      "One batch end: 1094.287876367569\n",
      "# of batch: 1430\n",
      "train accuracy: 0.91638017\n",
      "One batch end: 1095.2685267925262\n",
      "# of batch: 1431\n",
      "train accuracy: 0.9164246\n",
      "One batch end: 1096.2105252742767\n",
      "# of batch: 1432\n",
      "train accuracy: 0.916455\n",
      "One batch end: 1097.1415934562683\n",
      "# of batch: 1433\n",
      "train accuracy: 0.9164575\n",
      "One batch end: 1098.0801031589508\n",
      "# of batch: 1434\n",
      "train accuracy: 0.9164878\n",
      "One batch end: 1099.0171024799347\n",
      "# of batch: 1435\n",
      "train accuracy: 0.9165181\n",
      "One batch end: 1099.9256129264832\n",
      "# of batch: 1436\n",
      "train accuracy: 0.91652054\n",
      "One batch end: 1100.8436124324799\n",
      "# of batch: 1437\n",
      "train accuracy: 0.91653687\n",
      "One batch end: 1101.820123910904\n",
      "# of batch: 1438\n",
      "train accuracy: 0.916581\n",
      "One batch end: 1102.7896814346313\n",
      "# of batch: 1439\n",
      "train accuracy: 0.9165695\n",
      "One batch end: 1103.737679719925\n",
      "# of batch: 1440\n",
      "train accuracy: 0.91661346\n",
      "One batch end: 1104.6616790294647\n",
      "# of batch: 1441\n",
      "train accuracy: 0.91664356\n",
      "One batch end: 1105.5752711296082\n",
      "# of batch: 1442\n",
      "train accuracy: 0.9166597\n",
      "One batch end: 1106.4762697219849\n",
      "# of batch: 1443\n",
      "train accuracy: 0.91671747\n",
      "One batch end: 1107.3762693405151\n",
      "# of batch: 1444\n",
      "train accuracy: 0.9167474\n",
      "One batch end: 1108.2782685756683\n",
      "# of batch: 1445\n",
      "train accuracy: 0.91679114\n",
      "One batch end: 1109.170774936676\n",
      "# of batch: 1446\n",
      "train accuracy: 0.916821\n",
      "One batch end: 1110.0698127746582\n",
      "# of batch: 1447\n",
      "train accuracy: 0.9168508\n",
      "One batch end: 1110.9517738819122\n",
      "# of batch: 1448\n",
      "train accuracy: 0.916853\n",
      "One batch end: 1111.871773481369\n",
      "# of batch: 1449\n",
      "train accuracy: 0.9168965\n",
      "One batch end: 1112.7777736186981\n",
      "# of batch: 1450\n",
      "train accuracy: 0.9169538\n",
      "One batch end: 1113.6691453456879\n",
      "# of batch: 1451\n",
      "train accuracy: 0.917011\n",
      "One batch end: 1114.594652414322\n",
      "# of batch: 1452\n",
      "train accuracy: 0.9170406\n",
      "One batch end: 1115.5346524715424\n",
      "# of batch: 1453\n",
      "train accuracy: 0.9170977\n",
      "One batch end: 1116.4726512432098\n",
      "# of batch: 1454\n",
      "train accuracy: 0.9171546\n",
      "One batch end: 1117.4766504764557\n",
      "# of batch: 1455\n",
      "train accuracy: 0.91717035\n",
      "One batch end: 1118.527549982071\n",
      "# of batch: 1456\n",
      "train accuracy: 0.9172272\n",
      "One batch end: 1119.5115804672241\n",
      "# of batch: 1457\n",
      "train accuracy: 0.91728395\n",
      "One batch end: 1120.4390895366669\n",
      "# of batch: 1458\n",
      "train accuracy: 0.9172858\n",
      "One batch end: 1121.3730890750885\n",
      "# of batch: 1459\n",
      "train accuracy: 0.9173425\n",
      "One batch end: 1122.3150882720947\n",
      "# of batch: 1460\n",
      "train accuracy: 0.91739905\n",
      "One batch end: 1123.232171535492\n",
      "# of batch: 1461\n",
      "train accuracy: 0.91745555\n",
      "One batch end: 1124.128170967102\n",
      "# of batch: 1462\n",
      "train accuracy: 0.91751194\n",
      "One batch end: 1125.0746774673462\n",
      "# of batch: 1463\n",
      "train accuracy: 0.9175683\n",
      "One batch end: 1126.0486767292023\n",
      "# of batch: 1464\n",
      "train accuracy: 0.91758364\n",
      "One batch end: 1126.9986760616302\n",
      "# of batch: 1465\n",
      "train accuracy: 0.9176262\n",
      "One batch end: 1127.9326756000519\n",
      "# of batch: 1466\n",
      "train accuracy: 0.91765505\n",
      "One batch end: 1128.9006757736206\n",
      "# of batch: 1467\n",
      "train accuracy: 0.91765666\n",
      "One batch end: 1129.8996744155884\n",
      "# of batch: 1468\n",
      "train accuracy: 0.91765827\n",
      "One batch end: 1130.9416735172272\n",
      "# of batch: 1469\n",
      "train accuracy: 0.91767347\n",
      "One batch end: 1131.90767288208\n",
      "# of batch: 1470\n",
      "train accuracy: 0.91768867\n",
      "One batch end: 1132.8256723880768\n",
      "# of batch: 1471\n",
      "train accuracy: 0.9177174\n",
      "One batch end: 1133.7436726093292\n",
      "# of batch: 1472\n",
      "train accuracy: 0.91775966\n",
      "One batch end: 1134.6826717853546\n",
      "# of batch: 1473\n",
      "train accuracy: 0.91777474\n",
      "One batch end: 1135.5996704101562\n",
      "# of batch: 1474\n",
      "train accuracy: 0.9177898\n",
      "One batch end: 1136.494669675827\n",
      "# of batch: 1475\n",
      "train accuracy: 0.9178049\n",
      "One batch end: 1137.4796690940857\n",
      "# of batch: 1476\n",
      "train accuracy: 0.917847\n",
      "One batch end: 1138.4386694431305\n",
      "# of batch: 1477\n",
      "train accuracy: 0.91784847\n",
      "One batch end: 1139.4066679477692\n",
      "# of batch: 1478\n",
      "train accuracy: 0.91787696\n",
      "One batch end: 1140.3276815414429\n",
      "# of batch: 1479\n",
      "train accuracy: 0.9178784\n",
      "One batch end: 1141.2491929531097\n",
      "# of batch: 1480\n",
      "train accuracy: 0.9178933\n",
      "One batch end: 1142.2551922798157\n",
      "# of batch: 1481\n",
      "train accuracy: 0.91790825\n",
      "One batch end: 1143.2991926670074\n",
      "# of batch: 1482\n",
      "train accuracy: 0.9179501\n",
      "One batch end: 1144.2927029132843\n",
      "# of batch: 1483\n",
      "train accuracy: 0.91799194\n",
      "One batch end: 1145.189733505249\n",
      "# of batch: 1484\n",
      "train accuracy: 0.91803366\n",
      "One batch end: 1146.072732925415\n",
      "# of batch: 1485\n",
      "train accuracy: 0.9180754\n",
      "One batch end: 1146.9617323875427\n",
      "# of batch: 1486\n",
      "train accuracy: 0.918117\n",
      "One batch end: 1147.8447318077087\n",
      "# of batch: 1487\n",
      "train accuracy: 0.9181452\n",
      "One batch end: 1148.7567310333252\n",
      "# of batch: 1488\n",
      "train accuracy: 0.91814643\n",
      "One batch end: 1149.6548459529877\n",
      "# of batch: 1489\n",
      "train accuracy: 0.9181745\n",
      "One batch end: 1150.531845331192\n",
      "# of batch: 1490\n",
      "train accuracy: 0.9182025\n",
      "One batch end: 1151.4178447723389\n",
      "# of batch: 1491\n",
      "train accuracy: 0.91824394\n",
      "One batch end: 1152.3108439445496\n",
      "# of batch: 1492\n",
      "train accuracy: 0.9182987\n",
      "One batch end: 1153.217354774475\n",
      "# of batch: 1493\n",
      "train accuracy: 0.9183266\n",
      "One batch end: 1154.1163532733917\n",
      "# of batch: 1494\n",
      "train accuracy: 0.91834116\n",
      "One batch end: 1154.9988598823547\n",
      "# of batch: 1495\n",
      "train accuracy: 0.9183957\n",
      "One batch end: 1155.8948595523834\n",
      "# of batch: 1496\n",
      "train accuracy: 0.91842353\n",
      "One batch end: 1156.7468588352203\n",
      "# of batch: 1497\n",
      "train accuracy: 0.9184379\n",
      "One batch end: 1157.611368894577\n",
      "# of batch: 1498\n",
      "train accuracy: 0.9184656\n",
      "One batch end: 1158.4763691425323\n",
      "# of batch: 1499\n",
      "train accuracy: 0.91849333\n",
      "One batch end: 1159.3429944515228\n",
      "# of batch: 1500\n",
      "train accuracy: 0.91853434\n",
      "One batch end: 1160.217993736267\n",
      "# of batch: 1501\n",
      "train accuracy: 0.9185885\n",
      "One batch end: 1161.0929930210114\n",
      "# of batch: 1502\n",
      "train accuracy: 0.9186161\n",
      "One batch end: 1161.9865012168884\n",
      "# of batch: 1503\n",
      "train accuracy: 0.9186569\n",
      "One batch end: 1162.8933284282684\n",
      "# of batch: 1504\n",
      "train accuracy: 0.9186711\n",
      "One batch end: 1163.7883276939392\n",
      "# of batch: 1505\n",
      "train accuracy: 0.91871184\n",
      "One batch end: 1164.673355102539\n",
      "# of batch: 1506\n",
      "train accuracy: 0.9187658\n",
      "One batch end: 1165.570354938507\n",
      "# of batch: 1507\n",
      "train accuracy: 0.91876656\n",
      "One batch end: 1166.475354194641\n",
      "# of batch: 1508\n",
      "train accuracy: 0.9187939\n",
      "One batch end: 1167.3933534622192\n",
      "# of batch: 1509\n",
      "train accuracy: 0.9188477\n",
      "One batch end: 1168.2993531227112\n",
      "# of batch: 1510\n",
      "train accuracy: 0.91888815\n",
      "One batch end: 1169.2083525657654\n",
      "# of batch: 1511\n",
      "train accuracy: 0.91892856\n",
      "One batch end: 1170.1148598194122\n",
      "# of batch: 1512\n",
      "train accuracy: 0.9189425\n",
      "One batch end: 1171.02285861969\n",
      "# of batch: 1513\n",
      "train accuracy: 0.91896963\n",
      "One batch end: 1171.8908579349518\n",
      "# of batch: 1514\n",
      "train accuracy: 0.9190099\n",
      "One batch end: 1172.7668573856354\n",
      "# of batch: 1515\n",
      "train accuracy: 0.91902375\n",
      "One batch end: 1173.658856868744\n",
      "# of batch: 1516\n",
      "train accuracy: 0.9190639\n",
      "One batch end: 1174.5648562908173\n",
      "# of batch: 1517\n",
      "train accuracy: 0.91907775\n",
      "One batch end: 1175.4744079113007\n",
      "# of batch: 1518\n",
      "train accuracy: 0.919131\n",
      "One batch end: 1176.3784074783325\n",
      "# of batch: 1519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9191842\n",
      "One batch end: 1177.2869138717651\n",
      "# of batch: 1520\n",
      "train accuracy: 0.91921103\n",
      "One batch end: 1178.1944644451141\n",
      "# of batch: 1521\n",
      "train accuracy: 0.91926414\n",
      "One batch end: 1179.1129713058472\n",
      "# of batch: 1522\n",
      "train accuracy: 0.9192646\n",
      "One batch end: 1180.0689868927002\n",
      "# of batch: 1523\n",
      "train accuracy: 0.9193045\n",
      "One batch end: 1180.961986541748\n",
      "# of batch: 1524\n",
      "train accuracy: 0.91935736\n",
      "One batch end: 1181.839495897293\n",
      "# of batch: 1525\n",
      "train accuracy: 0.9193578\n",
      "One batch end: 1182.7314953804016\n",
      "# of batch: 1526\n",
      "train accuracy: 0.9193844\n",
      "One batch end: 1183.632001876831\n",
      "# of batch: 1527\n",
      "train accuracy: 0.919411\n",
      "One batch end: 1184.565001487732\n",
      "# of batch: 1528\n",
      "train accuracy: 0.91945064\n",
      "One batch end: 1185.4670016765594\n",
      "# of batch: 1529\n",
      "train accuracy: 0.9194771\n",
      "One batch end: 1186.3770003318787\n",
      "# of batch: 1530\n",
      "train accuracy: 0.91952974\n",
      "One batch end: 1187.276999950409\n",
      "# of batch: 1531\n",
      "train accuracy: 0.9195431\n",
      "One batch end: 1188.205999135971\n",
      "# of batch: 1532\n",
      "train accuracy: 0.9195695\n",
      "One batch end: 1189.1200001239777\n",
      "# of batch: 1533\n",
      "train accuracy: 0.91959584\n",
      "One batch end: 1190.0215101242065\n",
      "# of batch: 1534\n",
      "train accuracy: 0.9196352\n",
      "One batch end: 1190.9115085601807\n",
      "# of batch: 1535\n",
      "train accuracy: 0.91966146\n",
      "One batch end: 1191.8155081272125\n",
      "# of batch: 1536\n",
      "train accuracy: 0.9196747\n",
      "One batch end: 1192.7415075302124\n",
      "# of batch: 1537\n",
      "train accuracy: 0.9197139\n",
      "One batch end: 1193.6605069637299\n",
      "# of batch: 1538\n",
      "train accuracy: 0.91976607\n",
      "One batch end: 1194.5465064048767\n",
      "# of batch: 1539\n",
      "train accuracy: 0.91972727\n",
      "One batch end: 1195.4459624290466\n",
      "# of batch: 1540\n",
      "train accuracy: 0.91975343\n",
      "One batch end: 1196.35196185112\n",
      "# of batch: 1541\n",
      "train accuracy: 0.91980547\n",
      "One batch end: 1197.263961315155\n",
      "# of batch: 1542\n",
      "train accuracy: 0.91985744\n",
      "One batch end: 1198.150499343872\n",
      "# of batch: 1543\n",
      "train accuracy: 0.91989636\n",
      "One batch end: 1199.0524988174438\n",
      "# of batch: 1544\n",
      "train accuracy: 0.9199353\n",
      "One batch end: 1199.9570055007935\n",
      "# of batch: 1545\n",
      "train accuracy: 0.9199483\n",
      "One batch end: 1200.8850049972534\n",
      "# of batch: 1546\n",
      "train accuracy: 0.91997415\n",
      "One batch end: 1201.7920043468475\n",
      "# of batch: 1547\n",
      "train accuracy: 0.92\n",
      "One batch end: 1202.6950047016144\n",
      "# of batch: 1548\n",
      "train accuracy: 0.92005163\n",
      "One batch end: 1203.5730032920837\n",
      "# of batch: 1549\n",
      "train accuracy: 0.92007744\n",
      "One batch end: 1204.4550025463104\n",
      "# of batch: 1550\n",
      "train accuracy: 0.92011607\n",
      "One batch end: 1205.3360545635223\n",
      "# of batch: 1551\n",
      "train accuracy: 0.92014176\n",
      "One batch end: 1206.2560539245605\n",
      "# of batch: 1552\n",
      "train accuracy: 0.92016745\n",
      "One batch end: 1207.1805620193481\n",
      "# of batch: 1553\n",
      "train accuracy: 0.9201802\n",
      "One batch end: 1208.0905599594116\n",
      "# of batch: 1554\n",
      "train accuracy: 0.9202315\n",
      "One batch end: 1209.0085608959198\n",
      "# of batch: 1555\n",
      "train accuracy: 0.9202699\n",
      "One batch end: 1209.8995587825775\n",
      "# of batch: 1556\n",
      "train accuracy: 0.9203083\n",
      "One batch end: 1210.7815585136414\n",
      "# of batch: 1557\n",
      "train accuracy: 0.9203466\n",
      "One batch end: 1211.6785583496094\n",
      "# of batch: 1558\n",
      "train accuracy: 0.9203849\n",
      "One batch end: 1212.5830686092377\n",
      "# of batch: 1559\n",
      "train accuracy: 0.9204103\n",
      "One batch end: 1213.51406788826\n",
      "# of batch: 1560\n",
      "train accuracy: 0.9204484\n",
      "One batch end: 1214.462067604065\n",
      "# of batch: 1561\n",
      "train accuracy: 0.92048657\n",
      "One batch end: 1215.4686217308044\n",
      "# of batch: 1562\n",
      "train accuracy: 0.92052466\n",
      "One batch end: 1216.5006215572357\n",
      "# of batch: 1563\n",
      "train accuracy: 0.92057544\n",
      "One batch end: 1217.4930992126465\n",
      "# of batch: 1564\n",
      "train accuracy: 0.92060065\n",
      "One batch end: 1218.4800987243652\n",
      "# of batch: 1565\n",
      "train accuracy: 0.92063856\n",
      "One batch end: 1219.4080979824066\n",
      "# of batch: 1566\n",
      "train accuracy: 0.92067647\n",
      "One batch end: 1220.3576157093048\n",
      "# of batch: 1567\n",
      "train accuracy: 0.9207015\n",
      "One batch end: 1221.284613609314\n",
      "# of batch: 1568\n",
      "train accuracy: 0.92071384\n",
      "One batch end: 1222.2111201286316\n",
      "# of batch: 1569\n",
      "train accuracy: 0.9207643\n",
      "One batch end: 1223.1131193637848\n",
      "# of batch: 1570\n",
      "train accuracy: 0.92080206\n",
      "One batch end: 1224.0591187477112\n",
      "# of batch: 1571\n",
      "train accuracy: 0.92083967\n",
      "One batch end: 1224.9946539402008\n",
      "# of batch: 1572\n",
      "train accuracy: 0.9208773\n",
      "One batch end: 1225.8816530704498\n",
      "# of batch: 1573\n",
      "train accuracy: 0.92088944\n",
      "One batch end: 1226.7546525001526\n",
      "# of batch: 1574\n",
      "train accuracy: 0.9209397\n",
      "One batch end: 1227.6321594715118\n",
      "# of batch: 1575\n",
      "train accuracy: 0.9209645\n",
      "One batch end: 1228.5324184894562\n",
      "# of batch: 1576\n",
      "train accuracy: 0.9209892\n",
      "One batch end: 1229.4824159145355\n",
      "# of batch: 1577\n",
      "train accuracy: 0.9209886\n",
      "One batch end: 1230.4211120605469\n",
      "# of batch: 1578\n",
      "train accuracy: 0.9210386\n",
      "One batch end: 1231.3756148815155\n",
      "# of batch: 1579\n",
      "train accuracy: 0.92107594\n",
      "One batch end: 1232.3086173534393\n",
      "# of batch: 1580\n",
      "train accuracy: 0.92110056\n",
      "One batch end: 1233.2106127738953\n",
      "# of batch: 1581\n",
      "train accuracy: 0.9211252\n",
      "One batch end: 1234.1352043151855\n",
      "# of batch: 1582\n",
      "train accuracy: 0.92116237\n",
      "One batch end: 1235.0632002353668\n",
      "# of batch: 1583\n",
      "train accuracy: 0.9211742\n",
      "One batch end: 1236.02019906044\n",
      "# of batch: 1584\n",
      "train accuracy: 0.921224\n",
      "One batch end: 1237.0167214870453\n",
      "# of batch: 1585\n",
      "train accuracy: 0.92124844\n",
      "One batch end: 1238.0537250041962\n",
      "# of batch: 1586\n",
      "train accuracy: 0.92128545\n",
      "One batch end: 1239.0182461738586\n",
      "# of batch: 1587\n",
      "train accuracy: 0.9213098\n",
      "One batch end: 1239.9657626152039\n",
      "# of batch: 1588\n",
      "train accuracy: 0.9213216\n",
      "One batch end: 1240.8912827968597\n",
      "# of batch: 1589\n",
      "train accuracy: 0.92135847\n",
      "One batch end: 1241.8142867088318\n",
      "# of batch: 1590\n",
      "train accuracy: 0.92139536\n",
      "One batch end: 1242.7082862854004\n",
      "# of batch: 1591\n",
      "train accuracy: 0.92143214\n",
      "One batch end: 1243.6048080921173\n",
      "# of batch: 1592\n",
      "train accuracy: 0.9214564\n",
      "One batch end: 1244.5438072681427\n",
      "# of batch: 1593\n",
      "train accuracy: 0.92150563\n",
      "One batch end: 1245.4629549980164\n",
      "# of batch: 1594\n",
      "train accuracy: 0.9215047\n",
      "One batch end: 1246.3834767341614\n",
      "# of batch: 1595\n",
      "train accuracy: 0.9215288\n",
      "One batch end: 1247.3104763031006\n",
      "# of batch: 1596\n",
      "train accuracy: 0.9215404\n",
      "One batch end: 1248.2324755191803\n",
      "# of batch: 1597\n",
      "train accuracy: 0.9215269\n",
      "One batch end: 1249.1889960765839\n",
      "# of batch: 1598\n",
      "train accuracy: 0.9215009\n",
      "One batch end: 1250.1437423229218\n",
      "# of batch: 1599\n",
      "train accuracy: 0.9215375\n",
      "One batch end: 1251.1387422084808\n",
      "# of batch: 1600\n",
      "train accuracy: 0.921549\n",
      "One batch end: 1252.1737406253815\n",
      "# of batch: 1601\n",
      "train accuracy: 0.92157304\n",
      "One batch end: 1253.1497404575348\n",
      "# of batch: 1602\n",
      "train accuracy: 0.921572\n",
      "One batch end: 1254.0612637996674\n",
      "# of batch: 1603\n",
      "train accuracy: 0.92158353\n",
      "One batch end: 1254.9703497886658\n",
      "# of batch: 1604\n",
      "train accuracy: 0.92159504\n",
      "One batch end: 1255.9043481349945\n",
      "# of batch: 1605\n",
      "train accuracy: 0.9216314\n",
      "One batch end: 1256.7883474826813\n",
      "# of batch: 1606\n",
      "train accuracy: 0.92164284\n",
      "One batch end: 1257.701871395111\n",
      "# of batch: 1607\n",
      "train accuracy: 0.9216791\n",
      "One batch end: 1258.6158707141876\n",
      "# of batch: 1608\n",
      "train accuracy: 0.92169046\n",
      "One batch end: 1259.5109314918518\n",
      "# of batch: 1609\n",
      "train accuracy: 0.9217143\n",
      "One batch end: 1260.3969304561615\n",
      "# of batch: 1610\n",
      "train accuracy: 0.9217505\n",
      "One batch end: 1261.2879362106323\n",
      "# of batch: 1611\n",
      "train accuracy: 0.9217742\n",
      "One batch end: 1262.1759297847748\n",
      "# of batch: 1612\n",
      "train accuracy: 0.92181027\n",
      "One batch end: 1263.126540184021\n",
      "# of batch: 1613\n",
      "train accuracy: 0.9218092\n",
      "One batch end: 1264.1240758895874\n",
      "# of batch: 1614\n",
      "train accuracy: 0.9218452\n",
      "One batch end: 1265.127074956894\n",
      "# of batch: 1615\n",
      "train accuracy: 0.9218812\n",
      "One batch end: 1266.0680742263794\n",
      "# of batch: 1616\n",
      "train accuracy: 0.9219295\n",
      "One batch end: 1266.9695839881897\n",
      "# of batch: 1617\n",
      "train accuracy: 0.92196536\n",
      "One batch end: 1267.8525791168213\n",
      "# of batch: 1618\n",
      "train accuracy: 0.9219889\n",
      "One batch end: 1268.7665820121765\n",
      "# of batch: 1619\n",
      "train accuracy: 0.9220123\n",
      "One batch end: 1269.6885769367218\n",
      "# of batch: 1620\n",
      "train accuracy: 0.9220481\n",
      "One batch end: 1270.5761032104492\n",
      "# of batch: 1621\n",
      "train accuracy: 0.9220592\n",
      "One batch end: 1271.482102394104\n",
      "# of batch: 1622\n",
      "train accuracy: 0.92208254\n",
      "One batch end: 1272.3831021785736\n",
      "# of batch: 1623\n",
      "train accuracy: 0.92211825\n",
      "One batch end: 1273.2776055335999\n",
      "# of batch: 1624\n",
      "train accuracy: 0.92215383\n",
      "One batch end: 1274.162607908249\n",
      "# of batch: 1625\n",
      "train accuracy: 0.9221894\n",
      "One batch end: 1275.0466077327728\n",
      "# of batch: 1626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9221881\n",
      "One batch end: 1276.0102128982544\n",
      "# of batch: 1627\n",
      "train accuracy: 0.92218673\n",
      "One batch end: 1276.971207857132\n",
      "# of batch: 1628\n",
      "train accuracy: 0.9222222\n",
      "One batch end: 1277.890733242035\n",
      "# of batch: 1629\n",
      "train accuracy: 0.9222454\n",
      "One batch end: 1278.804728269577\n",
      "# of batch: 1630\n",
      "train accuracy: 0.922244\n",
      "One batch end: 1279.7307317256927\n",
      "# of batch: 1631\n",
      "train accuracy: 0.9222917\n",
      "One batch end: 1280.646330833435\n",
      "# of batch: 1632\n",
      "train accuracy: 0.92231476\n",
      "One batch end: 1281.5373349189758\n",
      "# of batch: 1633\n",
      "train accuracy: 0.92233783\n",
      "One batch end: 1282.433852672577\n",
      "# of batch: 1634\n",
      "train accuracy: 0.9223731\n",
      "One batch end: 1283.2968490123749\n",
      "# of batch: 1635\n",
      "train accuracy: 0.9223716\n",
      "One batch end: 1284.1668438911438\n",
      "# of batch: 1636\n",
      "train accuracy: 0.92239463\n",
      "One batch end: 1285.0223577022552\n",
      "# of batch: 1637\n",
      "train accuracy: 0.92240536\n",
      "One batch end: 1285.9159381389618\n",
      "# of batch: 1638\n",
      "train accuracy: 0.9224283\n",
      "One batch end: 1286.7949335575104\n",
      "# of batch: 1639\n",
      "train accuracy: 0.92243904\n",
      "One batch end: 1287.679459810257\n",
      "# of batch: 1640\n",
      "train accuracy: 0.9224497\n",
      "One batch end: 1288.5664591789246\n",
      "# of batch: 1641\n",
      "train accuracy: 0.92246044\n",
      "One batch end: 1289.4534542560577\n",
      "# of batch: 1642\n",
      "train accuracy: 0.9224954\n",
      "One batch end: 1290.303768157959\n",
      "# of batch: 1643\n",
      "train accuracy: 0.9225304\n",
      "One batch end: 1291.1927676200867\n",
      "# of batch: 1644\n",
      "train accuracy: 0.9225775\n",
      "One batch end: 1292.110762834549\n",
      "# of batch: 1645\n",
      "train accuracy: 0.9225881\n",
      "One batch end: 1292.9852888584137\n",
      "# of batch: 1646\n",
      "train accuracy: 0.9226351\n",
      "One batch end: 1293.8812882900238\n",
      "# of batch: 1647\n",
      "train accuracy: 0.9226578\n",
      "One batch end: 1294.7752833366394\n",
      "# of batch: 1648\n",
      "train accuracy: 0.9226562\n",
      "One batch end: 1295.651282787323\n",
      "# of batch: 1649\n",
      "train accuracy: 0.92267877\n",
      "One batch end: 1296.5423684120178\n",
      "# of batch: 1650\n",
      "train accuracy: 0.9227014\n",
      "One batch end: 1297.4283635616302\n",
      "# of batch: 1651\n",
      "train accuracy: 0.92273605\n",
      "One batch end: 1298.3683631420135\n",
      "# of batch: 1652\n",
      "train accuracy: 0.92278284\n",
      "One batch end: 1299.2983672618866\n",
      "# of batch: 1653\n",
      "train accuracy: 0.92276907\n",
      "One batch end: 1300.2433660030365\n",
      "# of batch: 1654\n",
      "train accuracy: 0.92279154\n",
      "One batch end: 1301.1358754634857\n",
      "# of batch: 1655\n",
      "train accuracy: 0.922814\n",
      "One batch end: 1302.0278706550598\n",
      "# of batch: 1656\n",
      "train accuracy: 0.9228364\n",
      "One batch end: 1302.9653923511505\n",
      "# of batch: 1657\n",
      "train accuracy: 0.92287093\n",
      "One batch end: 1303.9104008674622\n",
      "# of batch: 1658\n",
      "train accuracy: 0.9229174\n",
      "One batch end: 1304.8293929100037\n",
      "# of batch: 1659\n",
      "train accuracy: 0.92292774\n",
      "One batch end: 1305.7503917217255\n",
      "# of batch: 1660\n",
      "train accuracy: 0.9229741\n",
      "One batch end: 1306.687391281128\n",
      "# of batch: 1661\n",
      "train accuracy: 0.9229964\n",
      "One batch end: 1307.688899755478\n",
      "# of batch: 1662\n",
      "train accuracy: 0.9230066\n",
      "One batch end: 1308.694444179535\n",
      "# of batch: 1663\n",
      "train accuracy: 0.92301685\n",
      "One batch end: 1309.708443403244\n",
      "# of batch: 1664\n",
      "train accuracy: 0.92305106\n",
      "One batch end: 1310.6794428825378\n",
      "# of batch: 1665\n",
      "train accuracy: 0.92306125\n",
      "One batch end: 1311.6011610031128\n",
      "# of batch: 1666\n",
      "train accuracy: 0.9231074\n",
      "One batch end: 1312.5061633586884\n",
      "# of batch: 1667\n",
      "train accuracy: 0.9231295\n",
      "One batch end: 1313.4241635799408\n",
      "# of batch: 1668\n",
      "train accuracy: 0.9231756\n",
      "One batch end: 1314.3301622867584\n",
      "# of batch: 1669\n",
      "train accuracy: 0.9231976\n",
      "One batch end: 1315.2401616573334\n",
      "# of batch: 1670\n",
      "train accuracy: 0.9232316\n",
      "One batch end: 1316.1881568431854\n",
      "# of batch: 1671\n",
      "train accuracy: 0.9232656\n",
      "One batch end: 1317.1001720428467\n",
      "# of batch: 1672\n",
      "train accuracy: 0.92327553\n",
      "One batch end: 1318.0111711025238\n",
      "# of batch: 1673\n",
      "train accuracy: 0.92330945\n",
      "One batch end: 1318.915170431137\n",
      "# of batch: 1674\n",
      "train accuracy: 0.9233313\n",
      "One batch end: 1319.8271701335907\n",
      "# of batch: 1675\n",
      "train accuracy: 0.9233652\n",
      "One batch end: 1320.75039601326\n",
      "# of batch: 1676\n",
      "train accuracy: 0.92341083\n",
      "One batch end: 1321.663723707199\n",
      "# of batch: 1677\n",
      "train accuracy: 0.92344457\n",
      "One batch end: 1322.5597269535065\n",
      "# of batch: 1678\n",
      "train accuracy: 0.92347825\n",
      "One batch end: 1323.491242647171\n",
      "# of batch: 1679\n",
      "train accuracy: 0.9234762\n",
      "One batch end: 1324.4182379245758\n",
      "# of batch: 1680\n",
      "train accuracy: 0.9234979\n",
      "One batch end: 1325.325241804123\n",
      "# of batch: 1681\n",
      "train accuracy: 0.9235196\n",
      "One batch end: 1326.2303221225739\n",
      "# of batch: 1682\n",
      "train accuracy: 0.92355317\n",
      "One batch end: 1327.156831741333\n",
      "# of batch: 1683\n",
      "train accuracy: 0.9235867\n",
      "One batch end: 1328.095831155777\n",
      "# of batch: 1684\n",
      "train accuracy: 0.923632\n",
      "One batch end: 1329.02783036232\n",
      "# of batch: 1685\n",
      "train accuracy: 0.9236299\n",
      "One batch end: 1329.949345111847\n",
      "# of batch: 1686\n",
      "train accuracy: 0.9236633\n",
      "One batch end: 1330.8698649406433\n",
      "# of batch: 1687\n",
      "train accuracy: 0.92370856\n",
      "One batch end: 1331.81347322464\n",
      "# of batch: 1688\n",
      "train accuracy: 0.92373\n",
      "One batch end: 1332.7464728355408\n",
      "# of batch: 1689\n",
      "train accuracy: 0.9237515\n",
      "One batch end: 1333.686471939087\n",
      "# of batch: 1690\n",
      "train accuracy: 0.92378473\n",
      "One batch end: 1334.612114906311\n",
      "# of batch: 1691\n",
      "train accuracy: 0.92380613\n",
      "One batch end: 1335.5251185894012\n",
      "# of batch: 1692\n",
      "train accuracy: 0.92385113\n",
      "One batch end: 1336.4517149925232\n",
      "# of batch: 1693\n",
      "train accuracy: 0.9238961\n",
      "One batch end: 1337.383718252182\n",
      "# of batch: 1694\n",
      "train accuracy: 0.9239292\n",
      "One batch end: 1338.2962350845337\n",
      "# of batch: 1695\n",
      "train accuracy: 0.9239387\n",
      "One batch end: 1339.198234796524\n",
      "# of batch: 1696\n",
      "train accuracy: 0.92392457\n",
      "One batch end: 1340.1122381687164\n",
      "# of batch: 1697\n",
      "train accuracy: 0.92393404\n",
      "One batch end: 1341.0539164543152\n",
      "# of batch: 1698\n",
      "train accuracy: 0.92396706\n",
      "One batch end: 1342.0049154758453\n",
      "# of batch: 1699\n",
      "train accuracy: 0.924\n",
      "One batch end: 1342.9449145793915\n",
      "# of batch: 1700\n",
      "train accuracy: 0.92404467\n",
      "One batch end: 1343.8634359836578\n",
      "# of batch: 1701\n",
      "train accuracy: 0.9240776\n",
      "One batch end: 1344.8134365081787\n",
      "# of batch: 1702\n",
      "train accuracy: 0.92412215\n",
      "One batch end: 1345.756032705307\n",
      "# of batch: 1703\n",
      "train accuracy: 0.9241667\n",
      "One batch end: 1346.7006647586823\n",
      "# of batch: 1704\n",
      "train accuracy: 0.92421114\n",
      "One batch end: 1347.743664264679\n",
      "# of batch: 1705\n",
      "train accuracy: 0.92424387\n",
      "One batch end: 1348.7731864452362\n",
      "# of batch: 1706\n",
      "train accuracy: 0.92425305\n",
      "One batch end: 1349.8611850738525\n",
      "# of batch: 1707\n",
      "train accuracy: 0.92429745\n",
      "One batch end: 1350.9811842441559\n",
      "# of batch: 1708\n",
      "train accuracy: 0.92430663\n",
      "One batch end: 1351.9581832885742\n",
      "# of batch: 1709\n",
      "train accuracy: 0.9243275\n",
      "One batch end: 1352.9072155952454\n",
      "# of batch: 1710\n",
      "train accuracy: 0.92436004\n",
      "One batch end: 1353.8288884162903\n",
      "# of batch: 1711\n",
      "train accuracy: 0.92436916\n",
      "One batch end: 1354.7588877677917\n",
      "# of batch: 1712\n",
      "train accuracy: 0.92440164\n",
      "One batch end: 1355.660887002945\n",
      "# of batch: 1713\n",
      "train accuracy: 0.92443407\n",
      "One batch end: 1356.5840084552765\n",
      "# of batch: 1714\n",
      "train accuracy: 0.9244431\n",
      "One batch end: 1357.5530107021332\n",
      "# of batch: 1715\n",
      "train accuracy: 0.9244872\n",
      "One batch end: 1358.4722287654877\n",
      "# of batch: 1716\n",
      "train accuracy: 0.92453116\n",
      "One batch end: 1359.4183232784271\n",
      "# of batch: 1717\n",
      "train accuracy: 0.9245751\n",
      "One batch end: 1360.337322473526\n",
      "# of batch: 1718\n",
      "train accuracy: 0.9245957\n",
      "One batch end: 1361.3086042404175\n",
      "# of batch: 1719\n",
      "train accuracy: 0.9246163\n",
      "One batch end: 1362.3046038150787\n",
      "# of batch: 1720\n",
      "train accuracy: 0.92463684\n",
      "One batch end: 1363.3471262454987\n",
      "# of batch: 1721\n",
      "train accuracy: 0.92466897\n",
      "One batch end: 1364.3421249389648\n",
      "# of batch: 1722\n",
      "train accuracy: 0.92464304\n",
      "One batch end: 1365.2741243839264\n",
      "# of batch: 1723\n",
      "train accuracy: 0.92467517\n",
      "One batch end: 1366.1921677589417\n",
      "# of batch: 1724\n",
      "train accuracy: 0.9246957\n",
      "One batch end: 1367.10116648674\n",
      "# of batch: 1725\n",
      "train accuracy: 0.9247045\n",
      "One batch end: 1368.0361654758453\n",
      "# of batch: 1726\n",
      "train accuracy: 0.9246902\n",
      "One batch end: 1368.967248916626\n",
      "# of batch: 1727\n",
      "train accuracy: 0.92469907\n",
      "One batch end: 1369.8582525253296\n",
      "# of batch: 1728\n",
      "train accuracy: 0.9247311\n",
      "One batch end: 1370.7385470867157\n",
      "# of batch: 1729\n",
      "train accuracy: 0.9247746\n",
      "One batch end: 1371.6679508686066\n",
      "# of batch: 1730\n",
      "train accuracy: 0.9247487\n",
      "One batch end: 1372.567954301834\n",
      "# of batch: 1731\n",
      "train accuracy: 0.9247806\n",
      "One batch end: 1373.4469537734985\n",
      "# of batch: 1732\n",
      "train accuracy: 0.92478937\n",
      "One batch end: 1374.3811087608337\n",
      "# of batch: 1733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9248097\n",
      "One batch end: 1375.314112663269\n",
      "# of batch: 1734\n",
      "train accuracy: 0.92481846\n",
      "One batch end: 1376.2521073818207\n",
      "# of batch: 1735\n",
      "train accuracy: 0.9248272\n",
      "One batch end: 1377.1891069412231\n",
      "# of batch: 1736\n",
      "train accuracy: 0.9248474\n",
      "One batch end: 1378.113110780716\n",
      "# of batch: 1737\n",
      "train accuracy: 0.9248792\n",
      "One batch end: 1379.0301060676575\n",
      "# of batch: 1738\n",
      "train accuracy: 0.92489934\n",
      "One batch end: 1379.9441058635712\n",
      "# of batch: 1739\n",
      "train accuracy: 0.92491955\n",
      "One batch end: 1380.8444283008575\n",
      "# of batch: 1740\n",
      "train accuracy: 0.9249397\n",
      "One batch end: 1381.7784276008606\n",
      "# of batch: 1741\n",
      "train accuracy: 0.9249713\n",
      "One batch end: 1382.7364268302917\n",
      "# of batch: 1742\n",
      "train accuracy: 0.9250029\n",
      "One batch end: 1383.736426115036\n",
      "# of batch: 1743\n",
      "train accuracy: 0.92504585\n",
      "One batch end: 1384.7444298267365\n",
      "# of batch: 1744\n",
      "train accuracy: 0.9250774\n",
      "One batch end: 1385.6979513168335\n",
      "# of batch: 1745\n",
      "train accuracy: 0.9251088\n",
      "One batch end: 1386.6121997833252\n",
      "# of batch: 1746\n",
      "train accuracy: 0.92510587\n",
      "One batch end: 1387.526199579239\n",
      "# of batch: 1747\n",
      "train accuracy: 0.9251373\n",
      "One batch end: 1388.435199022293\n",
      "# of batch: 1748\n",
      "train accuracy: 0.9251458\n",
      "One batch end: 1389.3552026748657\n",
      "# of batch: 1749\n",
      "train accuracy: 0.9251429\n",
      "One batch end: 1390.2922022342682\n",
      "# of batch: 1750\n",
      "train accuracy: 0.9251742\n",
      "One batch end: 1391.221197605133\n",
      "# of batch: 1751\n",
      "train accuracy: 0.9252169\n",
      "One batch end: 1392.141196489334\n",
      "# of batch: 1752\n",
      "train accuracy: 0.92521393\n",
      "One batch end: 1393.0202000141144\n",
      "# of batch: 1753\n",
      "train accuracy: 0.9252337\n",
      "One batch end: 1393.9001967906952\n",
      "# of batch: 1754\n",
      "train accuracy: 0.92525357\n",
      "One batch end: 1394.7871992588043\n",
      "# of batch: 1755\n",
      "train accuracy: 0.92527336\n",
      "One batch end: 1395.7341985702515\n",
      "# of batch: 1756\n",
      "train accuracy: 0.9252703\n",
      "One batch end: 1396.6917293071747\n",
      "# of batch: 1757\n",
      "train accuracy: 0.9252901\n",
      "One batch end: 1397.5817279815674\n",
      "# of batch: 1758\n",
      "train accuracy: 0.9253212\n",
      "One batch end: 1398.4907274246216\n",
      "# of batch: 1759\n",
      "train accuracy: 0.92532957\n",
      "One batch end: 1399.3787274360657\n",
      "# of batch: 1760\n",
      "train accuracy: 0.9253606\n",
      "One batch end: 1400.2607262134552\n",
      "# of batch: 1761\n",
      "train accuracy: 0.92538023\n",
      "One batch end: 1401.1667263507843\n",
      "# of batch: 1762\n",
      "train accuracy: 0.92542255\n",
      "One batch end: 1402.0977256298065\n",
      "# of batch: 1763\n",
      "train accuracy: 0.92545354\n",
      "One batch end: 1403.0527250766754\n",
      "# of batch: 1764\n",
      "train accuracy: 0.92549574\n",
      "One batch end: 1404.019024848938\n",
      "# of batch: 1765\n",
      "train accuracy: 0.92549264\n",
      "One batch end: 1405.0300242900848\n",
      "# of batch: 1766\n",
      "train accuracy: 0.92552346\n",
      "One batch end: 1406.027074098587\n",
      "# of batch: 1767\n",
      "train accuracy: 0.925543\n",
      "One batch end: 1406.976550102234\n",
      "# of batch: 1768\n",
      "train accuracy: 0.9255512\n",
      "One batch end: 1407.930589914322\n",
      "# of batch: 1769\n",
      "train accuracy: 0.9255932\n",
      "One batch end: 1408.8625493049622\n",
      "# of batch: 1770\n",
      "train accuracy: 0.9256352\n",
      "One batch end: 1409.7546684741974\n",
      "# of batch: 1771\n",
      "train accuracy: 0.9256772\n",
      "One batch end: 1410.6336677074432\n",
      "# of batch: 1772\n",
      "train accuracy: 0.92569655\n",
      "One batch end: 1411.5057559013367\n",
      "# of batch: 1773\n",
      "train accuracy: 0.9257046\n",
      "One batch end: 1412.4067549705505\n",
      "# of batch: 1774\n",
      "train accuracy: 0.9257465\n",
      "One batch end: 1413.3127491474152\n",
      "# of batch: 1775\n",
      "train accuracy: 0.92576575\n",
      "One batch end: 1414.2242712974548\n",
      "# of batch: 1776\n",
      "train accuracy: 0.92580754\n",
      "One batch end: 1415.1282749176025\n",
      "# of batch: 1777\n",
      "train accuracy: 0.925838\n",
      "One batch end: 1416.0322706699371\n",
      "# of batch: 1778\n",
      "train accuracy: 0.9258797\n",
      "One batch end: 1416.9269766807556\n",
      "# of batch: 1779\n",
      "train accuracy: 0.9258764\n",
      "One batch end: 1417.8319718837738\n",
      "# of batch: 1780\n",
      "train accuracy: 0.92591804\n",
      "One batch end: 1418.7249703407288\n",
      "# of batch: 1781\n",
      "train accuracy: 0.9259596\n",
      "One batch end: 1419.6419746875763\n",
      "# of batch: 1782\n",
      "train accuracy: 0.9259899\n",
      "One batch end: 1420.557970046997\n",
      "# of batch: 1783\n",
      "train accuracy: 0.92600894\n",
      "One batch end: 1421.4496970176697\n",
      "# of batch: 1784\n",
      "train accuracy: 0.926028\n",
      "One batch end: 1422.3406944274902\n",
      "# of batch: 1785\n",
      "train accuracy: 0.92606944\n",
      "One batch end: 1423.255698442459\n",
      "# of batch: 1786\n",
      "train accuracy: 0.9260325\n",
      "One batch end: 1424.1606976985931\n",
      "# of batch: 1787\n",
      "train accuracy: 0.92605144\n",
      "One batch end: 1425.0687720775604\n",
      "# of batch: 1788\n",
      "train accuracy: 0.9260816\n",
      "One batch end: 1425.9741349220276\n",
      "# of batch: 1789\n",
      "train accuracy: 0.92611176\n",
      "One batch end: 1426.8761293888092\n",
      "# of batch: 1790\n",
      "train accuracy: 0.92609715\n",
      "One batch end: 1427.7396500110626\n",
      "# of batch: 1791\n",
      "train accuracy: 0.92612725\n",
      "One batch end: 1428.644650220871\n",
      "# of batch: 1792\n",
      "train accuracy: 0.92616844\n",
      "One batch end: 1429.5628669261932\n",
      "# of batch: 1793\n",
      "train accuracy: 0.92617613\n",
      "One batch end: 1430.4601473808289\n",
      "# of batch: 1794\n",
      "train accuracy: 0.9262061\n",
      "One batch end: 1431.3821425437927\n",
      "# of batch: 1795\n",
      "train accuracy: 0.9262361\n",
      "One batch end: 1432.2961421012878\n",
      "# of batch: 1796\n",
      "train accuracy: 0.926266\n",
      "One batch end: 1433.1951458454132\n",
      "# of batch: 1797\n",
      "train accuracy: 0.9262848\n",
      "One batch end: 1434.1071407794952\n",
      "# of batch: 1798\n",
      "train accuracy: 0.9263146\n",
      "One batch end: 1435.0148797035217\n",
      "# of batch: 1799\n",
      "train accuracy: 0.9263222\n",
      "One batch end: 1435.9118831157684\n",
      "# of batch: 1800\n",
      "train accuracy: 0.92634094\n",
      "One batch end: 1436.8324012756348\n",
      "# of batch: 1801\n",
      "train accuracy: 0.9263707\n",
      "One batch end: 1437.7003991603851\n",
      "# of batch: 1802\n",
      "train accuracy: 0.92636716\n",
      "One batch end: 1438.6114032268524\n",
      "# of batch: 1803\n",
      "train accuracy: 0.92636365\n",
      "One batch end: 1439.523398399353\n",
      "# of batch: 1804\n",
      "train accuracy: 0.92639333\n",
      "One batch end: 1440.4473974704742\n",
      "# of batch: 1805\n",
      "train accuracy: 0.926423\n",
      "One batch end: 1441.3404014110565\n",
      "# of batch: 1806\n",
      "train accuracy: 0.9264416\n",
      "One batch end: 1442.2580060958862\n",
      "# of batch: 1807\n",
      "train accuracy: 0.9264823\n",
      "One batch end: 1443.171005487442\n",
      "# of batch: 1808\n",
      "train accuracy: 0.926523\n",
      "One batch end: 1444.0730090141296\n",
      "# of batch: 1809\n",
      "train accuracy: 0.92656356\n",
      "One batch end: 1444.9985172748566\n",
      "# of batch: 1810\n",
      "train accuracy: 0.92659307\n",
      "One batch end: 1445.9035165309906\n",
      "# of batch: 1811\n",
      "train accuracy: 0.9266115\n",
      "One batch end: 1446.7905955314636\n",
      "# of batch: 1812\n",
      "train accuracy: 0.9266189\n",
      "One batch end: 1447.6945950984955\n",
      "# of batch: 1813\n",
      "train accuracy: 0.92665935\n",
      "One batch end: 1448.6015901565552\n",
      "# of batch: 1814\n",
      "train accuracy: 0.9266997\n",
      "One batch end: 1449.5055937767029\n",
      "# of batch: 1815\n",
      "train accuracy: 0.926707\n",
      "One batch end: 1450.4295935630798\n",
      "# of batch: 1816\n",
      "train accuracy: 0.92671436\n",
      "One batch end: 1451.3695886135101\n",
      "# of batch: 1817\n",
      "train accuracy: 0.92671067\n",
      "One batch end: 1452.3035924434662\n",
      "# of batch: 1818\n",
      "train accuracy: 0.92672896\n",
      "One batch end: 1453.2355871200562\n",
      "# of batch: 1819\n",
      "train accuracy: 0.9267582\n",
      "One batch end: 1454.1875863075256\n",
      "# of batch: 1820\n",
      "train accuracy: 0.92679846\n",
      "One batch end: 1455.0955865383148\n",
      "# of batch: 1821\n",
      "train accuracy: 0.9268167\n",
      "One batch end: 1456.0075900554657\n",
      "# of batch: 1822\n",
      "train accuracy: 0.9268349\n",
      "One batch end: 1456.8825843334198\n",
      "# of batch: 1823\n",
      "train accuracy: 0.9268092\n",
      "One batch end: 1457.7575886249542\n",
      "# of batch: 1824\n",
      "train accuracy: 0.92683834\n",
      "One batch end: 1458.6385846138\n",
      "# of batch: 1825\n",
      "train accuracy: 0.9268565\n",
      "One batch end: 1459.5485894680023\n",
      "# of batch: 1826\n",
      "train accuracy: 0.9268856\n",
      "One batch end: 1460.421582698822\n",
      "# of batch: 1827\n",
      "train accuracy: 0.92691463\n",
      "One batch end: 1461.3145816326141\n",
      "# of batch: 1828\n",
      "train accuracy: 0.9269546\n",
      "One batch end: 1462.209580898285\n",
      "# of batch: 1829\n",
      "train accuracy: 0.9269727\n",
      "One batch end: 1463.094689130783\n",
      "# of batch: 1830\n",
      "train accuracy: 0.9269907\n",
      "One batch end: 1464.0307443141937\n",
      "# of batch: 1831\n",
      "train accuracy: 0.92703056\n",
      "One batch end: 1465.015743970871\n",
      "# of batch: 1832\n",
      "train accuracy: 0.9270595\n",
      "One batch end: 1465.9937386512756\n",
      "# of batch: 1833\n",
      "train accuracy: 0.9270883\n",
      "One batch end: 1466.991738319397\n",
      "# of batch: 1834\n",
      "train accuracy: 0.92711717\n",
      "One batch end: 1468.0292630195618\n",
      "# of batch: 1835\n",
      "train accuracy: 0.92713505\n",
      "One batch end: 1469.0362622737885\n",
      "# of batch: 1836\n",
      "train accuracy: 0.92717475\n",
      "One batch end: 1470.0163838863373\n",
      "# of batch: 1837\n",
      "train accuracy: 0.9271817\n",
      "One batch end: 1470.9613831043243\n",
      "# of batch: 1838\n",
      "train accuracy: 0.9272213\n",
      "One batch end: 1471.9043827056885\n",
      "# of batch: 1839\n",
      "train accuracy: 0.92725\n",
      "One batch end: 1472.8483819961548\n",
      "# of batch: 1840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.92727864\n",
      "One batch end: 1473.786898612976\n",
      "# of batch: 1841\n",
      "train accuracy: 0.92731816\n",
      "One batch end: 1474.7068979740143\n",
      "# of batch: 1842\n",
      "train accuracy: 0.92731416\n",
      "One batch end: 1475.6408972740173\n",
      "# of batch: 1843\n",
      "train accuracy: 0.92733186\n",
      "One batch end: 1476.5798966884613\n",
      "# of batch: 1844\n",
      "train accuracy: 0.92734957\n",
      "One batch end: 1477.5138959884644\n",
      "# of batch: 1845\n",
      "train accuracy: 0.9273781\n",
      "One batch end: 1478.4158954620361\n",
      "# of batch: 1846\n",
      "train accuracy: 0.92739576\n",
      "One batch end: 1479.3838946819305\n",
      "# of batch: 1847\n",
      "train accuracy: 0.9274134\n",
      "One batch end: 1480.348417043686\n",
      "# of batch: 1848\n",
      "train accuracy: 0.92744184\n",
      "One batch end: 1481.2819283008575\n",
      "# of batch: 1849\n",
      "train accuracy: 0.92747027\n",
      "One batch end: 1482.1969277858734\n",
      "# of batch: 1850\n",
      "train accuracy: 0.92747706\n",
      "One batch end: 1483.117927312851\n",
      "# of batch: 1851\n",
      "train accuracy: 0.9274838\n",
      "One batch end: 1484.0049264431\n",
      "# of batch: 1852\n",
      "train accuracy: 0.92752296\n",
      "One batch end: 1484.8889257907867\n",
      "# of batch: 1853\n",
      "train accuracy: 0.9275297\n",
      "One batch end: 1485.7739253044128\n",
      "# of batch: 1854\n",
      "train accuracy: 0.92755795\n",
      "One batch end: 1486.682924747467\n",
      "# of batch: 1855\n",
      "train accuracy: 0.9275754\n",
      "One batch end: 1487.564521074295\n",
      "# of batch: 1856\n",
      "train accuracy: 0.92761445\n",
      "One batch end: 1488.452523946762\n",
      "# of batch: 1857\n",
      "train accuracy: 0.92764264\n",
      "One batch end: 1489.3365235328674\n",
      "# of batch: 1858\n",
      "train accuracy: 0.9276708\n",
      "One batch end: 1490.1935245990753\n",
      "# of batch: 1859\n",
      "train accuracy: 0.9276774\n",
      "One batch end: 1491.0570306777954\n",
      "# of batch: 1860\n",
      "train accuracy: 0.9276733\n",
      "One batch end: 1491.955551147461\n",
      "# of batch: 1861\n",
      "train accuracy: 0.9277014\n",
      "One batch end: 1492.867322921753\n",
      "# of batch: 1862\n",
      "train accuracy: 0.9277402\n",
      "One batch end: 1493.803321838379\n",
      "# of batch: 1863\n",
      "train accuracy: 0.9277575\n",
      "One batch end: 1494.7243211269379\n",
      "# of batch: 1864\n",
      "train accuracy: 0.9277855\n",
      "One batch end: 1495.6326611042023\n",
      "# of batch: 1865\n",
      "train accuracy: 0.92781353\n",
      "One batch end: 1496.573166847229\n",
      "# of batch: 1866\n",
      "train accuracy: 0.92782\n",
      "One batch end: 1497.5891661643982\n",
      "# of batch: 1867\n",
      "train accuracy: 0.92783725\n",
      "One batch end: 1498.515170097351\n",
      "# of batch: 1868\n",
      "train accuracy: 0.92784375\n",
      "One batch end: 1499.4461658000946\n",
      "# of batch: 1869\n",
      "train accuracy: 0.9278824\n",
      "One batch end: 1500.365169763565\n",
      "# of batch: 1870\n",
      "train accuracy: 0.9278888\n",
      "One batch end: 1501.4271638393402\n",
      "# of batch: 1871\n",
      "train accuracy: 0.92791665\n",
      "One batch end: 1502.3978071212769\n",
      "# of batch: 1872\n",
      "train accuracy: 0.92792314\n",
      "One batch end: 1503.4318022727966\n",
      "# of batch: 1873\n",
      "train accuracy: 0.9279509\n",
      "One batch end: 1504.4433283805847\n",
      "# of batch: 1874\n",
      "train accuracy: 0.92795736\n",
      "One batch end: 1505.3868656158447\n",
      "# of batch: 1875\n",
      "train accuracy: 0.9279851\n",
      "One batch end: 1506.3278622627258\n",
      "# of batch: 1876\n",
      "train accuracy: 0.9279595\n",
      "One batch end: 1507.413874387741\n",
      "# of batch: 1877\n",
      "train accuracy: 0.92796594\n",
      "One batch end: 1508.4703958034515\n",
      "# of batch: 1878\n",
      "train accuracy: 0.927983\n",
      "One batch end: 1509.4833943843842\n",
      "# of batch: 1879\n",
      "train accuracy: 0.92802125\n",
      "One batch end: 1510.4939053058624\n",
      "# of batch: 1880\n",
      "train accuracy: 0.9280595\n",
      "One batch end: 1511.5414266586304\n",
      "# of batch: 1881\n",
      "train accuracy: 0.9280765\n",
      "One batch end: 1512.4824595451355\n",
      "# of batch: 1882\n",
      "train accuracy: 0.9281147\n",
      "One batch end: 1513.3899800777435\n",
      "# of batch: 1883\n",
      "train accuracy: 0.92815286\n",
      "One batch end: 1514.3219766616821\n",
      "# of batch: 1884\n",
      "train accuracy: 0.92814857\n",
      "One batch end: 1515.2394943237305\n",
      "# of batch: 1885\n",
      "train accuracy: 0.92816544\n",
      "One batch end: 1516.1384935379028\n",
      "# of batch: 1886\n",
      "train accuracy: 0.9281823\n",
      "One batch end: 1517.0714888572693\n",
      "# of batch: 1887\n",
      "train accuracy: 0.9282097\n",
      "One batch end: 1518.113900899887\n",
      "# of batch: 1888\n",
      "train accuracy: 0.928216\n",
      "One batch end: 1519.0539000034332\n",
      "# of batch: 1889\n",
      "train accuracy: 0.9282328\n",
      "One batch end: 1520.0018994808197\n",
      "# of batch: 1890\n",
      "train accuracy: 0.9282602\n",
      "One batch end: 1520.979899406433\n",
      "# of batch: 1891\n",
      "train accuracy: 0.92827696\n",
      "One batch end: 1522.0114233493805\n",
      "# of batch: 1892\n",
      "train accuracy: 0.92828315\n",
      "One batch end: 1523.132934331894\n",
      "# of batch: 1893\n",
      "train accuracy: 0.9282999\n",
      "One batch end: 1524.2674508094788\n",
      "# of batch: 1894\n",
      "train accuracy: 0.9283272\n",
      "One batch end: 1525.3214502334595\n",
      "# of batch: 1895\n",
      "train accuracy: 0.9283439\n",
      "One batch end: 1526.3300213813782\n",
      "# of batch: 1896\n",
      "train accuracy: 0.9283606\n",
      "One batch end: 1527.3567230701447\n",
      "# of batch: 1897\n",
      "train accuracy: 0.9283772\n",
      "One batch end: 1528.3112399578094\n",
      "# of batch: 1898\n",
      "train accuracy: 0.9283939\n",
      "One batch end: 1529.3052423000336\n",
      "# of batch: 1899\n",
      "train accuracy: 0.9284316\n",
      "One batch end: 1530.3337528705597\n",
      "# of batch: 1900\n",
      "train accuracy: 0.92843765\n",
      "One batch end: 1531.4220473766327\n",
      "# of batch: 1901\n",
      "train accuracy: 0.92844373\n",
      "One batch end: 1532.5010523796082\n",
      "# of batch: 1902\n",
      "train accuracy: 0.92848134\n",
      "One batch end: 1533.5320689678192\n",
      "# of batch: 1903\n",
      "train accuracy: 0.9285084\n",
      "One batch end: 1534.3880681991577\n",
      "# of batch: 1904\n",
      "train accuracy: 0.9285249\n",
      "One batch end: 1535.4130728244781\n",
      "# of batch: 1905\n",
      "train accuracy: 0.92853093\n",
      "One batch end: 1536.4673569202423\n",
      "# of batch: 1906\n",
      "train accuracy: 0.92854744\n",
      "One batch end: 1537.536360502243\n",
      "# of batch: 1907\n",
      "train accuracy: 0.92858493\n",
      "One batch end: 1538.4819271564484\n",
      "# of batch: 1908\n",
      "train accuracy: 0.9286118\n",
      "One batch end: 1539.41392660141\n",
      "# of batch: 1909\n",
      "train accuracy: 0.92863876\n",
      "One batch end: 1540.3809263706207\n",
      "# of batch: 1910\n",
      "train accuracy: 0.92865515\n",
      "One batch end: 1541.3009254932404\n",
      "# of batch: 1911\n",
      "train accuracy: 0.928682\n",
      "One batch end: 1542.1975238323212\n",
      "# of batch: 1912\n",
      "train accuracy: 0.9286775\n",
      "One batch end: 1543.112032175064\n",
      "# of batch: 1913\n",
      "train accuracy: 0.9286834\n",
      "One batch end: 1544.0830311775208\n",
      "# of batch: 1914\n",
      "train accuracy: 0.92869973\n",
      "One batch end: 1545.0460350513458\n",
      "# of batch: 1915\n",
      "train accuracy: 0.9286952\n",
      "One batch end: 1546.0870342254639\n",
      "# of batch: 1916\n",
      "train accuracy: 0.9287324\n",
      "One batch end: 1547.1220335960388\n",
      "# of batch: 1917\n",
      "train accuracy: 0.9287591\n",
      "One batch end: 1548.0706408023834\n",
      "# of batch: 1918\n",
      "train accuracy: 0.92879623\n",
      "One batch end: 1549.0206408500671\n",
      "# of batch: 1919\n",
      "train accuracy: 0.9288125\n",
      "One batch end: 1549.980639219284\n",
      "# of batch: 1920\n",
      "train accuracy: 0.9288079\n",
      "One batch end: 1550.9080770015717\n",
      "# of batch: 1921\n",
      "train accuracy: 0.92883456\n",
      "One batch end: 1551.8295984268188\n",
      "# of batch: 1922\n",
      "train accuracy: 0.9288716\n",
      "One batch end: 1552.77459359169\n",
      "# of batch: 1923\n",
      "train accuracy: 0.92889816\n",
      "One batch end: 1553.7301235198975\n",
      "# of batch: 1924\n",
      "train accuracy: 0.92893505\n",
      "One batch end: 1554.683117866516\n",
      "# of batch: 1925\n",
      "train accuracy: 0.92897195\n",
      "One batch end: 1555.6055705547333\n",
      "# of batch: 1926\n",
      "train accuracy: 0.92898804\n",
      "One batch end: 1556.5355699062347\n",
      "# of batch: 1927\n",
      "train accuracy: 0.9290145\n",
      "One batch end: 1557.4892077445984\n",
      "# of batch: 1928\n",
      "train accuracy: 0.92905134\n",
      "One batch end: 1558.418728351593\n",
      "# of batch: 1929\n",
      "train accuracy: 0.92904663\n",
      "One batch end: 1559.3187327384949\n",
      "# of batch: 1930\n",
      "train accuracy: 0.9290419\n",
      "One batch end: 1560.2250201702118\n",
      "# of batch: 1931\n",
      "train accuracy: 0.92905796\n",
      "One batch end: 1561.1486191749573\n",
      "# of batch: 1932\n",
      "train accuracy: 0.9290843\n",
      "One batch end: 1562.0831379890442\n",
      "# of batch: 1933\n",
      "train accuracy: 0.929121\n",
      "One batch end: 1563.0381844043732\n",
      "# of batch: 1934\n",
      "train accuracy: 0.9291266\n",
      "One batch end: 1564.022183895111\n",
      "# of batch: 1935\n",
      "train accuracy: 0.9291322\n",
      "One batch end: 1565.0111832618713\n",
      "# of batch: 1936\n",
      "train accuracy: 0.92914814\n",
      "One batch end: 1565.987182855606\n",
      "# of batch: 1937\n",
      "train accuracy: 0.9291744\n",
      "One batch end: 1566.9737029075623\n",
      "# of batch: 1938\n",
      "train accuracy: 0.92917997\n",
      "One batch end: 1567.9997761249542\n",
      "# of batch: 1939\n",
      "train accuracy: 0.9292062\n",
      "One batch end: 1569.0237753391266\n",
      "# of batch: 1940\n",
      "train accuracy: 0.92921174\n",
      "One batch end: 1570.0387749671936\n",
      "# of batch: 1941\n",
      "train accuracy: 0.9292379\n",
      "One batch end: 1571.0273575782776\n",
      "# of batch: 1942\n",
      "train accuracy: 0.92923313\n",
      "One batch end: 1571.9759590625763\n",
      "# of batch: 1943\n",
      "train accuracy: 0.9292387\n",
      "One batch end: 1572.9409584999084\n",
      "# of batch: 1944\n",
      "train accuracy: 0.9292648\n",
      "One batch end: 1573.8769578933716\n",
      "# of batch: 1945\n",
      "train accuracy: 0.92929083\n",
      "One batch end: 1574.820957183838\n",
      "# of batch: 1946\n",
      "train accuracy: 0.9293066\n",
      "One batch end: 1575.778472185135\n",
      "# of batch: 1947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.92932236\n",
      "One batch end: 1576.7374675273895\n",
      "# of batch: 1948\n",
      "train accuracy: 0.92935866\n",
      "One batch end: 1577.688717365265\n",
      "# of batch: 1949\n",
      "train accuracy: 0.9293846\n",
      "One batch end: 1578.618236064911\n",
      "# of batch: 1950\n",
      "train accuracy: 0.9294106\n",
      "One batch end: 1579.5372359752655\n",
      "# of batch: 1951\n",
      "train accuracy: 0.9294365\n",
      "One batch end: 1580.4687569141388\n",
      "# of batch: 1952\n",
      "train accuracy: 0.9294521\n",
      "One batch end: 1581.3997569084167\n",
      "# of batch: 1953\n",
      "train accuracy: 0.92946774\n",
      "One batch end: 1582.3387529850006\n",
      "# of batch: 1954\n",
      "train accuracy: 0.9294629\n",
      "One batch end: 1583.258862733841\n",
      "# of batch: 1955\n",
      "train accuracy: 0.9294888\n",
      "One batch end: 1584.18186211586\n",
      "# of batch: 1956\n",
      "train accuracy: 0.9295146\n",
      "One batch end: 1585.087861776352\n",
      "# of batch: 1957\n",
      "train accuracy: 0.92953014\n",
      "One batch end: 1585.9708609580994\n",
      "# of batch: 1958\n",
      "train accuracy: 0.9295559\n",
      "One batch end: 1586.8808591365814\n",
      "# of batch: 1959\n",
      "train accuracy: 0.92958164\n",
      "One batch end: 1587.7988584041595\n",
      "# of batch: 1960\n",
      "train accuracy: 0.92959714\n",
      "One batch end: 1588.7078745365143\n",
      "# of batch: 1961\n",
      "train accuracy: 0.92961264\n",
      "One batch end: 1589.6138739585876\n",
      "# of batch: 1962\n",
      "train accuracy: 0.92960775\n",
      "One batch end: 1590.5078690052032\n",
      "# of batch: 1963\n",
      "train accuracy: 0.92962325\n",
      "One batch end: 1591.402872800827\n",
      "# of batch: 1964\n",
      "train accuracy: 0.929659\n",
      "One batch end: 1592.33287191391\n",
      "# of batch: 1965\n",
      "train accuracy: 0.92968464\n",
      "One batch end: 1593.2423655986786\n",
      "# of batch: 1966\n",
      "train accuracy: 0.9297102\n",
      "One batch end: 1594.1543791294098\n",
      "# of batch: 1967\n",
      "train accuracy: 0.92970526\n",
      "One batch end: 1595.025378704071\n",
      "# of batch: 1968\n",
      "train accuracy: 0.9297207\n",
      "One batch end: 1595.9183781147003\n",
      "# of batch: 1969\n",
      "train accuracy: 0.92975634\n",
      "One batch end: 1596.8403732776642\n",
      "# of batch: 1970\n",
      "train accuracy: 0.9297615\n",
      "One batch end: 1597.729377746582\n",
      "# of batch: 1971\n",
      "train accuracy: 0.9297667\n",
      "One batch end: 1598.6718916893005\n",
      "# of batch: 1972\n",
      "train accuracy: 0.92979217\n",
      "One batch end: 1599.5776550769806\n",
      "# of batch: 1973\n",
      "train accuracy: 0.9298176\n",
      "One batch end: 1600.4936542510986\n",
      "# of batch: 1974\n",
      "train accuracy: 0.929843\n",
      "One batch end: 1601.427175283432\n",
      "# of batch: 1975\n",
      "train accuracy: 0.92987853\n",
      "One batch end: 1602.3341748714447\n",
      "# of batch: 1976\n",
      "train accuracy: 0.9299039\n",
      "One batch end: 1603.25527882576\n",
      "# of batch: 1977\n",
      "train accuracy: 0.9299292\n",
      "One batch end: 1604.177803993225\n",
      "# of batch: 1978\n",
      "train accuracy: 0.9299545\n",
      "One batch end: 1605.0898036956787\n",
      "# of batch: 1979\n",
      "train accuracy: 0.92996967\n",
      "One batch end: 1606.0063247680664\n",
      "# of batch: 1980\n",
      "train accuracy: 0.92997473\n",
      "One batch end: 1606.9183232784271\n",
      "# of batch: 1981\n",
      "train accuracy: 0.93\n",
      "One batch end: 1607.8303229808807\n",
      "# of batch: 1982\n",
      "train accuracy: 0.93001515\n",
      "One batch end: 1608.7369170188904\n",
      "# of batch: 1983\n",
      "train accuracy: 0.9300403\n",
      "One batch end: 1609.6209163665771\n",
      "# of batch: 1984\n",
      "train accuracy: 0.93005544\n",
      "One batch end: 1610.5099122524261\n",
      "# of batch: 1985\n",
      "train accuracy: 0.9300705\n",
      "One batch end: 1611.416915178299\n",
      "# of batch: 1986\n",
      "train accuracy: 0.9300956\n",
      "One batch end: 1612.3584365844727\n",
      "# of batch: 1987\n",
      "train accuracy: 0.9301006\n",
      "One batch end: 1613.2729568481445\n",
      "# of batch: 1988\n",
      "train accuracy: 0.9301257\n",
      "One batch end: 1614.2099964618683\n",
      "# of batch: 1989\n",
      "train accuracy: 0.9301106\n",
      "One batch end: 1615.1079955101013\n",
      "# of batch: 1990\n",
      "train accuracy: 0.9301256\n",
      "One batch end: 1615.9910297393799\n",
      "# of batch: 1991\n",
      "train accuracy: 0.93013054\n",
      "One batch end: 1616.9310290813446\n",
      "# of batch: 1992\n",
      "train accuracy: 0.9301455\n",
      "One batch end: 1617.8420283794403\n",
      "# of batch: 1993\n",
      "train accuracy: 0.93016046\n",
      "One batch end: 1618.7730281352997\n",
      "# of batch: 1994\n",
      "train accuracy: 0.93018544\n",
      "One batch end: 1619.6946558952332\n",
      "# of batch: 1995\n",
      "train accuracy: 0.9301804\n",
      "One batch end: 1620.615663766861\n",
      "# of batch: 1996\n",
      "train accuracy: 0.9302053\n",
      "One batch end: 1621.5311748981476\n",
      "# of batch: 1997\n",
      "train accuracy: 0.93022025\n",
      "One batch end: 1622.4551787376404\n",
      "# of batch: 1998\n",
      "train accuracy: 0.9302451\n",
      "One batch end: 1623.3811774253845\n",
      "# of batch: 1999\n",
      "train accuracy: 0.93027\n",
      "One batch end: 1624.2956998348236\n",
      "# of batch: 2000\n",
      "train accuracy: 0.9303048\n",
      "One batch end: 1625.2036955356598\n",
      "# of batch: 2001\n",
      "train accuracy: 0.93031967\n",
      "One batch end: 1626.1002099514008\n",
      "# of batch: 2002\n",
      "train accuracy: 0.93034446\n",
      "One batch end: 1627.0192956924438\n",
      "# of batch: 2003\n",
      "train accuracy: 0.93036926\n",
      "One batch end: 1627.9412956237793\n",
      "# of batch: 2004\n",
      "train accuracy: 0.930394\n",
      "One batch end: 1628.8353509902954\n",
      "# of batch: 2005\n",
      "train accuracy: 0.9304088\n",
      "One batch end: 1629.756350517273\n",
      "# of batch: 2006\n",
      "train accuracy: 0.9304335\n",
      "One batch end: 1630.7073497772217\n",
      "# of batch: 2007\n",
      "train accuracy: 0.9304382\n",
      "One batch end: 1631.650865316391\n",
      "# of batch: 2008\n",
      "train accuracy: 0.93045294\n",
      "One batch end: 1632.5538654327393\n",
      "# of batch: 2009\n",
      "train accuracy: 0.9304876\n",
      "One batch end: 1633.4668686389923\n",
      "# of batch: 2010\n",
      "train accuracy: 0.9304823\n",
      "One batch end: 1634.3599824905396\n",
      "# of batch: 2011\n",
      "train accuracy: 0.9305169\n",
      "One batch end: 1635.255981683731\n",
      "# of batch: 2012\n",
      "train accuracy: 0.93053156\n",
      "One batch end: 1636.1885025501251\n",
      "# of batch: 2013\n",
      "train accuracy: 0.930566\n",
      "One batch end: 1637.0895018577576\n",
      "# of batch: 2014\n",
      "train accuracy: 0.9306005\n",
      "One batch end: 1638.0415012836456\n",
      "# of batch: 2015\n",
      "train accuracy: 0.9306052\n",
      "One batch end: 1638.9950811862946\n",
      "# of batch: 2016\n",
      "train accuracy: 0.9306297\n",
      "One batch end: 1639.9740843772888\n",
      "# of batch: 2017\n",
      "train accuracy: 0.9306541\n",
      "One batch end: 1640.9816088676453\n",
      "# of batch: 2018\n",
      "train accuracy: 0.93067855\n",
      "One batch end: 1642.0006132125854\n",
      "# of batch: 2019\n",
      "train accuracy: 0.930703\n",
      "One batch end: 1642.9716169834137\n",
      "# of batch: 2020\n",
      "train accuracy: 0.9307076\n",
      "One batch end: 1643.9026165008545\n",
      "# of batch: 2021\n",
      "train accuracy: 0.93074185\n",
      "One batch end: 1644.8016157150269\n",
      "# of batch: 2022\n",
      "train accuracy: 0.93074644\n",
      "One batch end: 1645.6966121196747\n",
      "# of batch: 2023\n",
      "train accuracy: 0.93077075\n",
      "One batch end: 1646.605614900589\n",
      "# of batch: 2024\n",
      "train accuracy: 0.9307951\n",
      "One batch end: 1647.503137588501\n",
      "# of batch: 2025\n",
      "train accuracy: 0.9308292\n",
      "One batch end: 1648.4511380195618\n",
      "# of batch: 2026\n",
      "train accuracy: 0.9308436\n",
      "One batch end: 1649.3736741542816\n",
      "# of batch: 2027\n",
      "train accuracy: 0.9308777\n",
      "One batch end: 1650.3086733818054\n",
      "# of batch: 2028\n",
      "train accuracy: 0.93089205\n",
      "One batch end: 1651.239194393158\n",
      "# of batch: 2029\n",
      "train accuracy: 0.93091625\n",
      "One batch end: 1652.1791896820068\n",
      "# of batch: 2030\n",
      "train accuracy: 0.93093055\n",
      "One batch end: 1653.1001930236816\n",
      "# of batch: 2031\n",
      "train accuracy: 0.93094486\n",
      "One batch end: 1654.0188784599304\n",
      "# of batch: 2032\n",
      "train accuracy: 0.93097883\n",
      "One batch end: 1654.9058775901794\n",
      "# of batch: 2033\n",
      "train accuracy: 0.931003\n",
      "One batch end: 1655.8008790016174\n",
      "# of batch: 2034\n",
      "train accuracy: 0.93102705\n",
      "One batch end: 1656.6913957595825\n",
      "# of batch: 2035\n",
      "train accuracy: 0.93104124\n",
      "One batch end: 1657.5743911266327\n",
      "# of batch: 2036\n",
      "train accuracy: 0.9310358\n",
      "One batch end: 1658.4775130748749\n",
      "# of batch: 2037\n",
      "train accuracy: 0.9310697\n",
      "One batch end: 1659.4060018062592\n",
      "# of batch: 2038\n",
      "train accuracy: 0.9310937\n",
      "One batch end: 1660.3239996433258\n",
      "# of batch: 2039\n",
      "train accuracy: 0.9311078\n",
      "One batch end: 1661.2423162460327\n",
      "# of batch: 2040\n",
      "train accuracy: 0.9311318\n",
      "One batch end: 1662.1683197021484\n",
      "# of batch: 2041\n",
      "train accuracy: 0.93115574\n",
      "One batch end: 1663.1083192825317\n",
      "# of batch: 2042\n",
      "train accuracy: 0.93116003\n",
      "One batch end: 1664.0368571281433\n",
      "# of batch: 2043\n",
      "train accuracy: 0.93118393\n",
      "One batch end: 1664.9278569221497\n",
      "# of batch: 2044\n",
      "train accuracy: 0.93119806\n",
      "One batch end: 1665.8418562412262\n",
      "# of batch: 2045\n",
      "train accuracy: 0.9312317\n",
      "One batch end: 1666.733372926712\n",
      "# of batch: 2046\n",
      "train accuracy: 0.9312653\n",
      "One batch end: 1667.6593725681305\n",
      "# of batch: 2047\n",
      "train accuracy: 0.9312793\n",
      "One batch end: 1668.5603783130646\n",
      "# of batch: 2048\n",
      "train accuracy: 0.93131286\n",
      "One batch end: 1669.4544610977173\n",
      "# of batch: 2049\n",
      "train accuracy: 0.93134636\n",
      "One batch end: 1670.3394613265991\n",
      "# of batch: 2050\n",
      "train accuracy: 0.9313701\n",
      "One batch end: 1671.2294640541077\n",
      "# of batch: 2051\n",
      "train accuracy: 0.93136454\n",
      "One batch end: 1672.194459438324\n",
      "# of batch: 2052\n",
      "train accuracy: 0.9313785\n",
      "One batch end: 1673.0800607204437\n",
      "# of batch: 2053\n",
      "train accuracy: 0.93140215\n",
      "One batch end: 1673.974583864212\n",
      "# of batch: 2054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.93141603\n",
      "One batch end: 1674.8665833473206\n",
      "# of batch: 2055\n",
      "train accuracy: 0.93143\n",
      "One batch end: 1675.7931027412415\n",
      "# of batch: 2056\n",
      "train accuracy: 0.93144387\n",
      "One batch end: 1676.6801056861877\n",
      "# of batch: 2057\n",
      "train accuracy: 0.9314675\n",
      "One batch end: 1677.6201045513153\n",
      "# of batch: 2058\n",
      "train accuracy: 0.9314813\n",
      "One batch end: 1678.4951045513153\n",
      "# of batch: 2059\n",
      "train accuracy: 0.93150485\n",
      "One batch end: 1679.4131035804749\n",
      "# of batch: 2060\n",
      "train accuracy: 0.9315187\n",
      "One batch end: 1680.3340997695923\n",
      "# of batch: 2061\n",
      "train accuracy: 0.9315422\n",
      "One batch end: 1681.2551023960114\n",
      "# of batch: 2062\n",
      "train accuracy: 0.9315657\n",
      "One batch end: 1682.1636154651642\n",
      "# of batch: 2063\n",
      "train accuracy: 0.9315795\n",
      "One batch end: 1683.0796165466309\n",
      "# of batch: 2064\n",
      "train accuracy: 0.9316126\n",
      "One batch end: 1683.974615097046\n",
      "# of batch: 2065\n",
      "train accuracy: 0.93161666\n",
      "One batch end: 1684.8792080879211\n",
      "# of batch: 2066\n",
      "train accuracy: 0.9316304\n",
      "One batch end: 1685.774207353592\n",
      "# of batch: 2067\n",
      "train accuracy: 0.9316344\n",
      "One batch end: 1686.6903166770935\n",
      "# of batch: 2068\n",
      "train accuracy: 0.9316385\n",
      "One batch end: 1687.5953161716461\n",
      "# of batch: 2069\n",
      "train accuracy: 0.93166184\n",
      "One batch end: 1688.4893155097961\n",
      "# of batch: 2070\n",
      "train accuracy: 0.9316755\n",
      "One batch end: 1689.3858375549316\n",
      "# of batch: 2071\n",
      "train accuracy: 0.9316506\n",
      "One batch end: 1690.2863883972168\n",
      "# of batch: 2072\n",
      "train accuracy: 0.9316739\n",
      "One batch end: 1691.1953880786896\n",
      "# of batch: 2073\n",
      "train accuracy: 0.93168753\n",
      "One batch end: 1692.0853877067566\n",
      "# of batch: 2074\n",
      "train accuracy: 0.93171084\n",
      "One batch end: 1692.9483833312988\n",
      "# of batch: 2075\n",
      "train accuracy: 0.9317341\n",
      "One batch end: 1693.8129098415375\n",
      "# of batch: 2076\n",
      "train accuracy: 0.931767\n",
      "One batch end: 1694.7010486125946\n",
      "# of batch: 2077\n",
      "train accuracy: 0.9317806\n",
      "One batch end: 1695.5670447349548\n",
      "# of batch: 2078\n",
      "train accuracy: 0.9317941\n",
      "One batch end: 1696.4760437011719\n",
      "# of batch: 2079\n",
      "train accuracy: 0.9318173\n",
      "One batch end: 1697.4265682697296\n",
      "# of batch: 2080\n",
      "train accuracy: 0.9318405\n",
      "One batch end: 1698.3805663585663\n",
      "# of batch: 2081\n",
      "train accuracy: 0.931854\n",
      "One batch end: 1699.2885677814484\n",
      "# of batch: 2082\n",
      "train accuracy: 0.9318675\n",
      "One batch end: 1700.1946127414703\n",
      "# of batch: 2083\n",
      "train accuracy: 0.9318906\n",
      "One batch end: 1701.1326117515564\n",
      "# of batch: 2084\n",
      "train accuracy: 0.9319233\n",
      "One batch end: 1702.0678970813751\n",
      "# of batch: 2085\n",
      "train accuracy: 0.93190795\n",
      "One batch end: 1702.9858977794647\n",
      "# of batch: 2086\n",
      "train accuracy: 0.93194056\n",
      "One batch end: 1703.9028968811035\n",
      "# of batch: 2087\n",
      "train accuracy: 0.9319636\n",
      "One batch end: 1704.8229985237122\n",
      "# of batch: 2088\n",
      "train accuracy: 0.93197703\n",
      "One batch end: 1705.7629981040955\n",
      "# of batch: 2089\n",
      "train accuracy: 0.932\n",
      "One batch end: 1706.6859998703003\n",
      "# of batch: 2090\n",
      "train accuracy: 0.9320229\n",
      "One batch end: 1707.6135187149048\n",
      "# of batch: 2091\n",
      "train accuracy: 0.9320555\n",
      "One batch end: 1708.5302262306213\n",
      "# of batch: 2092\n",
      "train accuracy: 0.9320688\n",
      "One batch end: 1709.452229499817\n",
      "# of batch: 2093\n",
      "train accuracy: 0.9320917\n",
      "One batch end: 1710.3872294425964\n",
      "# of batch: 2094\n",
      "train accuracy: 0.9321241\n",
      "One batch end: 1711.324224948883\n",
      "# of batch: 2095\n",
      "train accuracy: 0.9321374\n",
      "One batch end: 1712.2088425159454\n",
      "# of batch: 2096\n",
      "train accuracy: 0.93216026\n",
      "One batch end: 1713.0988383293152\n",
      "# of batch: 2097\n",
      "train accuracy: 0.932183\n",
      "One batch end: 1714.0078365802765\n",
      "# of batch: 2098\n",
      "train accuracy: 0.9322058\n",
      "One batch end: 1714.9161179065704\n",
      "# of batch: 2099\n",
      "train accuracy: 0.9322381\n",
      "One batch end: 1715.863113641739\n",
      "# of batch: 2100\n",
      "train accuracy: 0.9322608\n",
      "One batch end: 1716.747120141983\n",
      "# of batch: 2101\n",
      "train accuracy: 0.93229306\n",
      "One batch end: 1717.6336340904236\n",
      "# of batch: 2102\n",
      "train accuracy: 0.93232524\n",
      "One batch end: 1718.5352346897125\n",
      "# of batch: 2103\n",
      "train accuracy: 0.9323479\n",
      "One batch end: 1719.4332344532013\n",
      "# of batch: 2104\n",
      "train accuracy: 0.93237054\n",
      "One batch end: 1720.389229774475\n",
      "# of batch: 2105\n",
      "train accuracy: 0.93239313\n",
      "One batch end: 1721.2862334251404\n",
      "# of batch: 2106\n",
      "train accuracy: 0.9323873\n",
      "One batch end: 1722.1902332305908\n",
      "# of batch: 2107\n",
      "train accuracy: 0.9324099\n",
      "One batch end: 1723.1122319698334\n",
      "# of batch: 2108\n",
      "train accuracy: 0.93242294\n",
      "One batch end: 1724.0162315368652\n",
      "# of batch: 2109\n",
      "train accuracy: 0.9324455\n",
      "One batch end: 1724.9363396167755\n",
      "# of batch: 2110\n",
      "train accuracy: 0.9324586\n",
      "One batch end: 1725.7983393669128\n",
      "# of batch: 2111\n",
      "train accuracy: 0.9324621\n",
      "One batch end: 1726.7003383636475\n",
      "# of batch: 2112\n",
      "train accuracy: 0.93247515\n",
      "One batch end: 1727.6333377361298\n",
      "# of batch: 2113\n",
      "train accuracy: 0.9324598\n",
      "One batch end: 1728.5448591709137\n",
      "# of batch: 2114\n",
      "train accuracy: 0.9324917\n",
      "One batch end: 1729.4418585300446\n",
      "# of batch: 2115\n",
      "train accuracy: 0.9325047\n",
      "One batch end: 1730.3264133930206\n",
      "# of batch: 2116\n",
      "train accuracy: 0.9325366\n",
      "One batch end: 1731.2204158306122\n",
      "# of batch: 2117\n",
      "train accuracy: 0.932559\n",
      "One batch end: 1732.147936820984\n",
      "# of batch: 2118\n",
      "train accuracy: 0.93257195\n",
      "One batch end: 1733.0479364395142\n",
      "# of batch: 2119\n",
      "train accuracy: 0.93259436\n",
      "One batch end: 1733.9919352531433\n",
      "# of batch: 2120\n",
      "train accuracy: 0.9326167\n",
      "One batch end: 1734.8909285068512\n",
      "# of batch: 2121\n",
      "train accuracy: 0.93261075\n",
      "One batch end: 1735.768926858902\n",
      "# of batch: 2122\n",
      "train accuracy: 0.93263304\n",
      "One batch end: 1736.671926498413\n",
      "# of batch: 2123\n",
      "train accuracy: 0.9326648\n",
      "One batch end: 1737.5424375534058\n",
      "# of batch: 2124\n",
      "train accuracy: 0.9326682\n",
      "One batch end: 1738.4060423374176\n",
      "# of batch: 2125\n",
      "train accuracy: 0.9326811\n",
      "One batch end: 1739.279040813446\n",
      "# of batch: 2126\n",
      "train accuracy: 0.93271273\n",
      "One batch end: 1740.2335481643677\n",
      "# of batch: 2127\n",
      "train accuracy: 0.9327162\n",
      "One batch end: 1741.1545498371124\n",
      "# of batch: 2128\n",
      "train accuracy: 0.9327478\n",
      "One batch end: 1742.0505471229553\n",
      "# of batch: 2129\n",
      "train accuracy: 0.9327512\n",
      "One batch end: 1742.9495465755463\n",
      "# of batch: 2130\n",
      "train accuracy: 0.9327546\n",
      "One batch end: 1743.8630557060242\n",
      "# of batch: 2131\n",
      "train accuracy: 0.93276733\n",
      "One batch end: 1744.791080713272\n",
      "# of batch: 2132\n",
      "train accuracy: 0.9327707\n",
      "One batch end: 1745.6860799789429\n",
      "# of batch: 2133\n",
      "train accuracy: 0.9327929\n",
      "One batch end: 1746.5626006126404\n",
      "# of batch: 2134\n",
      "train accuracy: 0.932815\n",
      "One batch end: 1747.4416000843048\n",
      "# of batch: 2135\n",
      "train accuracy: 0.93283707\n",
      "One batch end: 1748.3181147575378\n",
      "# of batch: 2136\n",
      "train accuracy: 0.9328591\n",
      "One batch end: 1749.219114780426\n",
      "# of batch: 2137\n",
      "train accuracy: 0.9328718\n",
      "One batch end: 1750.1145627498627\n",
      "# of batch: 2138\n",
      "train accuracy: 0.9328939\n",
      "One batch end: 1751.004566192627\n",
      "# of batch: 2139\n",
      "train accuracy: 0.93290657\n",
      "One batch end: 1751.9125616550446\n",
      "# of batch: 2140\n",
      "train accuracy: 0.93293786\n",
      "One batch end: 1752.8100883960724\n",
      "# of batch: 2141\n",
      "train accuracy: 0.9329692\n",
      "One batch end: 1753.7150859832764\n",
      "# of batch: 2142\n",
      "train accuracy: 0.93300045\n",
      "One batch end: 1754.6250822544098\n",
      "# of batch: 2143\n",
      "train accuracy: 0.9330224\n",
      "One batch end: 1755.5035691261292\n",
      "# of batch: 2144\n",
      "train accuracy: 0.9330536\n",
      "One batch end: 1756.388572692871\n",
      "# of batch: 2145\n",
      "train accuracy: 0.9330848\n",
      "One batch end: 1757.2769470214844\n",
      "# of batch: 2146\n",
      "train accuracy: 0.93309736\n",
      "One batch end: 1758.1989452838898\n",
      "# of batch: 2147\n",
      "train accuracy: 0.9331285\n",
      "One batch end: 1759.0729398727417\n",
      "# of batch: 2148\n",
      "train accuracy: 0.9331596\n",
      "One batch end: 1759.9559435844421\n",
      "# of batch: 2149\n",
      "train accuracy: 0.9331907\n",
      "One batch end: 1760.8138959407806\n",
      "# of batch: 2150\n",
      "train accuracy: 0.93320316\n",
      "One batch end: 1761.7262094020844\n",
      "# of batch: 2151\n",
      "train accuracy: 0.9332249\n",
      "One batch end: 1762.6642088890076\n",
      "# of batch: 2152\n",
      "train accuracy: 0.9332466\n",
      "One batch end: 1763.605208158493\n",
      "# of batch: 2153\n",
      "train accuracy: 0.9332776\n",
      "One batch end: 1764.5772132873535\n",
      "# of batch: 2154\n",
      "train accuracy: 0.93328077\n",
      "One batch end: 1765.6112072467804\n",
      "# of batch: 2155\n",
      "train accuracy: 0.9333024\n",
      "One batch end: 1766.6542065143585\n",
      "# of batch: 2156\n",
      "train accuracy: 0.9333148\n",
      "One batch end: 1767.6422057151794\n",
      "# of batch: 2157\n",
      "train accuracy: 0.9333457\n",
      "One batch end: 1768.5872008800507\n",
      "# of batch: 2158\n",
      "train accuracy: 0.9333673\n",
      "One batch end: 1769.5017261505127\n",
      "# of batch: 2159\n",
      "train accuracy: 0.9333889\n",
      "One batch end: 1770.4118020534515\n",
      "# of batch: 2160\n",
      "train accuracy: 0.93337345\n",
      "One batch end: 1771.3357989788055\n",
      "# of batch: 2161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.93340427\n",
      "One batch end: 1772.2148010730743\n",
      "# of batch: 2162\n",
      "train accuracy: 0.93341655\n",
      "One batch end: 1773.160322189331\n",
      "# of batch: 2163\n",
      "train accuracy: 0.9334196\n",
      "One batch end: 1774.1343214511871\n",
      "# of batch: 2164\n",
      "train accuracy: 0.9334226\n",
      "One batch end: 1775.0809836387634\n",
      "# of batch: 2165\n",
      "train accuracy: 0.93344414\n",
      "One batch end: 1776.0009138584137\n",
      "# of batch: 2166\n",
      "train accuracy: 0.9334656\n",
      "One batch end: 1776.9269132614136\n",
      "# of batch: 2167\n",
      "train accuracy: 0.9334779\n",
      "One batch end: 1777.8574242591858\n",
      "# of batch: 2168\n",
      "train accuracy: 0.93346244\n",
      "One batch end: 1778.9580302238464\n",
      "# of batch: 2169\n",
      "train accuracy: 0.9334654\n",
      "One batch end: 1780.047027349472\n",
      "# of batch: 2170\n",
      "train accuracy: 0.93349606\n",
      "One batch end: 1781.1470596790314\n",
      "# of batch: 2171\n",
      "train accuracy: 0.9335175\n",
      "One batch end: 1782.1540586948395\n",
      "# of batch: 2172\n",
      "train accuracy: 0.9335389\n",
      "One batch end: 1783.179579257965\n",
      "# of batch: 2173\n",
      "train accuracy: 0.93356943\n",
      "One batch end: 1784.2215900421143\n",
      "# of batch: 2174\n",
      "train accuracy: 0.9335908\n",
      "One batch end: 1785.193112373352\n",
      "# of batch: 2175\n",
      "train accuracy: 0.9336121\n",
      "One batch end: 1786.2066278457642\n",
      "# of batch: 2176\n",
      "train accuracy: 0.93363345\n",
      "One batch end: 1787.164137840271\n",
      "# of batch: 2177\n",
      "train accuracy: 0.93363637\n",
      "One batch end: 1788.17613363266\n",
      "# of batch: 2178\n",
      "train accuracy: 0.93364847\n",
      "One batch end: 1789.118134021759\n",
      "# of batch: 2179\n",
      "train accuracy: 0.93366057\n",
      "One batch end: 1790.0555970668793\n",
      "# of batch: 2180\n",
      "train accuracy: 0.9336634\n",
      "One batch end: 1790.9841973781586\n",
      "# of batch: 2181\n",
      "train accuracy: 0.9336939\n",
      "One batch end: 1791.9961965084076\n",
      "# of batch: 2182\n",
      "train accuracy: 0.9337059\n",
      "One batch end: 1793.037192583084\n",
      "# of batch: 2183\n",
      "train accuracy: 0.9337271\n",
      "One batch end: 1794.0671911239624\n",
      "# of batch: 2184\n",
      "train accuracy: 0.9337574\n",
      "One batch end: 1795.0477120876312\n",
      "# of batch: 2185\n",
      "train accuracy: 0.93376946\n",
      "One batch end: 1795.9777064323425\n",
      "# of batch: 2186\n",
      "train accuracy: 0.93378145\n",
      "One batch end: 1797.0332453250885\n",
      "# of batch: 2187\n",
      "train accuracy: 0.93380255\n",
      "One batch end: 1798.106754541397\n",
      "# of batch: 2188\n",
      "train accuracy: 0.93382365\n",
      "One batch end: 1799.1977579593658\n",
      "# of batch: 2189\n",
      "train accuracy: 0.9338356\n",
      "One batch end: 1800.3247547149658\n",
      "# of batch: 2190\n",
      "train accuracy: 0.93384755\n",
      "One batch end: 1801.2722735404968\n",
      "# of batch: 2191\n",
      "train accuracy: 0.9338321\n",
      "One batch end: 1802.1962728500366\n",
      "# of batch: 2192\n",
      "train accuracy: 0.93385315\n",
      "One batch end: 1803.1622772216797\n",
      "# of batch: 2193\n",
      "train accuracy: 0.9338651\n",
      "One batch end: 1804.1392757892609\n",
      "# of batch: 2194\n",
      "train accuracy: 0.9338861\n",
      "One batch end: 1805.0762720108032\n",
      "# of batch: 2195\n",
      "train accuracy: 0.9339071\n",
      "One batch end: 1806.0346252918243\n",
      "# of batch: 2196\n",
      "train accuracy: 0.9339008\n",
      "One batch end: 1806.999624490738\n",
      "# of batch: 2197\n",
      "train accuracy: 0.9339309\n",
      "One batch end: 1808.0051455497742\n",
      "# of batch: 2198\n",
      "train accuracy: 0.9339609\n",
      "One batch end: 1809.0532674789429\n",
      "# of batch: 2199\n",
      "train accuracy: 0.9339727\n",
      "One batch end: 1810.0692670345306\n",
      "# of batch: 2200\n",
      "train accuracy: 0.9339846\n",
      "One batch end: 1811.0893120765686\n",
      "# of batch: 2201\n",
      "train accuracy: 0.93400544\n",
      "One batch end: 1812.0553088188171\n",
      "# of batch: 2202\n",
      "train accuracy: 0.9340354\n",
      "One batch end: 1813.0013115406036\n",
      "# of batch: 2203\n",
      "train accuracy: 0.9340563\n",
      "One batch end: 1813.9458270072937\n",
      "# of batch: 2204\n",
      "train accuracy: 0.93404084\n",
      "One batch end: 1814.863713979721\n",
      "# of batch: 2205\n",
      "train accuracy: 0.9340435\n",
      "One batch end: 1815.8112354278564\n",
      "# of batch: 2206\n",
      "train accuracy: 0.93405527\n",
      "One batch end: 1816.7202348709106\n",
      "# of batch: 2207\n",
      "train accuracy: 0.93408513\n",
      "One batch end: 1817.6182343959808\n",
      "# of batch: 2208\n",
      "train accuracy: 0.934115\n",
      "One batch end: 1818.5253257751465\n",
      "# of batch: 2209\n",
      "train accuracy: 0.93413574\n",
      "One batch end: 1819.459324836731\n",
      "# of batch: 2210\n",
      "train accuracy: 0.9341565\n",
      "One batch end: 1820.369845867157\n",
      "# of batch: 2211\n",
      "train accuracy: 0.9341863\n",
      "One batch end: 1821.277366399765\n",
      "# of batch: 2212\n",
      "train accuracy: 0.93420696\n",
      "One batch end: 1822.1653656959534\n",
      "# of batch: 2213\n",
      "train accuracy: 0.93422765\n",
      "One batch end: 1823.0828363895416\n",
      "# of batch: 2214\n",
      "train accuracy: 0.9342573\n",
      "One batch end: 1824.0138366222382\n",
      "# of batch: 2215\n",
      "train accuracy: 0.934287\n",
      "One batch end: 1824.9143583774567\n",
      "# of batch: 2216\n",
      "train accuracy: 0.93431664\n",
      "One batch end: 1825.8549556732178\n",
      "# of batch: 2217\n",
      "train accuracy: 0.93434626\n",
      "One batch end: 1826.8849589824677\n",
      "# of batch: 2218\n",
      "train accuracy: 0.9343578\n",
      "One batch end: 1827.8999590873718\n",
      "# of batch: 2219\n",
      "train accuracy: 0.9343784\n",
      "One batch end: 1828.9054601192474\n",
      "# of batch: 2220\n",
      "train accuracy: 0.93440795\n",
      "One batch end: 1829.8255302906036\n",
      "# of batch: 2221\n",
      "train accuracy: 0.93442845\n",
      "One batch end: 1830.8320536613464\n",
      "# of batch: 2222\n",
      "train accuracy: 0.93444896\n",
      "One batch end: 1831.8200492858887\n",
      "# of batch: 2223\n",
      "train accuracy: 0.93445146\n",
      "One batch end: 1832.7470526695251\n",
      "# of batch: 2224\n",
      "train accuracy: 0.9344719\n",
      "One batch end: 1833.6740517616272\n",
      "# of batch: 2225\n",
      "train accuracy: 0.93450135\n",
      "One batch end: 1834.7080473899841\n",
      "# of batch: 2226\n",
      "train accuracy: 0.9345218\n",
      "One batch end: 1835.6903386116028\n",
      "# of batch: 2227\n",
      "train accuracy: 0.9345422\n",
      "One batch end: 1836.6098456382751\n",
      "# of batch: 2228\n",
      "train accuracy: 0.9345446\n",
      "One batch end: 1837.5433719158173\n",
      "# of batch: 2229\n",
      "train accuracy: 0.934574\n",
      "One batch end: 1838.5293669700623\n",
      "# of batch: 2230\n",
      "train accuracy: 0.93459433\n",
      "One batch end: 1839.4733657836914\n",
      "# of batch: 2231\n",
      "train accuracy: 0.9346147\n",
      "One batch end: 1840.3763654232025\n",
      "# of batch: 2232\n",
      "train accuracy: 0.934644\n",
      "One batch end: 1841.311364889145\n",
      "# of batch: 2233\n",
      "train accuracy: 0.9346553\n",
      "One batch end: 1842.2518565654755\n",
      "# of batch: 2234\n",
      "train accuracy: 0.9346577\n",
      "One batch end: 1843.1898560523987\n",
      "# of batch: 2235\n",
      "train accuracy: 0.93468696\n",
      "One batch end: 1844.1003801822662\n",
      "# of batch: 2236\n",
      "train accuracy: 0.93471617\n",
      "One batch end: 1845.040384054184\n",
      "# of batch: 2237\n",
      "train accuracy: 0.9347453\n",
      "One batch end: 1845.9604239463806\n",
      "# of batch: 2238\n",
      "train accuracy: 0.93477446\n",
      "One batch end: 1847.2614192962646\n",
      "# of batch: 2239\n",
      "train accuracy: 0.9347857\n",
      "One batch end: 1848.1839308738708\n",
      "# of batch: 2240\n",
      "train accuracy: 0.93477017\n",
      "One batch end: 1849.1169335842133\n",
      "# of batch: 2241\n",
      "train accuracy: 0.9347904\n",
      "One batch end: 1850.0529327392578\n",
      "# of batch: 2242\n",
      "train accuracy: 0.93478376\n",
      "One batch end: 1851.049928188324\n",
      "# of batch: 2243\n",
      "train accuracy: 0.9348039\n",
      "One batch end: 1852.080931186676\n",
      "# of batch: 2244\n",
      "train accuracy: 0.93481517\n",
      "One batch end: 1853.1029269695282\n",
      "# of batch: 2245\n",
      "train accuracy: 0.93483526\n",
      "One batch end: 1854.1069667339325\n",
      "# of batch: 2246\n",
      "train accuracy: 0.93485534\n",
      "One batch end: 1855.0339663028717\n",
      "# of batch: 2247\n",
      "train accuracy: 0.9348754\n",
      "One batch end: 1855.9724822044373\n",
      "# of batch: 2248\n",
      "train accuracy: 0.9349044\n",
      "One batch end: 1856.8714818954468\n",
      "# of batch: 2249\n",
      "train accuracy: 0.93490666\n",
      "One batch end: 1857.7804811000824\n",
      "# of batch: 2250\n",
      "train accuracy: 0.9349267\n",
      "One batch end: 1858.70600938797\n",
      "# of batch: 2251\n",
      "train accuracy: 0.9349467\n",
      "One batch end: 1859.644008398056\n",
      "# of batch: 2252\n",
      "train accuracy: 0.93495786\n",
      "One batch end: 1860.5695233345032\n",
      "# of batch: 2253\n",
      "train accuracy: 0.93496895\n",
      "One batch end: 1861.5000462532043\n",
      "# of batch: 2254\n",
      "train accuracy: 0.93498003\n",
      "One batch end: 1862.4065690040588\n",
      "# of batch: 2255\n",
      "train accuracy: 0.935\n",
      "One batch end: 1863.336091518402\n",
      "# of batch: 2256\n",
      "train accuracy: 0.9350199\n",
      "One batch end: 1864.2800896167755\n",
      "# of batch: 2257\n",
      "train accuracy: 0.935031\n",
      "One batch end: 1865.1950886249542\n",
      "# of batch: 2258\n",
      "train accuracy: 0.9350421\n",
      "One batch end: 1866.192415714264\n",
      "# of batch: 2259\n",
      "train accuracy: 0.93506193\n",
      "One batch end: 1867.183930158615\n",
      "# of batch: 2260\n",
      "train accuracy: 0.93506414\n",
      "One batch end: 1868.2199342250824\n",
      "# of batch: 2261\n",
      "train accuracy: 0.93509287\n",
      "One batch end: 1869.1649286746979\n",
      "# of batch: 2262\n",
      "train accuracy: 0.93512154\n",
      "One batch end: 1870.0689330101013\n",
      "# of batch: 2263\n",
      "train accuracy: 0.9351502\n",
      "One batch end: 1870.9529750347137\n",
      "# of batch: 2264\n",
      "train accuracy: 0.9351612\n",
      "One batch end: 1871.8579740524292\n",
      "# of batch: 2265\n",
      "train accuracy: 0.93518096\n",
      "One batch end: 1872.8139741420746\n",
      "# of batch: 2266\n",
      "train accuracy: 0.9351742\n",
      "One batch end: 1873.7734832763672\n",
      "# of batch: 2267\n",
      "train accuracy: 0.935194\n",
      "One batch end: 1874.6744825839996\n",
      "# of batch: 2268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.93522257\n",
      "One batch end: 1875.6214821338654\n",
      "# of batch: 2269\n",
      "train accuracy: 0.9352511\n",
      "One batch end: 1876.6599988937378\n",
      "# of batch: 2270\n",
      "train accuracy: 0.9352796\n",
      "One batch end: 1877.6480021476746\n",
      "# of batch: 2271\n",
      "train accuracy: 0.9352993\n",
      "One batch end: 1878.6155169010162\n",
      "# of batch: 2272\n",
      "train accuracy: 0.93531895\n",
      "One batch end: 1879.597029209137\n",
      "# of batch: 2273\n",
      "train accuracy: 0.9353298\n",
      "One batch end: 1880.5635368824005\n",
      "# of batch: 2274\n",
      "train accuracy: 0.9353319\n",
      "One batch end: 1881.5070431232452\n",
      "# of batch: 2275\n",
      "train accuracy: 0.9353515\n",
      "One batch end: 1882.4610407352448\n",
      "# of batch: 2276\n",
      "train accuracy: 0.9353711\n",
      "One batch end: 1883.4140439033508\n",
      "# of batch: 2277\n",
      "train accuracy: 0.9353556\n",
      "One batch end: 1884.4080431461334\n",
      "# of batch: 2278\n",
      "train accuracy: 0.9353664\n",
      "One batch end: 1885.4370384216309\n",
      "# of batch: 2279\n",
      "train accuracy: 0.93539476\n",
      "One batch end: 1886.5145978927612\n",
      "# of batch: 2280\n",
      "train accuracy: 0.9354143\n",
      "One batch end: 1887.5275931358337\n",
      "# of batch: 2281\n",
      "train accuracy: 0.9354338\n",
      "One batch end: 1888.4775965213776\n",
      "# of batch: 2282\n",
      "train accuracy: 0.9354446\n",
      "One batch end: 1889.4275958538055\n",
      "# of batch: 2283\n",
      "train accuracy: 0.93547285\n",
      "One batch end: 1890.3760209083557\n",
      "# of batch: 2284\n",
      "train accuracy: 0.93549234\n",
      "One batch end: 1891.3000218868256\n",
      "# of batch: 2285\n",
      "train accuracy: 0.93548554\n",
      "One batch end: 1892.204020500183\n",
      "# of batch: 2286\n",
      "train accuracy: 0.9354875\n",
      "One batch end: 1893.1775407791138\n",
      "# of batch: 2287\n",
      "train accuracy: 0.93549824\n",
      "One batch end: 1894.1495401859283\n",
      "# of batch: 2288\n",
      "train accuracy: 0.93551767\n",
      "One batch end: 1895.0977716445923\n",
      "# of batch: 2289\n",
      "train accuracy: 0.9355284\n",
      "One batch end: 1896.041204214096\n",
      "# of batch: 2290\n",
      "train accuracy: 0.93554777\n",
      "One batch end: 1896.9747138023376\n",
      "# of batch: 2291\n",
      "train accuracy: 0.93555844\n",
      "One batch end: 1897.9347178936005\n",
      "# of batch: 2292\n",
      "train accuracy: 0.9355691\n",
      "One batch end: 1898.9567425251007\n",
      "# of batch: 2293\n",
      "train accuracy: 0.9355972\n",
      "One batch end: 1899.9607419967651\n",
      "# of batch: 2294\n",
      "train accuracy: 0.93562526\n",
      "One batch end: 1901.0127413272858\n",
      "# of batch: 2295\n",
      "train accuracy: 0.93563586\n",
      "One batch end: 1902.0444974899292\n",
      "# of batch: 2296\n",
      "train accuracy: 0.9356552\n",
      "One batch end: 1903.0234997272491\n",
      "# of batch: 2297\n",
      "train accuracy: 0.9356745\n",
      "One batch end: 1904.0034992694855\n",
      "# of batch: 2298\n",
      "train accuracy: 0.9356851\n",
      "One batch end: 1904.9744985103607\n",
      "# of batch: 2299\n",
      "train accuracy: 0.93570435\n",
      "One batch end: 1905.9304978847504\n",
      "# of batch: 2300\n",
      "train accuracy: 0.9357323\n",
      "One batch end: 1906.8922209739685\n",
      "# of batch: 2301\n",
      "train accuracy: 0.93574286\n",
      "One batch end: 1907.8452203273773\n",
      "# of batch: 2302\n",
      "train accuracy: 0.93577075\n",
      "One batch end: 1908.7802205085754\n",
      "# of batch: 2303\n",
      "train accuracy: 0.93578994\n",
      "One batch end: 1909.712741613388\n",
      "# of batch: 2304\n",
      "train accuracy: 0.93580043\n",
      "One batch end: 1910.6827414035797\n",
      "# of batch: 2305\n",
      "train accuracy: 0.9358196\n",
      "One batch end: 1911.6432583332062\n",
      "# of batch: 2306\n",
      "train accuracy: 0.93583006\n",
      "One batch end: 1912.57834982872\n",
      "# of batch: 2307\n",
      "train accuracy: 0.9358579\n",
      "One batch end: 1913.5238547325134\n",
      "# of batch: 2308\n",
      "train accuracy: 0.93588567\n",
      "One batch end: 1914.4773797988892\n",
      "# of batch: 2309\n",
      "train accuracy: 0.93590474\n",
      "One batch end: 1915.4513802528381\n",
      "# of batch: 2310\n",
      "train accuracy: 0.9359065\n",
      "One batch end: 1916.3838906288147\n",
      "# of batch: 2311\n",
      "train accuracy: 0.9359256\n",
      "One batch end: 1917.297894001007\n",
      "# of batch: 2312\n",
      "train accuracy: 0.9359447\n",
      "One batch end: 1918.214893579483\n",
      "# of batch: 2313\n",
      "train accuracy: 0.93595505\n",
      "One batch end: 1919.1384019851685\n",
      "# of batch: 2314\n",
      "train accuracy: 0.9359654\n",
      "One batch end: 1920.0814015865326\n",
      "# of batch: 2315\n",
      "train accuracy: 0.93598443\n",
      "One batch end: 1921.0289154052734\n",
      "# of batch: 2316\n",
      "train accuracy: 0.9360121\n",
      "One batch end: 1921.9754741191864\n",
      "# of batch: 2317\n",
      "train accuracy: 0.93603104\n",
      "One batch end: 1922.9454772472382\n",
      "# of batch: 2318\n",
      "train accuracy: 0.93605\n",
      "One batch end: 1923.8804771900177\n",
      "# of batch: 2319\n",
      "train accuracy: 0.9360517\n",
      "One batch end: 1924.8454720973969\n",
      "# of batch: 2320\n",
      "train accuracy: 0.9360707\n",
      "One batch end: 1925.8544716835022\n",
      "# of batch: 2321\n",
      "train accuracy: 0.9360982\n",
      "One batch end: 1926.8711564540863\n",
      "# of batch: 2322\n",
      "train accuracy: 0.9361257\n",
      "One batch end: 1927.8370740413666\n",
      "# of batch: 2323\n",
      "train accuracy: 0.9361532\n",
      "One batch end: 1928.7820899486542\n",
      "# of batch: 2324\n",
      "train accuracy: 0.93618065\n",
      "One batch end: 1929.7086112499237\n",
      "# of batch: 2325\n",
      "train accuracy: 0.9361995\n",
      "One batch end: 1930.67160654068\n",
      "# of batch: 2326\n",
      "train accuracy: 0.9362269\n",
      "One batch end: 1931.646606206894\n",
      "# of batch: 2327\n",
      "train accuracy: 0.9362457\n",
      "One batch end: 1932.5526068210602\n",
      "# of batch: 2328\n",
      "train accuracy: 0.9362645\n",
      "One batch end: 1933.5731315612793\n",
      "# of batch: 2329\n",
      "train accuracy: 0.9362747\n",
      "One batch end: 1934.599127292633\n",
      "# of batch: 2330\n",
      "train accuracy: 0.93628484\n",
      "One batch end: 1935.6776549816132\n",
      "# of batch: 2331\n",
      "train accuracy: 0.93629503\n",
      "One batch end: 1936.6506493091583\n",
      "# of batch: 2332\n",
      "train accuracy: 0.93629664\n",
      "One batch end: 1937.6451675891876\n",
      "# of batch: 2333\n",
      "train accuracy: 0.93631536\n",
      "One batch end: 1938.6184566020966\n",
      "# of batch: 2334\n",
      "train accuracy: 0.9363341\n",
      "One batch end: 1939.5374555587769\n",
      "# of batch: 2335\n",
      "train accuracy: 0.9363527\n",
      "One batch end: 1940.4524552822113\n",
      "# of batch: 2336\n",
      "train accuracy: 0.93637145\n",
      "One batch end: 1941.3804552555084\n",
      "# of batch: 2337\n",
      "train accuracy: 0.9363901\n",
      "One batch end: 1942.3194539546967\n",
      "# of batch: 2338\n",
      "train accuracy: 0.9364002\n",
      "One batch end: 1943.2394528388977\n",
      "# of batch: 2339\n",
      "train accuracy: 0.93641025\n",
      "One batch end: 1944.300855398178\n",
      "# of batch: 2340\n",
      "train accuracy: 0.9364289\n",
      "One batch end: 1945.2118542194366\n",
      "# of batch: 2341\n",
      "train accuracy: 0.9364389\n",
      "One batch end: 1946.2762324810028\n",
      "# of batch: 2342\n",
      "train accuracy: 0.9364661\n",
      "One batch end: 1947.2607491016388\n",
      "# of batch: 2343\n",
      "train accuracy: 0.93649316\n",
      "One batch end: 1948.1647481918335\n",
      "# of batch: 2344\n",
      "train accuracy: 0.9365203\n",
      "One batch end: 1949.07426571846\n",
      "# of batch: 2345\n",
      "train accuracy: 0.9365388\n",
      "One batch end: 1949.954268693924\n",
      "# of batch: 2346\n",
      "train accuracy: 0.9365573\n",
      "One batch end: 1950.8472645282745\n",
      "# of batch: 2347\n",
      "train accuracy: 0.9365673\n",
      "One batch end: 1951.8597745895386\n",
      "# of batch: 2348\n",
      "train accuracy: 0.93657726\n",
      "One batch end: 1952.7787730693817\n",
      "# of batch: 2349\n",
      "train accuracy: 0.93659574\n",
      "One batch end: 1953.7677776813507\n",
      "# of batch: 2350\n",
      "train accuracy: 0.9366057\n",
      "One batch end: 1954.7507772445679\n",
      "# of batch: 2351\n",
      "train accuracy: 0.93662417\n",
      "One batch end: 1955.7442905902863\n",
      "# of batch: 2352\n",
      "train accuracy: 0.9366426\n",
      "One batch end: 1956.7082905769348\n",
      "# of batch: 2353\n",
      "train accuracy: 0.9366525\n",
      "One batch end: 1957.5897986888885\n",
      "# of batch: 2354\n",
      "train accuracy: 0.9366794\n",
      "One batch end: 1958.5087978839874\n",
      "# of batch: 2355\n",
      "train accuracy: 0.93667233\n",
      "One batch end: 1959.449793100357\n",
      "# of batch: 2356\n",
      "train accuracy: 0.9366907\n",
      "One batch end: 1960.42031788826\n",
      "# of batch: 2357\n",
      "train accuracy: 0.93671757\n",
      "One batch end: 1961.3703129291534\n",
      "# of batch: 2358\n",
      "train accuracy: 0.9367359\n",
      "One batch end: 1962.3447844982147\n",
      "# of batch: 2359\n",
      "train accuracy: 0.93674576\n",
      "One batch end: 1963.3237793445587\n",
      "# of batch: 2360\n",
      "train accuracy: 0.9367556\n",
      "One batch end: 1964.201297044754\n",
      "# of batch: 2361\n",
      "train accuracy: 0.93676543\n",
      "One batch end: 1965.1162972450256\n",
      "# of batch: 2362\n",
      "train accuracy: 0.9367922\n",
      "One batch end: 1966.0723638534546\n",
      "# of batch: 2363\n",
      "train accuracy: 0.936802\n",
      "One batch end: 1967.0018706321716\n",
      "# of batch: 2364\n",
      "train accuracy: 0.93681186\n",
      "One batch end: 1967.933875799179\n",
      "# of batch: 2365\n",
      "train accuracy: 0.9368301\n",
      "One batch end: 1968.8853936195374\n",
      "# of batch: 2366\n",
      "train accuracy: 0.93684834\n",
      "One batch end: 1969.8133957386017\n",
      "# of batch: 2367\n",
      "train accuracy: 0.936875\n",
      "One batch end: 1970.7673902511597\n",
      "# of batch: 2368\n",
      "train accuracy: 0.93688476\n",
      "One batch end: 1971.8079149723053\n",
      "# of batch: 2369\n",
      "train accuracy: 0.93689454\n",
      "One batch end: 1972.9049253463745\n",
      "# of batch: 2370\n",
      "train accuracy: 0.9369211\n",
      "One batch end: 1973.9039261341095\n",
      "# of batch: 2371\n",
      "train accuracy: 0.93693084\n",
      "One batch end: 1974.9490118026733\n",
      "# of batch: 2372\n",
      "train accuracy: 0.93693215\n",
      "One batch end: 1975.9697749614716\n",
      "# of batch: 2373\n",
      "train accuracy: 0.93694186\n",
      "One batch end: 1977.1017763614655\n",
      "# of batch: 2374\n",
      "train accuracy: 0.93696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One batch end: 1978.2697739601135\n",
      "# of batch: 2375\n",
      "train accuracy: 0.9369613\n",
      "One batch end: 1979.388299703598\n",
      "# of batch: 2376\n",
      "train accuracy: 0.9369794\n",
      "One batch end: 1980.381294965744\n",
      "# of batch: 2377\n",
      "train accuracy: 0.9370059\n",
      "One batch end: 1981.318818807602\n",
      "# of batch: 2378\n",
      "train accuracy: 0.9370071\n",
      "One batch end: 1982.3783173561096\n",
      "# of batch: 2379\n",
      "train accuracy: 0.9370168\n",
      "One batch end: 1983.4601881504059\n",
      "# of batch: 2380\n",
      "train accuracy: 0.93703485\n",
      "One batch end: 1984.5271909236908\n",
      "# of batch: 2381\n",
      "train accuracy: 0.9370445\n",
      "One batch end: 1985.513712644577\n",
      "# of batch: 2382\n",
      "train accuracy: 0.9370625\n",
      "One batch end: 1986.3943111896515\n",
      "# of batch: 2383\n",
      "train accuracy: 0.93706375\n",
      "One batch end: 1987.4233105182648\n",
      "# of batch: 2384\n",
      "train accuracy: 0.937065\n",
      "One batch end: 1988.4138135910034\n",
      "# of batch: 2385\n",
      "train accuracy: 0.93709135\n",
      "One batch end: 1989.484820842743\n",
      "# of batch: 2386\n",
      "train accuracy: 0.9371177\n",
      "One batch end: 1990.4728157520294\n",
      "# of batch: 2387\n",
      "train accuracy: 0.93711895\n",
      "One batch end: 1991.4213180541992\n",
      "# of batch: 2388\n",
      "train accuracy: 0.9371285\n",
      "One batch end: 1992.3813216686249\n",
      "# of batch: 2389\n",
      "train accuracy: 0.9371464\n",
      "One batch end: 1993.3267362117767\n",
      "# of batch: 2390\n",
      "train accuracy: 0.93716437\n",
      "One batch end: 1994.2637360095978\n",
      "# of batch: 2391\n",
      "train accuracy: 0.93718225\n",
      "One batch end: 1995.1957309246063\n",
      "# of batch: 2392\n",
      "train accuracy: 0.93720853\n",
      "One batch end: 1996.1277341842651\n",
      "# of batch: 2393\n",
      "train accuracy: 0.93721807\n",
      "One batch end: 1997.0637340545654\n",
      "# of batch: 2394\n",
      "train accuracy: 0.93721086\n",
      "One batch end: 1998.053806066513\n",
      "# of batch: 2395\n",
      "train accuracy: 0.937212\n",
      "One batch end: 1999.074805021286\n",
      "# of batch: 2396\n",
      "train accuracy: 0.9372382\n",
      "One batch end: 2000.146800994873\n",
      "# of batch: 2397\n",
      "train accuracy: 0.93725604\n",
      "One batch end: 2001.215799331665\n",
      "# of batch: 2398\n",
      "train accuracy: 0.9372822\n",
      "One batch end: 2002.2128591537476\n",
      "# of batch: 2399\n",
      "train accuracy: 0.9373083\n",
      "One batch end: 2003.1498584747314\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "\n",
    "W_w1 = w_initial1\n",
    "W_b1 = b_initial1\n",
    "W_w2 = w_initial2\n",
    "W_b2 = b_initial2\n",
    "# W_w3 = w_initial3\n",
    "# W_b3 = b_initial3\n",
    "# W_w4 = w_initial4\n",
    "# W_b4 = b_initial4\n",
    "# W_w5 = w_initial5\n",
    "# W_b5 = b_initial5\n",
    "\n",
    "grads_list_w1 = np.zeros((kernelsize,kernelsize,1,filter_num1))\n",
    "grads_list_b1 = np.zeros((filter_num1,))\n",
    "grads_list_w2 = np.zeros((kernelsize,kernelsize,filter_num1,filter_num2))\n",
    "grads_list_b2 = np.zeros((filter_num2,))\n",
    "# grads_list_w3 = np.zeros((kernelsize,kernelsize,filter_num2,filter_num3))\n",
    "# grads_list_b3 = np.zeros((filter_num3,))\n",
    "# grads_list_w4 = np.zeros((784,64))\n",
    "# grads_list_b4 = np.zeros((64,))\n",
    "# grads_list_w5 = np.zeros((64,10))\n",
    "# grads_list_b5 = np.zeros((10,))\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    print('# of batch:',batch_index)\n",
    "    X, y = data_loader.get_batch(batch_size)  \n",
    "    W_x ,_ = intitial_opinion_trust(X)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred,opinion_conv1,opinion_conv2,opinion_conv = model([X, W_x, W_w1, W_b1, W_w2, W_b2])\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        \n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "    sparse_categorical_accuracy.update_state(y_true=y, y_pred=y_pred)\n",
    "    opinion_convlist.append(np.reshape(opinion_conv, (opinion_conv.shape[0], \n",
    "                                                      int(opinion_conv.shape[1]*opinion_conv.shape[2]*opinion_conv.shape[3]),4)))\n",
    "    train_loss.append(tf.reduce_mean(loss).numpy())\n",
    "    train_acc.append(sparse_categorical_accuracy.result().numpy())\n",
    "    print(\"train accuracy:\",sparse_categorical_accuracy.result().numpy())\n",
    "#     print('Model update end:',(time.time()-start))\n",
    "    \n",
    "    \n",
    "# update W_w, W_b\n",
    "    grads_list_w1 = grads_list_w1 + tf.where(abs(grads[0])<threshold1, 1, 0)  #  shape=(5, 5, 3, 8)\n",
    "    grads_list_b1 = grads_list_b1 + tf.where(abs(grads[1])<threshold1, 1, 0)  #  shape=(8,)\n",
    "    grads_list_w2 = grads_list_w2 + tf.where(abs(grads[2])<threshold2, 1, 0)  #  shape=(5, 5, 8, 12)\n",
    "    grads_list_b2 = grads_list_b2 + tf.where(abs(grads[3])<threshold2, 1, 0)  #  shape=(12,)\n",
    "#     grads_list_w3 = grads_list_w3 + tf.where(abs(grads[4])<threshold, 1, 0)  #  shape=(5, 5, 12, 12)\n",
    "#     grads_list_b3 = grads_list_b3 + tf.where(abs(grads[5])<threshold, 1, 0)  #  shape=(12,)\n",
    "#     grads_list_w4 = grads_list_w4 + tf.where(abs(grads[6])<threshold, 1, 0)  #  shape=(5, 5, 12, 12)\n",
    "#     grads_list_b4 = grads_list_b4 + tf.where(abs(grads[7])<threshold, 1, 0)  #  shape=(12,)\n",
    "#     grads_list_w5 = grads_list_w5 + tf.where(abs(grads[8])<threshold, 1, 0)  #  shape=(5, 5, 12, 12)\n",
    "#     grads_list_b5 = grads_list_b5 + tf.where(abs(grads[9])<threshold, 1, 0)  #  shape=(12,)\n",
    "    \n",
    "#     print('W_w, W_b update start:',(time.time()-start))  # 20s\n",
    "    \n",
    "    W_w1, W_b1 = get_update_matrix(grads_list_w1, grads_list_b1, batch_index+1)\n",
    "    W_w2, W_b2 = get_update_matrix(grads_list_w2, grads_list_b2, batch_index+1)\n",
    "#     W_w3, W_b3 = get_update_matrix(grads_list_w3, grads_list_b3, batch_index+1)\n",
    "#     W_w4, W_b4 = get_update_matrix(grads_list_w4, grads_list_b4, batch_index+1)\n",
    "#     W_w5, W_b5 = get_update_matrix(grads_list_w5, grads_list_b5, batch_index+1)\n",
    "\n",
    "    y_N_update, y_N_op = evidence_collect(y, y_pred)\n",
    "    \n",
    "    y_N_opinion.append(y_N_op)\n",
    "    y_update_wb.append(y_N_update)\n",
    "\n",
    "    print('One batch end:',(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    " \n",
    "def load_variavle(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_pool_MINIST1_bias_opinion2_sub)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_weights('model_max_pool_MINIST1_sub')\n",
    "save_variable(W_w1, 'max_pool_MINST1_weight_opinion1_sub')\n",
    "save_variable(W_w2, 'max_pool_MINST1_weight_opinion2_sub')\n",
    "\n",
    "save_variable(W_b1, 'max_pool_MINIST1_bias_opinion1_sub')\n",
    "save_variable(W_b2, 'max_pool_MINIST1_bias_opinion2_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('my_model_max_trust_MINIST')\n",
    "# W_w1 = load_variavle('MINST_weight_opinion1')\n",
    "# W_w2 = load_variavle('MINST_weight_opinion2')\n",
    "# W_b1 = load_variavle('MINIST_bias_opinion1')\n",
    "# W_b2 = load_variavle('MINIST_bias_opinion2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "sGew9OE1xVOK",
    "outputId": "59d12435-fc79-4e80-ddb5-e5754206c192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test= data_loader.get_batch_test()\n",
    "W_x ,_ = intitial_opinion_trust(X_test)\n",
    "y_pred_test, _,_,_ = model([X_test, W_x, W_w1, W_b1, W_w2, W_b2])\n",
    "sparse_categorical_accuracy.reset_states()\n",
    "sparse_categorical_accuracy.update_state(y_true=y_test, y_pred=y_pred_test)\n",
    "print(\"test accuracy:\",sparse_categorical_accuracy.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "TOopxTrEO4b6",
    "outputId": "158d5df2-337f-44eb-eda3-a5881e38c6b8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.6267\n"
     ]
    }
   ],
   "source": [
    "# test noisy dataset\n",
    "X_test_noise, y_test_noise = data_loader.get_noise('gaussian')\n",
    "noise_opinion,_ = intitial_opinion_trust(X_test_noise)\n",
    "y_pred_test_noise,opinion_conv1,opinion_conv2,opinion_conv = model([X_test_noise, noise_opinion, W_w1, W_b1, W_w2, W_b2])\n",
    "sparse_categorical_accuracy.reset_states()\n",
    "sparse_categorical_accuracy.update_state(y_true=y_test_noise, y_pred=y_pred_test_noise)\n",
    "print(\"test accuracy:\",sparse_categorical_accuracy.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8536\n"
     ]
    }
   ],
   "source": [
    "# test localvar\n",
    "X_test_local_noise, y_test_local_noise, case1_opinion, noise_index = data_loader.get_local_noise()\n",
    "case2_opinion = intitial_W_x_location(X_test_local_noise, noise_index)\n",
    "case3_opinion = intitial_W_x(X_test_local_noise)\n",
    "y_pred_test_local_noise,_,_,opinion_noise = model([X_test_local_noise, case1_opinion, W_w1, W_b1, W_w2, W_b2])\n",
    "sparse_categorical_accuracy.reset_states()\n",
    "sparse_categorical_accuracy.update_state(y_true=y_test_local_noise, y_pred=y_pred_test_local_noise)\n",
    "print(\"test accuracy:\",sparse_categorical_accuracy.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.76336>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.030073041>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.moments(tf.constant([0.7659,0.5648,0.573,0.9616,0.9515]),axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NN_trust(opinion_last_layer, true_label, pre_label, y_update_wb):\n",
    "    # compute dense opinion\n",
    "    opinion_dense = np.average(np.array(opinion_last_layer), axis=0)\n",
    "    opinion_dense = np.reshape(opinion_dense, \n",
    "                                     (int(opinion_dense.shape[0]*opinion_dense.shape[1]*opinion_dense.shape[2]),4))\n",
    "    y_true_op = [1.0, 0.0, 0.0, 0.5]\n",
    "    W_y_update = evidence_collect_test(true_label, pre_label)\n",
    "    \n",
    "    # compute Backward opinion of neuron W_N_Y  \n",
    "    W_N_Y=[]\n",
    "    for j in W_y_update:\n",
    "        W_N_Y.append(multi(j,y_true_op)) # change when add flaw in label\n",
    "        \n",
    "    W_w_dense = y_update_wb[-1]\n",
    "    W_b_dense = y_update_wb[-1]\n",
    "\n",
    "    W_xw=[]\n",
    "    W_xw.append(W_b_dense)\n",
    "    for j in range(opinion_dense.shape[0]):\n",
    "        W_xw.append(multi(W_w_dense,opinion_dense[j])) # (513, 4)\n",
    "    dense_out = fusion(np.array(W_xw))\n",
    "    #     print('Underflow or not: ',np.isnan(np.min(np.array(dense1_out_list))))\n",
    "\n",
    "    # last layer\n",
    "    W_xw=[]\n",
    "    W_xw.append(W_b_dense)\n",
    "    for j in range(64):\n",
    "        W_xw.append(multi(W_w_dense,dense_out))\n",
    "    W_NN = fusion(np.array(W_xw))\n",
    "    \n",
    "    # compute last layer output opinion and trust\n",
    "    W_XY_one = []\n",
    "    for j in range(10):\n",
    "        W_XY_one.append(fusion_2(W_NN,W_N_Y[j]))\n",
    "    W_NN = fusion(np.array(W_XY_one))\n",
    "    W_trust = W_NN[0]+W_NN[2]*W_NN[3]\n",
    "    \n",
    "    return W_trust, W_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FybnpbUYxVOI"
   },
   "outputs": [],
   "source": [
    "# forward opinion prop\n",
    "def mul_scale(W_x):\n",
    "    mul_u = 1.0\n",
    "    for i in range(len(W_x)):\n",
    "        mul_u = mul_u * W_x[i]\n",
    "        if mul_u <= 1e-24: \n",
    "            mul_u = mul_u*1e25\n",
    "    return mul_u\n",
    "\n",
    "def fusion(W_x): # W_x array\n",
    "\n",
    "    deno = 0.0\n",
    "    mole = 0.0\n",
    "    full_multi = mul_scale(W_x[:,2])\n",
    "    for i in range(len(W_x[:,2])):\n",
    "        deno = deno + full_multi/W_x[:,2][i]\n",
    "        mole = mole + (full_multi/W_x[:,2][i])*W_x[:,0][i]\n",
    "    W_b = mole/deno  #  change\n",
    "    W_u = (len(W_x)*full_multi)/deno   # change\n",
    "    W_a = sum(W_x[:,3])/len(W_x)  \n",
    "\n",
    "#     W_b = sum(1/len(W_x)*W_x[:,0])\n",
    "#     W_u = 0.0\n",
    "#     W_a = sum(1/len(W_x)*W_x[:,3])\n",
    "        \n",
    "    return [W_b,1-W_b-W_u,W_u,W_a]\n",
    "\n",
    "def fusion_2(W_x, W_y):\n",
    "    if W_x[2]!=0 or W_y[2]!=0:\n",
    "        W_b = (W_x[0]*W_y[2]+W_y[0]*W_x[2])/(W_x[2]+W_y[2])\n",
    "        W_u = 2*W_x[2]*W_y[2]/(W_x[2]+W_y[2])\n",
    "        W_a = (W_x[3]+W_y[3])/2  \n",
    "    elif W_x[2]==0 and W_y[2]==0:\n",
    "        W_b = 0.5*W_x[0]+0.5*W_y[0]\n",
    "        W_u = 0\n",
    "        W_a = 0.5*W_x[3]+0.5*W_y[3]\n",
    "    return [W_b,1-W_b-W_u,W_u,W_a]\n",
    "\n",
    "def multi(W_x, W_y): # \n",
    "    W_b = W_x[0]*W_y[0]+((1-W_x[3])*W_y[3]*W_x[0]*W_y[2]+W_x[3]*(1-W_y[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n",
    "    W_d = W_x[1]+W_y[1]-W_x[1]*W_y[1]\n",
    "    W_u = W_x[2]*W_y[2]+((1-W_y[3])*W_x[0]*W_y[2]+(1-W_x[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n",
    "    W_a = W_x[3]*W_y[3]\n",
    "    return [W_b,W_d,W_u,W_a]\n",
    "\n",
    "\n",
    "def evidence_collect_test(y, y_pred):\n",
    "    r_list = [0]*10\n",
    "    s_list = [0]*10\n",
    "    index = np.random.randint(0,y.shape[0], 5000)\n",
    "    \n",
    "    for j in index:\n",
    "        for i in range(len(y_pred[0])):\n",
    "            if i == y[j]:\n",
    "                if y_pred[j][i] > 0.9:\n",
    "                    r_list[i]+=1\n",
    "                else:\n",
    "                    s_list[i]+=1\n",
    "            else:\n",
    "                if y_pred[j][i] < 0.1:\n",
    "                    r_list[i]+=1\n",
    "                else:\n",
    "                    s_list[i]+=1\n",
    "                    \n",
    "    y_N_op = []\n",
    "    for i in range(len(r_list)):\n",
    "        y_N_op.append(np.array([r_list[i]/(r_list[i]+s_list[i]+2), \n",
    "                       s_list[i]/(r_list[i]+s_list[i]+2), 2/(r_list[i]+s_list[i]+2), 0.5]))\n",
    "\n",
    "    \n",
    "    return y_N_op\n",
    "\n",
    "# def evidence_collect_test(y, y_pred):\n",
    "#     r_list = 0\n",
    "#     s_list = 0\n",
    "#     index = np.random.randint(0,y.shape[0], 1000)\n",
    "    \n",
    "#     for i in index:\n",
    "#         if np.argmax(y_pred[i]) == y[i]:\n",
    "#             r_list+=1\n",
    "#         else:\n",
    "#             s_list+=1\n",
    "                    \n",
    "#     return np.array([r_list/(r_list+s_list+2), s_list/(r_list+s_list+2), 2/(r_list+s_list+2), 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original image trustworthiness result:',get_NN_trust(opinion_original, y_test, y_pred_test, y_update_wb)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Noise image trustworthiness result:',get_NN_trust(opinion_noise, y_test_noise, y_pred_test_noise, y_update_wb)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LcVJfx3xVOK"
   },
   "outputs": [],
   "source": [
    "W_y_update = evidence_collect_test(y_test_noise, y_pred_test_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TqNZd2HxVOK"
   },
   "outputs": [],
   "source": [
    "# compute Backward opinion of neuron W_N_Y  \n",
    "W_N_Y_noise=[]\n",
    "for j in W_y_update:\n",
    "    W_N_Y_noise.append(multi(j,y_true_op)) # change when add flaw in label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXBrgH_rxVOK"
   },
   "outputs": [],
   "source": [
    "opinion_dense_noise = np.average(np.array(opinion_mid), axis=0)\n",
    "opinion_dense_noise = np.reshape(opinion_dense_noise, \n",
    "                                 (int(opinion_dense_noise.shape[0]*opinion_dense_noise.shape[1]*opinion_dense_noise.shape[2]),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFSE0_K0xVOL"
   },
   "outputs": [],
   "source": [
    "W_w_dense = y_update_wb[-1]\n",
    "W_b_dense = y_update_wb[-1]\n",
    "input_dense = opinion_dense_noise\n",
    "\n",
    "W_xw=[]\n",
    "W_xw.append(W_b_dense)\n",
    "for j in range(opinion_dense_noise.shape[0]):\n",
    "    W_xw.append(multi(W_w_dense,input_dense[j]))\n",
    "dense_out_noise = fusion(np.array(W_xw))\n",
    "#     print('Underflow or not: ',np.isnan(np.min(np.array(dense1_out_list))))\n",
    "\n",
    "# last layer\n",
    "W_xw=[]\n",
    "W_xw.append(W_b_dense)\n",
    "for j in range(64):\n",
    "    W_xw.append(multi(W_w_dense,dense_out_noise))\n",
    "W_NN_noise = fusion(np.array(W_xw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7jZ9h3IxVOL"
   },
   "outputs": [],
   "source": [
    "# compute last layer output opinion and trust\n",
    "W_XY_one = []\n",
    "for j in range(10):\n",
    "    W_XY_one.append(fusion_2(W_NN_noise,W_N_Y_noise[j]))\n",
    "\n",
    "W_NN = fusion(np.array(W_XY_one))\n",
    "W_trust = W_NN[0]+W_NN[2]*W_NN[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpeUgQToxVOL",
    "outputId": "19e6ba2d-52be-454d-8ac5-fdd3e00c2d24",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVDfqwfrxVOL",
    "outputId": "76bce6f9-297c-475a-93f1-8f43a7d64d93"
   },
   "outputs": [],
   "source": [
    "entropy = -1 * (W_trust * np.log2(W_trust) + (1-W_trust) * np.log2(1-W_trust))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yls_EC7oxVOL"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSYzNaUQxVOL"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(1,11):\n",
    "    plt.subplot(1,10,i)\n",
    "    plt.imshow(trust1[0,:,:,i-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtsl2qAKxVOL"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(1,20,i)\n",
    "    plt.imshow(trust2[0,:,:,i-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ANNnXlXxVOL"
   },
   "outputs": [],
   "source": [
    "trust2[0,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKJvi3SuxVOL"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(1,13):\n",
    "    plt.subplot(1,12,i)\n",
    "    plt.imshow(trust3[0,:,:,i-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YCh8VjXxVOM"
   },
   "outputs": [],
   "source": [
    "opinion1 = np.moveaxis(opinion1, -1, 0)\n",
    "opinion2 = np.moveaxis(opinion2, -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuVFnYxZxVOM"
   },
   "outputs": [],
   "source": [
    "opinion1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsNvBIMGxVOM"
   },
   "outputs": [],
   "source": [
    "max_trust1 = opinion1[0]+opinion1[2]*opinion1[3]\n",
    "max_trust2 = opinion2[0]+opinion2[2]*opinion2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za93xzI-xVOM"
   },
   "outputs": [],
   "source": [
    "max_trust1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDVtYMEMxVOM"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(1,7):\n",
    "    plt.subplot(1,6,i)\n",
    "    plt.imshow(max_trust1[0,:,:,i-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsJokfiHxVOM"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(1,13):\n",
    "    plt.subplot(1,12,i)\n",
    "    plt.imshow(max_trust2[0,:,:,i-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztTfT8va5SGy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MINIST_with_Max_Trust.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
