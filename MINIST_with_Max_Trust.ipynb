{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MINIST_with_Max_Trust.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"code","metadata":{"id":"YZdxOxa97fZO","executionInfo":{"status":"ok","timestamp":1637736748068,"user_tz":-480,"elapsed":3568,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import random\n","import time\n","import matplotlib.pyplot as plt\n","from skimage import util\n","from math import exp, log\n","from tensorflow.python.keras.engine.base_layer import Layer\n","from matplotlib import rc\n","rc('mathtext', default='regular')"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0dm5c_T7fZX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637736748966,"user_tz":-480,"elapsed":900,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}},"outputId":"0c91fa1e-fa48-4518-cd3c-4924825b5b1a"},"source":["class MNISTLoader():\n","    \"\"\"\n","    Dataset loader class\n","    \n","    \"\"\"\n","    def __init__(self):\n","        mnist = tf.keras.datasets.mnist\n","        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n","        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n","        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n","        self.train_label = self.train_label.astype(np.int32)    # [60000]\n","        self.test_label = self.test_label.astype(np.int32)      # [10000]\n","        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n","\n","    def get_batch(self, batch_size):\n","\n","        index = np.random.randint(0, self.num_train_data, batch_size)\n","        return self.train_data[index, :], self.train_label[index]\n","    \n","    def get_batch_test(self):\n","    \n","        return self.test_data, self.test_label\n","    \n","    def get_local_noise(self, noise_num=400, batch_size=10000):\n","        \"\"\"\n","        Self-defined Gaussian-distributed noise: Generate noise_num # of gassaion noise knowing location and intensity\n","        \n","        \"\"\"\n","        process_image = np.copy(self.test_data[:batch_size])\n","        W_x = intitial_W_x(process_image)\n","        uncer_mass = np.zeros(W_x[:,0,0,:,0].shape) + 0.01\n","        base_rate = np.zeros(W_x[:,0,0,:,0].shape) + 0.5\n","        image_size = process_image.shape[1]\n","        index_noise = np.random.randint(0, image_size*image_size, noise_num)\n","        \n","        for i in index_noise:\n","            noise_value = np.random.randn(batch_size,1) * 0.4\n","            feature_value = process_image[:,int(i/image_size),int(i%image_size),:]\n","            process_image[:,int(i/image_size),int(i%image_size),:] = feature_value + noise_value\n","            belief_mass = W_x[:,int(i/image_size),int(i%image_size),:,0] * (1-noise_value)\n","            disbelief_mass = 1 - belief_mass - uncer_mass\n","            W_x[:,int(i/image_size),int(i%image_size),:] = np.moveaxis(np.array([belief_mass,disbelief_mass,uncer_mass,base_rate]), 0, -1)\n","            \n","        return tf.clip_by_value(process_image, 0, 1), self.test_label[:batch_size], W_x, index_noise\n","\n","    def get_noise(self, noise_mode):\n","        process_image = np.copy(self.test_data)\n","        noise_gs_img = util.random_noise(process_image,mode=noise_mode,var=0.2)\n","        return noise_gs_img, self.test_label\n","    \n","data_loader = MNISTLoader()"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"kFxBUMwrU1sC","executionInfo":{"status":"ok","timestamp":1637736748967,"user_tz":-480,"elapsed":6,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def intitial_opinion_trust(X):\n","    \"\"\"\n","    Initialize input opinion for caseI.\n","    \n","    \"\"\"\n","    W_dis = np.zeros(X.shape).astype(np.float32)\n","    W_base = W_dis + 0.5\n","    W_x = np.array([X, W_dis, 1 - X, W_base]).astype(np.float32)\n","    x_trust = (W_x[0] + W_x[2] * 0.5)\n","    W_x = np.moveaxis(W_x, 0, -1)\n","    \n","    return W_x, x_trust"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4BFTLiT6G_p","executionInfo":{"status":"ok","timestamp":1637736748967,"user_tz":-480,"elapsed":5,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def intitial_W_x_location(X, index):\n","    \"\"\"\n","    Initialize input opinion for caseII&III.\n","    \n","    \"\"\"\n","    W_x = intitial_W_x(X)\n","    uncer_mass = np.zeros(W_x[:,0,0,:,0].shape) + 0.01\n","    base_rate = np.zeros(W_x[:,0,0,:,0].shape) + 0.5\n","    image_size = X.shape[1]\n","    for i in index:\n","        belief_mass = W_x[:,int(i/image_size),int(i%image_size),:,0]\n","        disbelief_mass = 1 - belief_mass - uncer_mass\n","        W_x[:,int(i/image_size),int(i%image_size),:] = np.moveaxis(np.array([belief_mass,disbelief_mass,uncer_mass,base_rate]), 0, -1)\n","\n","    return W_x"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXJNv0AWxVOC","executionInfo":{"status":"ok","timestamp":1637736748967,"user_tz":-480,"elapsed":5,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def padding(opinion, kernel_size):\n","    \"\"\"\n","    Trust map padding with maximum unblief mass on the padding cell. \n","    \n","    \"\"\"\n","    image_size = opinion.shape[1]\n","    p = int((kernel_size - 1)/2)\n","    padding_size = int(image_size + 2 * p)\n","    opinion_pad = np.zeros((int(opinion.shape[0]),padding_size,padding_size,int(opinion.shape[-2]),int(opinion.shape[-1])))\n","    opinion_pad[:,:,:,:] = np.array([0.0, 0.99, 0.01, 0.5])\n","    opinion_pad[:,p:p+image_size,p:p+image_size,:,:] = opinion\n","    return opinion_pad"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvzahj3w7fZZ","executionInfo":{"status":"ok","timestamp":1637736748967,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def multi(W_x, W_y): \n","    \"\"\"\n","    Multi-sources multiplication Operator \n","\n","    \"\"\"\n","    W_x = W_x.astype(np.float64) \n","    W_y = W_y.astype(np.float64)    \n","    W_b = W_x[0]*W_y[0]+((1-W_x[3])*W_y[3]*W_x[0]*W_y[2]+W_x[3]*(1-W_y[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n","    W_d = W_x[1]+W_y[1]-W_x[1]*W_y[1]\n","    W_u = W_x[2]*W_y[2]+((1-W_y[3])*W_x[0]*W_y[2]+(1-W_x[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n","    W_a = W_x[3]*W_y[3]\n","    return W_b,W_d,W_u,W_a"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGsdrX0k7fZZ","executionInfo":{"status":"ok","timestamp":1637736748967,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def conv_single_step_trust(W_x, W_w, W_b): \n","    \"\"\"\n","    Compute the output opinion of one convolutional kernel window.\n","    inputs:\n","    W_x - input opinion (batch size, kernel size, kernel size, channel, 4)\n","    W_w - weight opinion (kernel size, kernel size, channel, filter number, 4)\n","    W_b - bias opinion (filter number, 4)\n","    \n","    \"\"\"    \n","    W_x = W_x.astype(np.float64)   \n","    W_w = W_w.numpy().astype(np.float64) \n","    W_b = W_b.numpy().astype(np.float64) \n","    \n","    filter_number = W_b.shape[0]\n","    batch_size = W_x.shape[0]\n","    fusion_result = []\n","    W_x_expand = np.tile(np.expand_dims(W_x, axis=(4)), [1,1,1,1,filter_number,1])\n","    W_w_expand = np.tile(np.expand_dims(W_w, axis=(0)), [batch_size,1,1,1,1,1])\n","    W_b_expand = np.tile(np.expand_dims(W_b, axis=(0)), [batch_size,1,1])\n","    W_wx = multi(np.transpose(W_x_expand),np.transpose(W_w_expand)) # (4, 32, 3, 5, 5, 50)\n","    fusion_result = avg_fusion(np.asarray(W_wx), np.transpose(W_b_expand))\n","    \n","    return np.transpose(fusion_result[0]),np.transpose(fusion_result[1])\n","\n","def avg_fusion(W_wx, W_b):\n","    \"\"\"\n","    Multi-sources average opinion operator.\n","    W_wx - opinion of input mutiplied with weight opinion (4, filter number, channel, kernel size, kernel size, batch size)\n","    W_b - bias opinion (4, filter number, batch size)\n","\n","    \"\"\"\n","    \n","    W_wx = np.reshape(W_wx, (W_wx.shape[0],W_wx.shape[1], \n","                             W_wx.shape[2]*W_wx.shape[3]*W_wx.shape[4], W_wx.shape[5])).astype(np.float64) # (4, 32, 75, 50)\n","    W_b = W_b.astype(np.float64)\n","    \n","    n_filter = W_b.shape[1]\n","    batch_size = W_wx.shape[-1]\n","    num_para = W_wx.shape[2]\n","    b_wx, u_wx, a_wx = W_wx[0], W_wx[2], W_wx[3]\n","    b_b, u_b, a_b = W_b[0], W_b[2], W_b[3]\n","\n","    u_combine = np.concatenate((u_wx, np.reshape(u_b,(u_b.shape[0], 1, batch_size))), axis=1) # (32, 76, 50)\n","    b_combine = np.concatenate((b_wx, np.reshape(b_b,(b_b.shape[0], 1, batch_size))), axis=1) # (32, 76, 50)\n","    u_combine_recip = (np.zeros(u_combine.shape)+1)/u_combine\n","    \n","    numerator = np.sum(b_combine * u_combine_recip, axis = 1) # (32, 50)\n","    denominator = np.sum(u_combine_recip, axis = (1))\n","    \n","    b_fusion = numerator / denominator\n","    u_fusion = (num_para+1) / denominator\n","    a_fusion = (np.sum(a_wx, axis=(1)) + a_b) / (num_para+1)\n","    \n","    return np.array([b_fusion, 1-b_fusion-u_fusion, u_fusion, a_fusion]).astype(np.float32), (b_fusion + u_fusion * a_fusion).astype(np.float32)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lg8hAmI7fZa","executionInfo":{"status":"ok","timestamp":1637736748968,"user_tz":-480,"elapsed":5,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def conv_forward(A_prev, W_w, W_b):\n","    \"\"\"\n","    Forward convolutional opinion calculation.\n","    A_prev - feature map from last layer (batch_size, H, W, channel) \n","    W_w - opinion of weight (kernel_size, kernel_size, 3, filter_num, 4)\n","    W_b - opinion of bias (filter_num, 4)\n","\n","    \"\"\"\n","    (batch_size, n_H_prev, n_W_prev, _, _) = A_prev.shape\n","    ( f , f , _, n_C, _) = W_w.shape\n","    stride = 1\n","    pad = 1\n","    A_prev_pad = padding(A_prev,f)\n","\n","    n_H = int(( n_H_prev - f + 2 * pad )/ stride) + 1\n","    n_W = int(( n_W_prev - f + 2 * pad )/ stride) + 1\n"," \n","    Z = np.zeros((batch_size, n_H, n_W, n_C, 4)) \n","    Z_trust = np.zeros((batch_size, n_H, n_W, n_C))\n","\n","    for h in range(n_H):                       \n","        for w in range(n_W):                              \n","            vert_start = h * stride        \n","            vert_end = vert_start + f       \n","            horiz_start = w * stride        \n","            horiz_end = horiz_start + f     \n","            a_slice_prev = A_prev_pad[:,vert_start:vert_end,horiz_start:horiz_end,:,:]   \n","            opinion, trust = conv_single_step_trust(a_slice_prev,W_w,W_b)   \n","            Z[:,h,w,:,:] = opinion\n","            Z_trust[:,h,w,:] = trust\n","\n","    return Z, Z_trust"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"15za74YC7fZa","executionInfo":{"status":"ok","timestamp":1637736748968,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def get_update_matrix(grads_w, grads_b, rs):\n","    \"\"\"\n","    Update the opinion of weight and bias during training process.\n","    grads_w - gradient evidence matrix of weight\n","    grads_b - gradient evidence matrix of bias\n","    rs - positive evidence plus negative evidence\n","    \n","    \"\"\"\n","    \n","    W_w = np.array([grads_w/(rs+2),(rs-grads_w)/(rs+2),np.full(grads_w.shape, 2/(rs+2)),\n","                    np.full(grads_w.shape, 0.5)]).astype(np.float32)\n","    W_b = np.array([grads_b/(rs+2), (rs-grads_b)/(rs+2), np.full(grads_b.shape, 2/(rs+2)), \n","                    np.full(grads_b.shape, 0.5)]).astype(np.float32)\n","    W_w = np.moveaxis(W_w, 0, -1)\n","    \n","    return W_w, W_b.swapaxes(0,1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgpZvsdWxVOG","executionInfo":{"status":"ok","timestamp":1637736748968,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["def evidence_collect(y, y_pred):\n","    \"\"\"\n","    Collect positive and negative evidence during the training process.\n","\n","    \"\"\"\n","    r = 0\n","    s = 0\n","    r_list = [0]*10\n","    s_list = [0]*10\n","    \n","    for j in range(len(y_pred)):\n","        for i in range(len(y_pred[0])):\n","            if i == y[j]:\n","                if y_pred[j][i] > 0.8:\n","                    r_list[i]+=1\n","                    r+=1\n","                else:\n","                    s+=1\n","                    s_list[i]+=1\n","            else:\n","                if y_pred[j][i] < 0.2:\n","                    r_list[i]+=1\n","                else:\n","                    s_list[i]+=1\n","                    \n","    y_N_op = []\n","    for i in range(len(r_list)):\n","        y_N_op.append([r_list[i]/(r_list[i]+s_list[i]+2), \n","                       s_list[i]/(r_list[i]+s_list[i]+2), 2/(r_list[i]+s_list[i]+2), 0.5])\n","\n","    \n","    return [r/(r+s+2), s/(r+s+2), 2/(r+s+2), 0.5], y_N_op"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJbAPUO37fZY","executionInfo":{"status":"ok","timestamp":1637736748968,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["class MyLayerMaxTrust(tf.keras.layers.Layer):\n","    \"\"\"\n","    Max-trust fuction layer\n","    A new pooling layer function based on trustworthiness values instead of feature values. \n","\n","    \"\"\"\n","    def __init__(self):\n","        super(MyLayerMaxTrust, self).__init__()\n","\n","\n","    def call(self, x, opinion, trust):\n","        \"\"\"\n","        x - feature map from last layer (batch, H, W, C)\n","        opinion - corresponding opinion (batch, H, W, C, 4)\n","        trust - corresponding trust (batch, H, W, C)\n","\n","        \"\"\"\n","        trust_mul = np.zeros(x.shape)\n","        image_size = x.shape[1]\n","        opinion_out = np.zeros((opinion.shape[0],int(opinion.shape[1]/2), int(opinion.shape[2]/2),opinion.shape[3], 4)).astype(np.float32)\n","        for i in range(trust.shape[0]):\n","            for k in range(trust.shape[-1]):\n","                input_max = trust[i,:,:,k].reshape((1,trust.shape[1],trust.shape[2],1))\n","                _, argmax = tf.nn.max_pool_with_argmax(input = input_max, ksize = [1, 2, 2, 1],\n","                                                    strides = [1, 2, 2, 1], padding = 'VALID')\n","                argmax_1d = argmax.numpy().flatten()\n","                for j in range(len(argmax_1d)):\n","                    trust_mul[i,int(argmax_1d[j]/image_size),int(argmax_1d[j]%image_size),k] = 1\n","                    opinion_out[i,int(j/int(image_size/2)), int(j%int(image_size/2)),k,:] = opinion[i,int(argmax_1d[j]/image_size),int(argmax_1d[j]%image_size),k,:] \n","\n","        x = x * trust_mul\n","\n","        return x, opinion_out"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0xqqyWB7fZZ","executionInfo":{"status":"ok","timestamp":1637736749419,"user_tz":-480,"elapsed":455,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["class MyLayerCONV(tf.keras.layers.Layer):\n","    \"\"\"\n","    Conv opinion computation layer\n","    \n","    \"\"\"\n","    def __init__(self):\n","        super(MyLayerCONV, self).__init__()\n","\n","\n","    def call(self, input1, input2, input3):\n","        \n","        Z, Z_trust = conv_forward(input1, input2, input3)\n","           \n","        return Z.astype(np.float32), Z_trust.astype(np.float32)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLQTm3SOxVOF","executionInfo":{"status":"ok","timestamp":1637736749420,"user_tz":-480,"elapsed":5,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["class MyLayerMaxPool(tf.keras.layers.Layer):\n","    \"\"\"\n","    Original max-pooling with trust framework\n","     \n","    \"\"\"\n","    def __init__(self):\n","        super(MyLayerMaxPool, self).__init__()\n","\n","\n","    def call(self, x, opinion):\n","        \"\"\"\n","        x-(50,14,14,8)\n","        \n","        \"\"\"\n","        image_size = x.shape[1]\n","        opinion_out = np.zeros((opinion.shape[0],int(opinion.shape[1]/2), \n","                                int(opinion.shape[2]/2),opinion.shape[3], 4)).astype(np.float32)\n","\n","        for i in range(x.shape[0]):\n","            for k in range(x.shape[-1]):\n","                input_max = x[i,:,:,k].numpy().reshape((1,x.shape[1],x.shape[2],1))\n","                _, argmax = tf.nn.max_pool_with_argmax(input = input_max, ksize = [1, 2, 2, 1],\n","                                                    strides = [1, 2, 2, 1], padding = 'VALID')\n","                argmax_1d = argmax.numpy().flatten()\n","                for j in range(len(argmax_1d)):\n","                    opinion_out[i,int(j/int(image_size/2)), int(j%int(image_size/2)),k,:] = opinion[i,int(argmax_1d[j]/image_size),int(argmax_1d[j]%image_size),k,:] \n","        return opinion_out"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"yby1VsHD5SGv","scrolled":true,"executionInfo":{"status":"ok","timestamp":1637736749420,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["filter_num1 = 4\n","filter_num2 = 8\n","filter_num3 = 8\n","kernelsize = 3"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNz1JJI56G_t","executionInfo":{"status":"ok","timestamp":1637736749420,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["class CNN(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.convopinion1 = MyLayerCONV()\n","        self.convopinion2 = MyLayerCONV()\n","        self.convopinion3 = MyLayerCONV()\n","\n","        self.conv1 = tf.keras.layers.Conv2D(\n","            filters=filter_num1,             \n","            kernel_size=[kernelsize, kernelsize],\n","            padding='same',\n","            activation=tf.nn.relu   \n","        )\n","        self.conv2 = tf.keras.layers.Conv2D(\n","            filters=filter_num2,           \n","            kernel_size=[kernelsize, kernelsize],    \n","            padding='same',\n","            activation=tf.nn.relu   \n","        )\n","        self.conv3 = tf.keras.layers.Conv2D(\n","            filters=filter_num3,            \n","            kernel_size=[kernelsize, kernelsize],     \n","            padding='same',         \n","            activation=tf.nn.relu   \n","        )\n","\n","        self.maxtrust1 = MyLayerMaxTrust()\n","        self.maxtrust2 = MyLayerMaxTrust()\n","        self.maxfeature1 = MyLayerMaxPool()\n","        self.maxfeature2 = MyLayerMaxPool()\n","        \n","        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n","        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n","       \n","        self.flatten = tf.keras.layers.Flatten()\n","        self.dense1 = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)\n","        self.dense2 = tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        x - 50,32,32,3\n","        W_x - 50,32,32,3,4\n","        \n","        \n","        \"\"\"\n","        X, W_x, W_w1, W_b1, W_w2, W_b2 = inputs\n"," \n","        opinion1, trust1 = self.convopinion1(W_x, W_w1, W_b1)\n","        x = self.conv1(X)   \n","        x, pooling_opinion1 = self.maxtrust1(x, opinion1, trust1)\n","        x = self.pool1(x) \n","        opinion2, _ = self.convopinion2(pooling_opinion1, W_w2, W_b2)\n","        x = self.conv2(x)\n","        pooling_opinion2 = self.maxfeature2(x, opinion2)\n","        x = self.pool2(x)    \n","        x = self.flatten(x) \n","        x = self.dense1(x)    \n","        output = self.dense2(x)                    \n","        return output,opinion1,opinion2,pooling_opinion2\n","    \n","model = CNN()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mYRIhMy7fZY","executionInfo":{"status":"ok","timestamp":1637736749421,"user_tz":-480,"elapsed":5,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["num_epochs = 2\n","batch_size = 50\n","threshold1 = 3.0\n","threshold2 = 5.0\n","optimizer = tf.keras.optimizers.Adam()\n","sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMt5J6p-7fZb","executionInfo":{"status":"ok","timestamp":1637736749421,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["# initial parameters opinion\n","\n","w_initial1 = np.array([[[[[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num1)] \n","                         for _ in range(1)] for _ in range(kernelsize)] for _ in range(kernelsize)]).astype(np.float32)\n","b_initial1 = np.array([[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num1)]).astype(np.float32)\n","\n","w_initial2 = np.array([[[[[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num2)] \n","                         for _ in range(filter_num1)] for _ in range(kernelsize)] for _ in range(kernelsize)]).astype(np.float32)\n","b_initial2 = np.array([[0.0, 0.0, 1.0, 0.5] for _ in range(filter_num2)]).astype(np.float32)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsdJSsfTxVOH","executionInfo":{"status":"ok","timestamp":1637736749421,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingyang Sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08895688805861299526"}}},"source":["train_acc = []\n","train_loss = []\n","opinion_convlist = []\n","y_update_wb = []\n","y_N_opinion = []"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Cc0BUeq7fZb","scrolled":true},"source":["# training process\n","num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n","W_w1 = w_initial1\n","W_b1 = b_initial1\n","W_w2 = w_initial2\n","W_b2 = b_initial2\n","\n","grads_list_w1 = np.zeros((kernelsize,kernelsize,1,filter_num1))\n","grads_list_b1 = np.zeros((filter_num1,))\n","grads_list_w2 = np.zeros((kernelsize,kernelsize,filter_num1,filter_num2))\n","grads_list_b2 = np.zeros((filter_num2,))\n","\n","import time\n","start = time.time()\n","\n","for batch_index in range(num_batches):\n","    print('# of batch:',batch_index)\n","    X, y = data_loader.get_batch(batch_size)  \n","    W_x ,_ = intitial_opinion_trust(X)\n","    with tf.GradientTape() as tape:\n","        y_pred,opinion_conv1,opinion_conv2,opinion_conv = model([X, W_x, W_w1, W_b1, W_w2, W_b2])\n","        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n","        \n","    grads = tape.gradient(loss, model.variables)\n","    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n","    \n","    sparse_categorical_accuracy.update_state(y_true=y, y_pred=y_pred)\n","    opinion_convlist.append(np.reshape(opinion_conv, (opinion_conv.shape[0],int(opinion_conv.shape[1]*opinion_conv.shape[2]*opinion_conv.shape[3]),4)))\n","    train_loss.append(tf.reduce_mean(loss).numpy())\n","    train_acc.append(sparse_categorical_accuracy.result().numpy())\n","    print(\"train accuracy:\",sparse_categorical_accuracy.result().numpy())\n","   \n","# update W_w, W_b\n","    grads_list_w1 = grads_list_w1 + tf.where(abs(grads[0])<threshold1, 1, 0)  \n","    grads_list_b1 = grads_list_b1 + tf.where(abs(grads[1])<threshold1, 1, 0)  \n","    grads_list_w2 = grads_list_w2 + tf.where(abs(grads[2])<threshold2, 1, 0)  \n","    grads_list_b2 = grads_list_b2 + tf.where(abs(grads[3])<threshold2, 1, 0) \n","    W_w1, W_b1 = get_update_matrix(grads_list_w1, grads_list_b1, batch_index+1)\n","    W_w2, W_b2 = get_update_matrix(grads_list_w2, grads_list_b2, batch_index+1)\n","\n","    y_N_update, y_N_op = evidence_collect(y, y_pred)\n","    y_N_opinion.append(y_N_op)\n","    y_update_wb.append(y_N_update)\n","    print('One batch end:',(time.time()-start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UDLBHel6G_v"},"source":["import pickle\n","def save_variable(v,filename):\n","    f=open(filename,'wb')\n","    pickle.dump(v,f)\n","    f.close()\n","    return filename\n"," \n","def load_variavle(filename):\n","    f=open(filename,'rb')\n","    r=pickle.load(f)\n","    f.close()\n","    return r"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMxGkIAL6G_v"},"source":["# model.save_weights('my_model_max_trust_MINIST')\n","# save_variable(W_w1, 'MINST_weight_opinion1')\n","# save_variable(W_w2, 'MINST_weight_opinion2')\n","# save_variable(W_b1, 'MINIST_bias_opinion1')\n","# save_variable(W_b2, 'MINIST_bias_opinion2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKLsuDNk6G_v"},"source":["model.load_weights('my_model_max_trust_MINIST')\n","W_w1 = load_variavle('max_trust_MINST_weight_opinion1')\n","W_w2 = load_variavle('max_trust_MINST_weight_opinion2')\n","W_b1 = load_variavle('max_trust_MINIST_bias_opinion1')\n","W_b2 = load_variavle('max_trust_MINIST_bias_opinion2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGew9OE1xVOK"},"source":["# X_test, y_test= data_loader.get_batch_test()\n","# W_x ,_ = intitial_opinion_trust(X_test)\n","# y_pred_test, _,_,_ = model([X_test, W_x, W_w1, W_b1, W_w2, W_b2])\n","# sparse_categorical_accuracy.reset_states()\n","# sparse_categorical_accuracy.update_state(y_true=y_test, y_pred=y_pred_test)\n","# print(\"test accuracy:\",sparse_categorical_accuracy.result().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOopxTrEO4b6","scrolled":true},"source":["# Evaluation of noisy dataset\n","X_test_noise, y_test_noise = data_loader.get_noise('gaussian')\n","noise_opinion,_ = intitial_opinion_trust(X_test_noise)\n","y_pred_test_noise,opinion_conv1,opinion_conv2,opinion_conv = model([X_test_noise, noise_opinion, W_w1, W_b1, W_w2, W_b2])\n","sparse_categorical_accuracy.reset_states()\n","sparse_categorical_accuracy.update_state(y_true=y_test_noise, y_pred=y_pred_test_noise)\n","print(\"test accuracy:\",sparse_categorical_accuracy.result().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7JjOBlcY6G_w"},"source":["# Evaluation of three cases\n","X_test_local_noise, y_test_local_noise, case1_opinion, noise_index = data_loader.get_local_noise()\n","case2_opinion = intitial_W_x_location(X_test_local_noise, noise_index)\n","case3_opinion = intitial_W_x(X_test_local_noise)\n","y_pred_test_local_noise,_,_,opinion_noise = model([X_test_local_noise, case1_opinion, W_w1, W_b1, W_w2, W_b2])\n","sparse_categorical_accuracy.reset_states()\n","sparse_categorical_accuracy.update_state(y_true=y_test_local_noise, y_pred=y_pred_test_local_noise)\n","print(\"test accuracy:\",sparse_categorical_accuracy.result().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LwDNLH86G_w"},"source":["def get_NN_trust(opinion_last_layer, true_label, pre_label, y_update_wb):\n","    # compute dense opinion\n","    opinion_dense = np.average(np.array(opinion_last_layer), axis=0)\n","    opinion_dense = np.reshape(opinion_dense, (int(opinion_dense.shape[0]*opinion_dense.shape[1]*opinion_dense.shape[2]),4))\n","    y_true_op = [1.0, 0.0, 0.0, 0.5]\n","    W_y_update = evidence_collect_test(true_label, pre_label)\n","    \n","    # compute Backward opinion of neuron W_N_Y  \n","    W_N_Y=[]\n","    for j in W_y_update:\n","        W_N_Y.append(multi(j,y_true_op)) # change when add flaw in label\n","        \n","    W_w_dense = y_update_wb[-1]\n","    W_b_dense = y_update_wb[-1]\n","\n","    W_xw=[]\n","    W_xw.append(W_b_dense)\n","    for j in range(opinion_dense.shape[0]):\n","        W_xw.append(multi(W_w_dense,opinion_dense[j])) # (513, 4)\n","    dense_out = fusion(np.array(W_xw))\n","    #     print('Underflow or not: ',np.isnan(np.min(np.array(dense1_out_list))))\n","\n","    # last layer\n","    W_xw=[]\n","    W_xw.append(W_b_dense)\n","    for j in range(64):\n","        W_xw.append(multi(W_w_dense,dense_out))\n","    W_NN = fusion(np.array(W_xw))\n","    \n","    # compute last layer output opinion and trust\n","    W_XY_one = []\n","    for j in range(10):\n","        W_XY_one.append(fusion_2(W_NN,W_N_Y[j]))\n","    W_NN = fusion(np.array(W_XY_one))\n","    W_trust = W_NN[0]+W_NN[2]*W_NN[3]\n","    \n","    return W_trust, W_NN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FybnpbUYxVOI"},"source":["# forward opinion prop\n","def mul_scale(W_x):\n","    mul_u = 1.0\n","    for i in range(len(W_x)):\n","        mul_u = mul_u * W_x[i]\n","        if mul_u <= 1e-24: \n","            mul_u = mul_u*1e25\n","    return mul_u\n","\n","def fusion(W_x): # W_x array\n","\n","    deno = 0.0\n","    mole = 0.0\n","    full_multi = mul_scale(W_x[:,2])\n","    for i in range(len(W_x[:,2])):\n","        deno = deno + full_multi/W_x[:,2][i]\n","        mole = mole + (full_multi/W_x[:,2][i])*W_x[:,0][i]\n","    W_b = mole/deno  #  change\n","    W_u = (len(W_x)*full_multi)/deno   # change\n","    W_a = sum(W_x[:,3])/len(W_x)  \n","\n","#     W_b = sum(1/len(W_x)*W_x[:,0])\n","#     W_u = 0.0\n","#     W_a = sum(1/len(W_x)*W_x[:,3])\n","        \n","    return [W_b,1-W_b-W_u,W_u,W_a]\n","\n","def fusion_2(W_x, W_y):\n","    if W_x[2]!=0 or W_y[2]!=0:\n","        W_b = (W_x[0]*W_y[2]+W_y[0]*W_x[2])/(W_x[2]+W_y[2])\n","        W_u = 2*W_x[2]*W_y[2]/(W_x[2]+W_y[2])\n","        W_a = (W_x[3]+W_y[3])/2  \n","    elif W_x[2]==0 and W_y[2]==0:\n","        W_b = 0.5*W_x[0]+0.5*W_y[0]\n","        W_u = 0\n","        W_a = 0.5*W_x[3]+0.5*W_y[3]\n","    return [W_b,1-W_b-W_u,W_u,W_a]\n","\n","def multi(W_x, W_y): # \n","    W_b = W_x[0]*W_y[0]+((1-W_x[3])*W_y[3]*W_x[0]*W_y[2]+W_x[3]*(1-W_y[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n","    W_d = W_x[1]+W_y[1]-W_x[1]*W_y[1]\n","    W_u = W_x[2]*W_y[2]+((1-W_y[3])*W_x[0]*W_y[2]+(1-W_x[3])*W_y[0]*W_x[2])/(1-W_x[3]*W_y[3])\n","    W_a = W_x[3]*W_y[3]\n","    return [W_b,W_d,W_u,W_a]\n","\n","\n","def evidence_collect_test(y, y_pred):\n","    r_list = [0]*10\n","    s_list = [0]*10\n","    index = np.random.randint(0,y.shape[0], 5000)\n","    \n","    for j in index:\n","        for i in range(len(y_pred[0])):\n","            if i == y[j]:\n","                if y_pred[j][i] > 0.9:\n","                    r_list[i]+=1\n","                else:\n","                    s_list[i]+=1\n","            else:\n","                if y_pred[j][i] < 0.1:\n","                    r_list[i]+=1\n","                else:\n","                    s_list[i]+=1\n","                    \n","    y_N_op = []\n","    for i in range(len(r_list)):\n","        y_N_op.append(np.array([r_list[i]/(r_list[i]+s_list[i]+2), \n","                       s_list[i]/(r_list[i]+s_list[i]+2), 2/(r_list[i]+s_list[i]+2), 0.5]))\n","\n","    \n","    return y_N_op"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TqNZd2HxVOK"},"source":["# compute Backward opinion of neuron W_N_Y  \n","W_N_Y_noise=[]\n","for j in W_y_update:\n","    W_N_Y_noise.append(multi(j,y_true_op)) # change when add flaw in label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXBrgH_rxVOK"},"source":["opinion_dense_noise = np.average(np.array(opinion_mid), axis=0)\n","opinion_dense_noise = np.reshape(opinion_dense_noise, (int(opinion_dense_noise.shape[0]*opinion_dense_noise.shape[1]*opinion_dense_noise.shape[2]),4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFSE0_K0xVOL"},"source":["W_w_dense = y_update_wb[-1]\n","W_b_dense = y_update_wb[-1]\n","input_dense = opinion_dense_noise\n","\n","W_xw=[]\n","W_xw.append(W_b_dense)\n","for j in range(opinion_dense_noise.shape[0]):\n","    W_xw.append(multi(W_w_dense,input_dense[j]))\n","dense_out_noise = fusion(np.array(W_xw))\n","#     print('Underflow or not: ',np.isnan(np.min(np.array(dense1_out_list))))\n","\n","# last layer\n","W_xw=[]\n","W_xw.append(W_b_dense)\n","for j in range(64):\n","    W_xw.append(multi(W_w_dense,dense_out_noise))\n","W_NN_noise = fusion(np.array(W_xw))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7jZ9h3IxVOL"},"source":["# compute last layer output opinion and trust\n","W_XY_one = []\n","for j in range(10):\n","    W_XY_one.append(fusion_2(W_NN_noise,W_N_Y_noise[j]))\n","\n","W_NN = fusion(np.array(W_XY_one))\n","W_trust = W_NN[0]+W_NN[2]*W_NN[3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVDfqwfrxVOL"},"source":["entropy = -1 * (W_trust * np.log2(W_trust) + (1-W_trust) * np.log2(1-W_trust))\n","entropy"],"execution_count":null,"outputs":[]}]}